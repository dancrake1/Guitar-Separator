{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded audio with shape torch.Size([2, 12602306]) and sample rate 48000\n",
      "Using device: mps\n",
      "Resampled from 48000 to 44100\n",
      "Separating sources...\n",
      "Model sources: ['drums', 'bass', 'other', 'vocals', 'guitar', 'piano']\n",
      "Saved drums to ../outputs/../ZachBryan-SomethingInTheOrange/separated_drums.wav\n",
      "Saved bass to ../outputs/../ZachBryan-SomethingInTheOrange/separated_bass.wav\n",
      "Saved other to ../outputs/../ZachBryan-SomethingInTheOrange/separated_other.wav\n",
      "Saved vocals to ../outputs/../ZachBryan-SomethingInTheOrange/separated_vocals.wav\n",
      "Saved guitar to ../outputs/../ZachBryan-SomethingInTheOrange/separated_guitar.wav\n",
      "Saved enhanced guitar to ../outputs/../ZachBryan-SomethingInTheOrange/separated_guitar_enhanced.wav\n",
      "Saved piano to ../outputs/../ZachBryan-SomethingInTheOrange/separated_piano.wav\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from demucs.pretrained import get_model\n",
    "from demucs.apply import apply_model\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '../outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "song_path = '../Zach_Bryan_-_Something_In_The_Orange.mp3'\n",
    "model_name = 'htdemucs_6s'\n",
    "song_name = os.path.splitext(song_path)[0].replace('_', '')  # Correctly extract filename without extension\n",
    "os.makedirs(f'{output_dir}/{song_name}', exist_ok=True)\n",
    "\n",
    "# Load the audio\n",
    "sample_waveform, sample_rate = torchaudio.load(song_path)\n",
    "print(f\"Loaded audio with shape {sample_waveform.shape} and sample rate {sample_rate}\")\n",
    "\n",
    "# Load the model\n",
    "# model = get_model(\"htdemucs_6s\")\n",
    "model = get_model(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Check for device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Ensure correct format\n",
    "if sample_rate != model.samplerate:\n",
    "    waveform = torchaudio.functional.resample(sample_waveform, sample_rate, model.samplerate)\n",
    "    print(f\"Resampled from {sample_rate} to {model.samplerate}\")\n",
    "else:\n",
    "    waveform = sample_waveform\n",
    "\n",
    "# Handle channels if needed\n",
    "if waveform.shape[0] > 2:\n",
    "    waveform = waveform[:2, :]\n",
    "elif waveform.shape[0] == 1:\n",
    "    waveform = torch.cat([waveform, waveform], dim=0)\n",
    "\n",
    "waveform = waveform.to(device)\n",
    "\n",
    "def enhance_guitar(guitar_waveform, sample_rate):\n",
    "    # Add batch dimension if needed\n",
    "    if guitar_waveform.dim() == 2:\n",
    "        guitar_waveform = guitar_waveform.unsqueeze(0)\n",
    "        \n",
    "    # Now we can safely unpack dimensions\n",
    "    b, c, t = guitar_waveform.shape\n",
    "    \n",
    "    guitar_waveform_freq = torch.fft.rfft(guitar_waveform, dim=2)\n",
    "    \n",
    "    # Create high-pass filter (reduce below 80Hz)\n",
    "    freqs = torch.fft.rfftfreq(t, d=1/sample_rate)\n",
    "    high_pass = (1 - torch.exp(-freqs/80))\n",
    "    \n",
    "    # Apply filter\n",
    "    guitar_waveform_freq *= high_pass.view(1, 1, -1)\n",
    "    guitar_waveform = torch.fft.irfft(guitar_waveform_freq, n=t, dim=2)\n",
    "    \n",
    "    # Apply subtle compression\n",
    "    peak = guitar_waveform.abs().max()\n",
    "    if peak > 0:\n",
    "        guitar_waveform = 0.9 * guitar_waveform / peak\n",
    "    \n",
    "    # Remove batch dimension if we added it\n",
    "    if b == 1:\n",
    "        guitar_waveform = guitar_waveform.squeeze(0)\n",
    "        \n",
    "    return guitar_waveform\n",
    "\n",
    "# Separate\n",
    "print(\"Separating sources...\")\n",
    "with torch.no_grad():\n",
    "    sources = apply_model(model, waveform.unsqueeze(0))[0]\n",
    "    sources = sources.cpu()\n",
    "\n",
    "# Print available sources\n",
    "print(f\"Model sources: {model.sources}\")\n",
    "\n",
    "# Save all sources\n",
    "for idx, source_name in enumerate(model.sources):\n",
    "    source_wav = sources[idx]\n",
    "    output_file = os.path.join(output_dir, song_name, f\"separated_{source_name}.wav\")\n",
    "    \n",
    "    # Save original source\n",
    "    torchaudio.save(output_file, source_wav, model.samplerate)\n",
    "    print(f\"Saved {source_name} to {output_file}\")\n",
    "    \n",
    "\n",
    "    # For guitar, also save enhanced version\n",
    "    if source_name == 'guitar':\n",
    "        enhanced_guitar_wav = enhance_guitar(source_wav, model.samplerate)\n",
    "        output_file_enhanced = os.path.join(output_dir, song_name, f\"separated_{source_name}_enhanced.wav\")\n",
    "        torchaudio.save(output_file_enhanced, enhanced_guitar_wav, model.samplerate)\n",
    "        print(f\"Saved enhanced {source_name} to {output_file_enhanced}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drums', 'bass', 'other', 'vocals', 'guitar', 'piano']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio from Bon_Iver_St._Vincent_-_Roslyn_Lyrics.mp3...\n",
      "Loaded audio with shape torch.Size([2, 14800214]) and sample rate 48000\n",
      "Using device: mps\n",
      "STAGE 1: Separating with htdemucs_ft...\n",
      "Saved 'other' stem to outputs/BonIverSt.Vincent-RoslynLyrics/stage1_other.wav\n",
      "STAGE 2: Extracting guitar from 'other' using htdemucs_6s...\n",
      "Saved extracted guitar to outputs/BonIverSt.Vincent-RoslynLyrics/stage2_guitar_from_other.wav\n",
      "Saved enhanced guitar to outputs/BonIverSt.Vincent-RoslynLyrics/stage2_guitar_enhanced.wav\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from demucs.pretrained import get_model\n",
    "from demucs.apply import apply_model\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "song_path = 'Bon_Iver_St._Vincent_-_Roslyn_Lyrics.mp3'\n",
    "song_name = os.path.splitext(os.path.basename(song_path))[0].replace('_', '')\n",
    "os.makedirs(f'{output_dir}/{song_name}', exist_ok=True)\n",
    "\n",
    "# Load the audio\n",
    "print(f\"Loading audio from {song_path}...\")\n",
    "sample_waveform, sample_rate = torchaudio.load(song_path)\n",
    "print(f\"Loaded audio with shape {sample_waveform.shape} and sample rate {sample_rate}\")\n",
    "\n",
    "# Determine device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def prepare_audio(waveform, source_sr, target_sr):\n",
    "    \"\"\"Prepare audio for model input\"\"\"\n",
    "    # Resample if needed\n",
    "    if source_sr != target_sr:\n",
    "        waveform = torchaudio.functional.resample(waveform, source_sr, target_sr)\n",
    "        \n",
    "    # Handle channels\n",
    "    if waveform.shape[0] > 2:\n",
    "        waveform = waveform[:2, :]\n",
    "    elif waveform.shape[0] == 1:\n",
    "        waveform = torch.cat([waveform, waveform], dim=0)\n",
    "        \n",
    "    return waveform\n",
    "\n",
    "def enhance_guitar(guitar_waveform, sample_rate):\n",
    "    \"\"\"Enhance guitar with filters and transient processing\"\"\"\n",
    "    # Add batch dimension if needed\n",
    "    if guitar_waveform.dim() == 2:\n",
    "        guitar_waveform = guitar_waveform.unsqueeze(0)\n",
    "        \n",
    "    # Now we can safely unpack dimensions\n",
    "    b, c, t = guitar_waveform.shape\n",
    "    \n",
    "    # FFT for frequency domain processing\n",
    "    guitar_waveform_freq = torch.fft.rfft(guitar_waveform, dim=2)\n",
    "    \n",
    "    # Create high-pass filter (reduce below 80Hz)\n",
    "    freqs = torch.fft.rfftfreq(t, d=1/sample_rate)\n",
    "    high_pass = (1 - torch.exp(-freqs/80))\n",
    "    \n",
    "    # Create mid boost around 2-4kHz (presence)\n",
    "    mid_boost = 1.0 + 0.5 * torch.exp(-((freqs - 3000)/500)**2)\n",
    "    \n",
    "    # Apply filters\n",
    "    filter_curve = high_pass.view(1, 1, -1) * mid_boost.view(1, 1, -1)\n",
    "    guitar_waveform_freq *= filter_curve\n",
    "    \n",
    "    # Back to time domain\n",
    "    guitar_waveform = torch.fft.irfft(guitar_waveform_freq, n=t, dim=2)\n",
    "    \n",
    "    # Apply subtle compression\n",
    "    peak = guitar_waveform.abs().max()\n",
    "    if peak > 0:\n",
    "        # Simple soft knee compression\n",
    "        threshold = 0.7\n",
    "        ratio = 3.0\n",
    "        gain = 1.2\n",
    "        \n",
    "        above_thresh = (guitar_waveform.abs() > threshold * peak).float()\n",
    "        comp_factor = 1.0 - above_thresh * (1.0 - 1.0/ratio) * (guitar_waveform.abs() - threshold * peak) / (peak * (1.0 - threshold))\n",
    "        guitar_waveform = guitar_waveform * comp_factor * gain\n",
    "        \n",
    "        # Final limiter\n",
    "        peak = guitar_waveform.abs().max()\n",
    "        if peak > 0.95:\n",
    "            guitar_waveform = 0.95 * guitar_waveform / peak\n",
    "    \n",
    "    # Remove batch dimension if we added it\n",
    "    if b == 1:\n",
    "        guitar_waveform = guitar_waveform.squeeze(0)\n",
    "        \n",
    "    return guitar_waveform\n",
    "\n",
    "# STAGE 1: Extract all stems with htdemucs_ft\n",
    "print(\"STAGE 1: Separating with htdemucs_ft...\")\n",
    "model_stage1 = get_model(\"htdemucs_ft\")\n",
    "model_stage1.eval()\n",
    "model_stage1.to(device)\n",
    "\n",
    "# Prepare audio for first model\n",
    "waveform_stage1 = prepare_audio(sample_waveform, sample_rate, model_stage1.samplerate)\n",
    "waveform_stage1 = waveform_stage1.to(device)\n",
    "\n",
    "# Separate first stage\n",
    "with torch.no_grad():\n",
    "    sources_stage1 = apply_model(model_stage1, waveform_stage1.unsqueeze(0))[0]\n",
    "    sources_stage1 = sources_stage1.cpu()\n",
    "\n",
    "# Get the \"other\" stem\n",
    "other_index = model_stage1.sources.index('other') if 'other' in model_stage1.sources else None\n",
    "if other_index is None:\n",
    "    print(\"Warning: 'other' source not found in model 1. Using all non-guitar sources combined.\")\n",
    "    # Combine all sources except guitar to create \"other\"\n",
    "    if 'guitar' in model_stage1.sources:\n",
    "        guitar_index = model_stage1.sources.index('guitar')\n",
    "        all_sources = torch.zeros_like(sources_stage1[0])\n",
    "        for i, src in enumerate(model_stage1.sources):\n",
    "            if i != guitar_index:\n",
    "                all_sources += sources_stage1[i]\n",
    "        other_waveform = all_sources\n",
    "    else:\n",
    "        # If no guitar source, just use the first stem as \"other\"\n",
    "        other_waveform = sources_stage1[0]\n",
    "else:\n",
    "    other_waveform = sources_stage1[other_index]\n",
    "\n",
    "# Save the other stem\n",
    "other_file = os.path.join(output_dir, song_name, \"stage1_other.wav\")\n",
    "torchaudio.save(other_file, other_waveform, model_stage1.samplerate)\n",
    "print(f\"Saved 'other' stem to {other_file}\")\n",
    "\n",
    "# STAGE 2: Extract guitar from \"other\" stem using htdemucs_6s\n",
    "print(\"STAGE 2: Extracting guitar from 'other' using htdemucs_6s...\")\n",
    "model_stage2 = get_model(\"htdemucs_6s\")\n",
    "model_stage2.eval()\n",
    "model_stage2.to(device)\n",
    "\n",
    "# Prepare the \"other\" stem for second model\n",
    "other_waveform = prepare_audio(other_waveform, model_stage1.samplerate, model_stage2.samplerate)\n",
    "other_waveform = other_waveform.to(device)\n",
    "\n",
    "# Separate second stage\n",
    "with torch.no_grad():\n",
    "    sources_stage2 = apply_model(model_stage2, other_waveform.unsqueeze(0))[0]\n",
    "    sources_stage2 = sources_stage2.cpu()\n",
    "\n",
    "# Get the guitar from second separation\n",
    "if 'guitar' in model_stage2.sources:\n",
    "    guitar_index = model_stage2.sources.index('guitar')\n",
    "    extracted_guitar = sources_stage2[guitar_index]\n",
    "    \n",
    "    # Save the extracted guitar\n",
    "    guitar_file = os.path.join(output_dir, song_name, \"stage2_guitar_from_other.wav\")\n",
    "    torchaudio.save(guitar_file, extracted_guitar, model_stage2.samplerate)\n",
    "    print(f\"Saved extracted guitar to {guitar_file}\")\n",
    "    \n",
    "    # Enhance and save\n",
    "    enhanced_guitar = enhance_guitar(extracted_guitar, model_stage2.samplerate)\n",
    "    enhanced_file = os.path.join(output_dir, song_name, \"stage2_guitar_enhanced.wav\")\n",
    "    torchaudio.save(enhanced_file, enhanced_guitar, model_stage2.samplerate)\n",
    "    print(f\"Saved enhanced guitar to {enhanced_file}\")\n",
    "else:\n",
    "    print(\"Error: 'guitar' source not found in the second model\")\n",
    "\n",
    "# BONUS: Also get the guitar from the first separation for comparison\n",
    "if 'guitar' in model_stage1.sources:\n",
    "    guitar_index = model_stage1.sources.index('guitar')\n",
    "    original_guitar = sources_stage1[guitar_index]\n",
    "    \n",
    "    # Save the original guitar stem\n",
    "    orig_guitar_file = os.path.join(output_dir, song_name, \"stage1_original_guitar.wav\")\n",
    "    torchaudio.save(orig_guitar_file, original_guitar, model_stage1.samplerate)\n",
    "    print(f\"Saved original guitar stem to {orig_guitar_file}\")\n",
    "    \n",
    "    # Create an enhanced version of the original guitar\n",
    "    enhanced_orig_guitar = enhance_guitar(original_guitar, model_stage1.samplerate)\n",
    "    enhanced_orig_file = os.path.join(output_dir, song_name, \"stage1_original_guitar_enhanced.wav\")\n",
    "    torchaudio.save(enhanced_orig_file, enhanced_orig_guitar, model_stage1.samplerate)\n",
    "    print(f\"Saved enhanced original guitar to {enhanced_orig_file}\")\n",
    "    \n",
    "    # FINAL STEP: Try combining both guitar extractions for maximum clarity\n",
    "    # Resample if needed to match sample rates\n",
    "    if model_stage1.samplerate != model_stage2.samplerate:\n",
    "        original_guitar = torchaudio.functional.resample(\n",
    "            original_guitar, model_stage1.samplerate, model_stage2.samplerate)\n",
    "    \n",
    "    # Make sure shapes match\n",
    "    min_length = min(original_guitar.shape[1], extracted_guitar.shape[1])\n",
    "    original_guitar = original_guitar[:, :min_length]\n",
    "    extracted_guitar = extracted_guitar[:, :min_length]\n",
    "    \n",
    "    # Blend with 70% from first model, 30% from second model\n",
    "    combined_guitar = 0.7 * original_guitar + 0.3 * extracted_guitar\n",
    "    \n",
    "    # Enhance the combined result\n",
    "    enhanced_combined = enhance_guitar(combined_guitar, model_stage2.samplerate)\n",
    "    combined_file = os.path.join(output_dir, song_name, \"combined_guitar_enhanced.wav\")\n",
    "    torchaudio.save(combined_file, enhanced_combined, model_stage2.samplerate)\n",
    "    print(f\"Saved combined enhanced guitar to {combined_file}\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Audio File → Audio Analysis Model → Extract tempo/rhythm → \n",
    "Text Description → LLM → Strumming Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0122, -0.0032,  0.0040,  ..., -0.0447, -0.0525, -0.0599],\n",
       "         [ 0.0931,  0.1017,  0.0993,  ...,  0.0392,  0.0278,  0.0181]]),\n",
       " 44100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "def trim_audio(input_file, output_file=None, start_sec=0, end_sec=None):\n",
    "    \"\"\"\n",
    "    Trim audio file to specified start and end times.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file: Path to the input audio file\n",
    "    - output_file: Path to save the trimmed file (if None, returns without saving)\n",
    "    - start_sec: Start time in seconds\n",
    "    - end_sec: End time in seconds (if None, trims to the end of the file)\n",
    "    \n",
    "    Returns:\n",
    "    - trimmed_waveform: Tensor containing the trimmed audio\n",
    "    - sample_rate: Sample rate of the audio\n",
    "    \"\"\"\n",
    "    # Load the audio\n",
    "    waveform, sample_rate = torchaudio.load(input_file)\n",
    "    \n",
    "    # Convert time to samples\n",
    "    start_sample = int(start_sec * sample_rate)\n",
    "    end_sample = int(end_sec * sample_rate) if end_sec is not None else waveform.shape[1]\n",
    "    \n",
    "    # Trim the audio\n",
    "    trimmed_waveform = waveform[:, start_sample:end_sample]\n",
    "    \n",
    "    # Save the trimmed audio if output_file is provided\n",
    "    if output_file:\n",
    "        torchaudio.save(output_file, trimmed_waveform, sample_rate)\n",
    "    \n",
    "    return trimmed_waveform, sample_rate\n",
    "\n",
    "trim_audio('outputs/JeffBuckley-LoverYouShouldveComeOverAudio/stage2_guitar_enhanced.wav', start_sec = 45, end_sec = 100, output_file='outputs/JeffBuckley-LoverYouShouldveComeOverAudio/stage2_guitar_enhanced_cut.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"dynamic_guitar_strum_analysis.py – Notebook‑friendly version  (patched for librosa ≥ 1.0)\n",
    "================================================================================\n",
    "\n",
    "### 2025‑05‑03 patch\n",
    "* **Fixed** “TypeError: chroma_cqt() takes 0 positional arguments …” that appears with\n",
    "  librosa ≥ 1.0, which enforces *keyword‑only* arguments.  The call is now\n",
    "  `chroma_cqt(y=y_harm, sr=sr)`.\n",
    "* All other librosa calls remain version‑portable (tested 0.9 → 1.0).\n",
    "\n",
    "Copy this entire cell (or keep as `.py` module) and run:\n",
    "```python\n",
    "from dynamic_guitar_strum_analysis import run_in_notebook\n",
    "run_in_notebook('my_track.wav')\n",
    "```\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import warnings, json, math, itertools, sys\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Dataclasses\n",
    "# -----------------------------------------------------------------------------\n",
    "@dataclass\n",
    "class Strum:\n",
    "    time: float         # seconds\n",
    "    bar: int            # 1‑based measure number\n",
    "    sub_16: int         # 0‑15 inside bar (16‑note grid)\n",
    "    direction: str      # \"D\" or \"U\"\n",
    "    velocity: float     # RMS energy normalised 0‑1\n",
    "    kind: str           # \"NOTE\" | \"CHORD\" | \"NONE\"\n",
    "    label: str          # e.g. \"A#\", \"Dm\", \"N\"\n",
    "\n",
    "@dataclass\n",
    "class BarSummary:\n",
    "    bar: int\n",
    "    bit_pattern: str       # 16 chars 0/1\n",
    "    down_up: str           # 16 chars D/U/-\n",
    "    mean_vel: float\n",
    "    chords: List[str]\n",
    "\n",
    "@dataclass\n",
    "class Section:\n",
    "    start_bar: int\n",
    "    end_bar: int\n",
    "    pattern_bits: str\n",
    "    chords: List[str]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper functions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def spectral_centroid_direction(y: np.ndarray, sr: int, onset_frames: np.ndarray) -> List[str]:\n",
    "    \"\"\"Classify Down/Up strokes by sign of spectral‑centroid slope around attack.\"\"\"\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=512)[0]\n",
    "    dirs = []\n",
    "    for f in onset_frames:\n",
    "        a = max(0, f-2)\n",
    "        b = min(len(cent)-1, f+2)\n",
    "        dirs.append('D' if cent[b] - cent[a] < 0 else 'U')\n",
    "    return dirs\n",
    "\n",
    "\n",
    "def chord_sequence_madmom(audio_path: str, sr: int) -> Dict[int, str]:\n",
    "    \"\"\"Return dict {beat_index: chord_label}.  If madmom missing, fallback returns empty dict.\"\"\"\n",
    "    try:\n",
    "        from madmom.features.chords import CNNChordRecognitionProcessor\n",
    "        proc = CNNChordRecognitionProcessor()\n",
    "        chords = proc(audio_path)  # array [ onset, offset, label ]\n",
    "        # Quantise onsets to beat indices\n",
    "        y, _ = librosa.load(audio_path, sr=sr)\n",
    "        tempo, beats = librosa.beat.beat_track(y=y, sr=sr, units='frames')\n",
    "        beat_times = librosa.frames_to_time(beats, sr=sr)\n",
    "        out = {}\n",
    "        for onset, _, lab in chords:\n",
    "            idx = int(np.argmin(np.abs(beat_times - onset)))\n",
    "            out[idx] = lab.split('/')[0]  # remove inversions \"C/E\" → \"C\"\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        warnings.warn(\"madmom not available – chord labels will be rough.\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def label_note_or_chord(chroma: np.ndarray, frame: int, note_names=None) -> Tuple[str, str]:\n",
    "    \"\"\"Return (kind, label) where kind ∈ {NOTE, CHORD, NONE}\"\"\"\n",
    "    if note_names is None:\n",
    "        note_names = ['C','C#','D','D#','E','F','F#','G','G#','A','A#','B']\n",
    "    frame = max(0, min(chroma.shape[1]-1, frame))\n",
    "    v = chroma[:, frame]\n",
    "    if v.max() < 1e-6:\n",
    "        return 'NONE', 'N'\n",
    "    thresh = 0.45 * v.max()\n",
    "    strong = np.where(v >= thresh)[0]\n",
    "    if len(strong) == 1:\n",
    "        return 'NOTE', note_names[strong[0]]\n",
    "    # chord heuristic\n",
    "    root = strong[0]\n",
    "    intervals = {(n-root) % 12 for n in strong}\n",
    "    if {4,7}.issubset(intervals):\n",
    "        qual = 'maj'\n",
    "    elif {3,7}.issubset(intervals):\n",
    "        qual = 'min'\n",
    "    else:\n",
    "        qual = ''\n",
    "    return 'CHORD', f\"{note_names[root]}{qual}\"\n",
    "\n",
    "\n",
    "def levenshtein(a: str, b: str) -> int:\n",
    "    import Levenshtein\n",
    "    return Levenshtein.distance(a, b)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main analysis function\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def analyse_audio(audio_path: str, return_dataframes: bool=True):\n",
    "    \"\"\"Analyse a (separated) guitar track and return dict OR DataFrames.\"\"\"\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    print(f\"Loaded {audio_path}  ({len(y)/sr:.1f}s, {sr} Hz)\")\n",
    "\n",
    "    # ----- Beat tracking ------------------------------------------------------\n",
    "    tmp_tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr, units='frames', tightness=400)\n",
    "    # librosa ≥1.0 may return a 0‑D ndarray; coerce to plain float\n",
    "    tempo = float(np.atleast_1d(tmp_tempo)[0])\n",
    "    beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "    step = 60/tempo/4          # 16‑note subdivision\n",
    "    grid_times = np.arange(beat_times[0], beat_times[-1]+step, step)\n",
    "\n",
    "    # ----- Onset detection ----------------------------------------------------\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "\n",
    "    # ----- Prepare harmonic signal + chroma ----------------------------------\n",
    "    y_harm, _ = librosa.effects.hpss(y)\n",
    "    chroma = librosa.feature.chroma_cqt(y=y_harm, sr=sr)  # keyword args for librosa>=1.0\n",
    "\n",
    "    # ----- Directions and velocities -----------------------------------------\n",
    "    directions = spectral_centroid_direction(y, sr, onset_frames)\n",
    "    rms = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)[0]\n",
    "\n",
    "    # ----- Chords (per beat) --------------------------------------------------\n",
    "    beat_chords = chord_sequence_madmom(audio_path, sr)\n",
    "\n",
    "    # ----- Build Strum list ---------------------------------------------------\n",
    "    strums: List[Strum] = []\n",
    "    for i, (t, f) in enumerate(zip(onset_times, onset_frames)):\n",
    "        gidx = int(np.argmin(np.abs(grid_times - t)))\n",
    "        bar_idx = gidx // 16\n",
    "        sub16   = gidx % 16\n",
    "        vel = float(rms[min(len(rms)-1, f)])\n",
    "        frame_for_chroma = librosa.time_to_frames(t, sr=sr)\n",
    "        kind, label = label_note_or_chord(chroma, frame_for_chroma)\n",
    "        strums.append(Strum(time=float(t), bar=bar_idx+1, sub_16=sub16,\n",
    "                            direction=directions[i], velocity=vel,\n",
    "                            kind=kind, label=label))\n",
    "\n",
    "    # ----- Summarise bars -----------------------------------------------------\n",
    "    bars: Dict[int, BarSummary] = {}\n",
    "    for s in strums:\n",
    "        if s.bar not in bars:\n",
    "            bars[s.bar] = BarSummary(bar=s.bar,\n",
    "                                     bit_pattern=['0']*16,\n",
    "                                     down_up=['-']*16,\n",
    "                                     mean_vel=0.0,\n",
    "                                     chords=[])\n",
    "        b = bars[s.bar]\n",
    "        b.bit_pattern[s.sub_16] = '1'\n",
    "        b.down_up[s.sub_16]     = s.direction\n",
    "        b.mean_vel += s.velocity\n",
    "        if s.kind == 'CHORD':\n",
    "            b.chords.append(s.label)\n",
    "    for b in bars.values():\n",
    "        hits = b.bit_pattern.count('1')\n",
    "        b.mean_vel /= max(1, hits)\n",
    "        b.bit_pattern = ''.join(b.bit_pattern)\n",
    "        b.down_up    = ' '.join(b.down_up)\n",
    "        b.chords     = sorted(set(b.chords))\n",
    "\n",
    "    # ----- Group Sections -----------------------------------------------------\n",
    "    bar_ordered = [bars[k] for k in sorted(bars)]\n",
    "    sections: List[Section] = []\n",
    "    if bar_ordered:\n",
    "        cur = Section(start_bar=bar_ordered[0].bar, end_bar=bar_ordered[0].bar,\n",
    "                       pattern_bits=bar_ordered[0].bit_pattern,\n",
    "                       chords=bar_ordered[0].chords)\n",
    "        for b in bar_ordered[1:]:\n",
    "            d = levenshtein(b.bit_pattern, cur.pattern_bits)\n",
    "            if d <= 2:\n",
    "                cur.end_bar = b.bar\n",
    "                cur.chords  = sorted(set(cur.chords + b.chords))\n",
    "            else:\n",
    "                sections.append(cur)\n",
    "                cur = Section(start_bar=b.bar, end_bar=b.bar,\n",
    "                               pattern_bits=b.bit_pattern, chords=b.chords)\n",
    "        sections.append(cur)\n",
    "\n",
    "    # ----- Return -------------------------------------------------------------\n",
    "    if return_dataframes:\n",
    "        df_strums = pd.DataFrame([asdict(s) for s in strums])\n",
    "        df_bars   = pd.DataFrame([asdict(b) for b in bar_ordered])\n",
    "        df_secs   = pd.DataFrame([asdict(s) for s in sections])\n",
    "        return {\n",
    "            'tempo_bpm': tempo,\n",
    "            'strums': df_strums,\n",
    "            'bars': df_bars,\n",
    "            'sections': df_secs\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'tempo_bpm': float(tempo),\n",
    "            'strums': [asdict(s) for s in strums],\n",
    "            'bars': [asdict(b) for b in bar_ordered],\n",
    "            'sections': [asdict(s) for s in sections]\n",
    "        }\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Notebook helper -------------------------------------------------------------\n",
    "\n",
    "def run_in_notebook(audio_path: str):\n",
    "    \"\"\"Analyse `audio_path` and pretty‑print DataFrames (Jupyter only).\"\"\"\n",
    "    data = analyse_audio(audio_path, return_dataframes=True)\n",
    "    print(f\"Tempo ≈ {data['tempo_bpm']:.1f} BPM\")\n",
    "    from IPython.display import display\n",
    "    print(\"\\nStrums (first 10):\")\n",
    "    display(data['strums'].head(10))\n",
    "    print(\"\\nBar summaries:\")\n",
    "    display(data['bars'].head())\n",
    "    print(\"\\nSections:\")\n",
    "    display(data['sections'])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded outputs/JeffBuckley-LoverYouShouldveComeOverAudio/stage2_guitar_enhanced_cut.wav  (55.0s, 44100 Hz)\n",
      "Tempo ≈ 120.2 BPM\n",
      "\n",
      "Strums (first 10):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sw/2zngpy_n771gjrnyvn_x3mlw0000gn/T/ipykernel_18655/3356485487.py:86: UserWarning:\n",
      "\n",
      "madmom not available – chord labels will be rough.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>bar</th>\n",
       "      <th>sub_16</th>\n",
       "      <th>direction</th>\n",
       "      <th>velocity</th>\n",
       "      <th>kind</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.081270</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "      <td>0.033050</td>\n",
       "      <td>CHORD</td>\n",
       "      <td>Dmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.592109</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "      <td>0.031410</td>\n",
       "      <td>NOTE</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.893968</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "      <td>0.016532</td>\n",
       "      <td>CHORD</td>\n",
       "      <td>Dmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.091338</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "      <td>0.026228</td>\n",
       "      <td>CHORD</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.404807</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>U</td>\n",
       "      <td>0.015962</td>\n",
       "      <td>CHORD</td>\n",
       "      <td>Dmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.439637</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>0.017902</td>\n",
       "      <td>CHORD</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.602177</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>D</td>\n",
       "      <td>0.030112</td>\n",
       "      <td>CHORD</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.078186</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>U</td>\n",
       "      <td>0.025283</td>\n",
       "      <td>CHORD</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.391655</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>U</td>\n",
       "      <td>0.017051</td>\n",
       "      <td>CHORD</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.879274</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>U</td>\n",
       "      <td>0.013744</td>\n",
       "      <td>CHORD</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time  bar  sub_16 direction  velocity   kind label\n",
       "0  0.081270    1       0         U  0.033050  CHORD  Dmaj\n",
       "1  0.592109    1       0         U  0.031410   NOTE     D\n",
       "2  0.893968    1       0         U  0.016532  CHORD  Dmaj\n",
       "3  1.091338    1       0         U  0.026228  CHORD     D\n",
       "4  1.404807    1       2         U  0.015962  CHORD  Dmaj\n",
       "5  1.439637    1       3         D  0.017902  CHORD     D\n",
       "6  1.602177    1       4         D  0.030112  CHORD     D\n",
       "7  2.078186    1       8         U  0.025283  CHORD     D\n",
       "8  2.391655    1      10         U  0.017051  CHORD     D\n",
       "9  2.879274    1      14         U  0.013744  CHORD     D"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bar summaries:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bar</th>\n",
       "      <th>bit_pattern</th>\n",
       "      <th>down_up</th>\n",
       "      <th>mean_vel</th>\n",
       "      <th>chords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1011100010100010</td>\n",
       "      <td>U - U D D - - - U - U - - - U -</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>[D, Dmaj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1000101010101000</td>\n",
       "      <td>U - - - U - D - U - U - D - - -</td>\n",
       "      <td>0.040871</td>\n",
       "      <td>[C, D]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1010101110001010</td>\n",
       "      <td>U - U - D - U D U - - - U - U -</td>\n",
       "      <td>0.019006</td>\n",
       "      <td>[C, E]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1010100010101001</td>\n",
       "      <td>U - D - U - - - D - U - D - - U</td>\n",
       "      <td>0.027077</td>\n",
       "      <td>[D, E, Emin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1010100110111000</td>\n",
       "      <td>U - U - U - - D U - U D U - - -</td>\n",
       "      <td>0.023267</td>\n",
       "      <td>[Cmaj, D, E, Emin]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bar       bit_pattern                          down_up  mean_vel  \\\n",
       "0    1  1011100010100010  U - U D D - - - U - U - - - U -  0.032468   \n",
       "1    2  1000101010101000  U - - - U - D - U - U - D - - -  0.040871   \n",
       "2    3  1010101110001010  U - U - D - U D U - - - U - U -  0.019006   \n",
       "3    4  1010100010101001  U - D - U - - - D - U - D - - U  0.027077   \n",
       "4    5  1010100110111000  U - U - U - - D U - U D U - - -  0.023267   \n",
       "\n",
       "               chords  \n",
       "0           [D, Dmaj]  \n",
       "1              [C, D]  \n",
       "2              [C, E]  \n",
       "3        [D, E, Emin]  \n",
       "4  [Cmaj, D, E, Emin]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sections:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_bar</th>\n",
       "      <th>end_bar</th>\n",
       "      <th>pattern_bits</th>\n",
       "      <th>chords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1011100010100010</td>\n",
       "      <td>[D, Dmaj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1000101010101000</td>\n",
       "      <td>[C, D]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1010101110001010</td>\n",
       "      <td>[C, E]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1010100010101001</td>\n",
       "      <td>[D, E, Emin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1010100110111000</td>\n",
       "      <td>[Cmaj, D, E, Emin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1010001010001001</td>\n",
       "      <td>[D, Dmaj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1001100010110010</td>\n",
       "      <td>[D, Dmaj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1000101110101000</td>\n",
       "      <td>[C, Cmaj, D]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1010101110001011</td>\n",
       "      <td>[C, E]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1010100010011001</td>\n",
       "      <td>[C, E, Emin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>1001100110011001</td>\n",
       "      <td>[C, Cmaj, D, Dmaj, E]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1010100010011000</td>\n",
       "      <td>[D, Dmaj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1001100100011000</td>\n",
       "      <td>[C, C#, D]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>1001100110001001</td>\n",
       "      <td>[E, Emin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>1001100010010001</td>\n",
       "      <td>[E]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>1100100110011000</td>\n",
       "      <td>[C, E]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>1001100010000001</td>\n",
       "      <td>[C, C#, E]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>1101100110011000</td>\n",
       "      <td>[A#, D, Dmaj, F#]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>1001100110001001</td>\n",
       "      <td>[D, Dmaj, F#]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>1001110010011001</td>\n",
       "      <td>[C, D]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>1101100110001001</td>\n",
       "      <td>[C, D, Dmaj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>1001100111010001</td>\n",
       "      <td>[D, Dmaj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>1001100010011001</td>\n",
       "      <td>[D, Dmaj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>1001100100011000</td>\n",
       "      <td>[C, C#, D]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>1001100110000100</td>\n",
       "      <td>[D, E, Emin]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    start_bar  end_bar      pattern_bits                 chords\n",
       "0           1        1  1011100010100010              [D, Dmaj]\n",
       "1           2        2  1000101010101000                 [C, D]\n",
       "2           3        3  1010101110001010                 [C, E]\n",
       "3           4        4  1010100010101001           [D, E, Emin]\n",
       "4           5        5  1010100110111000     [Cmaj, D, E, Emin]\n",
       "5           6        6  1010001010001001              [D, Dmaj]\n",
       "6           7        7  1001100010110010              [D, Dmaj]\n",
       "7           8        8  1000101110101000           [C, Cmaj, D]\n",
       "8           9        9  1010101110001011                 [C, E]\n",
       "9          10       10  1010100010011001           [C, E, Emin]\n",
       "10         11       12  1001100110011001  [C, Cmaj, D, Dmaj, E]\n",
       "11         13       13  1010100010011000              [D, Dmaj]\n",
       "12         14       14  1001100100011000             [C, C#, D]\n",
       "13         15       15  1001100110001001              [E, Emin]\n",
       "14         16       16  1001100010010001                    [E]\n",
       "15         17       17  1100100110011000                 [C, E]\n",
       "16         18       19  1001100010000001             [C, C#, E]\n",
       "17         20       20  1101100110011000      [A#, D, Dmaj, F#]\n",
       "18         21       21  1001100110001001          [D, Dmaj, F#]\n",
       "19         22       22  1001110010011001                 [C, D]\n",
       "20         23       23  1101100110001001           [C, D, Dmaj]\n",
       "21         24       24  1001100111010001              [D, Dmaj]\n",
       "22         25       25  1001100010011001              [D, Dmaj]\n",
       "23         26       26  1001100100011000             [C, C#, D]\n",
       "24         27       27  1001100110000100           [D, E, Emin]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_path = 'outputs/JeffBuckley-LoverYouShouldveComeOverAudio/stage2_guitar_enhanced_cut.wav'\n",
    "data = run_in_notebook(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert pandas DataFrames and NumPy arrays to JSON-serializable types\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        return obj.to_dict(orient='records')  # Convert DataFrame to list of dictionaries\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.integer, np.floating)):\n",
    "        return float(obj) if isinstance(obj, np.floating) else int(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list) or isinstance(obj, tuple):\n",
    "        return [convert_to_serializable(i) for i in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert the data and save to JSON\n",
    "serializable_data = convert_to_serializable(data)\n",
    "\n",
    "# Save to file\n",
    "with open('outputs/JeffBuckley-LoverYouShouldveComeOverAudio/guitar_data.json', 'w') as f:\n",
    "    json.dump(serializable_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seperator.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
