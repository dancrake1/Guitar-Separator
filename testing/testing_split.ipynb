{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "song_path = 'Fontaines_D.C._-_Bug_Official_Video.mp3'\n",
    "\n",
    "song_name = os.path.splitext(os.path.basename(song_path))[0].replace('_', '')\n",
    "os.makedirs(f'{output_dir}/{song_name}', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio from Fontaines_D.C._-_Bug_Official_Video.mp3...\n",
      "Loaded audio with shape torch.Size([2, 9854921]) and sample rate 48000\n",
      "Using device: mps\n",
      "STAGE 1: Separating with htdemucs_ft...\n",
      "Saved 'other' stem to outputs/FontainesD.C.-BugOfficialVideo/stage1_other.wav\n",
      "STAGE 2: Extracting guitar from 'other' using htdemucs_6s...\n",
      "Saved extracted guitar to outputs/FontainesD.C.-BugOfficialVideo/stage2_guitar_from_other.wav\n",
      "Saved enhanced guitar to outputs/FontainesD.C.-BugOfficialVideo/stage2_guitar_enhanced.wav\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from demucs.pretrained import get_model\n",
    "from demucs.apply import apply_model\n",
    "\n",
    "\n",
    "\n",
    "# Load the audio\n",
    "print(f\"Loading audio from {song_path}...\")\n",
    "sample_waveform, sample_rate = torchaudio.load(song_path)\n",
    "print(f\"Loaded audio with shape {sample_waveform.shape} and sample rate {sample_rate}\")\n",
    "\n",
    "# Determine device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def prepare_audio(waveform, source_sr, target_sr):\n",
    "    \"\"\"Prepare audio for model input\"\"\"\n",
    "    # Resample if needed\n",
    "    if source_sr != target_sr:\n",
    "        waveform = torchaudio.functional.resample(waveform, source_sr, target_sr)\n",
    "        \n",
    "    # Handle channels\n",
    "    if waveform.shape[0] > 2:\n",
    "        waveform = waveform[:2, :]\n",
    "    elif waveform.shape[0] == 1:\n",
    "        waveform = torch.cat([waveform, waveform], dim=0)\n",
    "        \n",
    "    return waveform\n",
    "\n",
    "def enhance_guitar(guitar_waveform, sample_rate):\n",
    "    \"\"\"Enhance guitar with filters and transient processing\"\"\"\n",
    "    # Add batch dimension if needed\n",
    "    if guitar_waveform.dim() == 2:\n",
    "        guitar_waveform = guitar_waveform.unsqueeze(0)\n",
    "        \n",
    "    # Now we can safely unpack dimensions\n",
    "    b, c, t = guitar_waveform.shape\n",
    "    \n",
    "    # FFT for frequency domain processing\n",
    "    guitar_waveform_freq = torch.fft.rfft(guitar_waveform, dim=2)\n",
    "    \n",
    "    # Create high-pass filter (reduce below 80Hz)\n",
    "    freqs = torch.fft.rfftfreq(t, d=1/sample_rate)\n",
    "    high_pass = (1 - torch.exp(-freqs/80))\n",
    "    \n",
    "    # Create mid boost around 2-4kHz (presence)\n",
    "    mid_boost = 1.0 + 0.5 * torch.exp(-((freqs - 3000)/500)**2)\n",
    "    \n",
    "    # Apply filters\n",
    "    filter_curve = high_pass.view(1, 1, -1) * mid_boost.view(1, 1, -1)\n",
    "    guitar_waveform_freq *= filter_curve\n",
    "    \n",
    "    # Back to time domain\n",
    "    guitar_waveform = torch.fft.irfft(guitar_waveform_freq, n=t, dim=2)\n",
    "    \n",
    "    # Apply subtle compression\n",
    "    peak = guitar_waveform.abs().max()\n",
    "    if peak > 0:\n",
    "        # Simple soft knee compression\n",
    "        threshold = 0.7\n",
    "        ratio = 3.0\n",
    "        gain = 1.2\n",
    "        \n",
    "        above_thresh = (guitar_waveform.abs() > threshold * peak).float()\n",
    "        comp_factor = 1.0 - above_thresh * (1.0 - 1.0/ratio) * (guitar_waveform.abs() - threshold * peak) / (peak * (1.0 - threshold))\n",
    "        guitar_waveform = guitar_waveform * comp_factor * gain\n",
    "        \n",
    "        # Final limiter\n",
    "        peak = guitar_waveform.abs().max()\n",
    "        if peak > 0.95:\n",
    "            guitar_waveform = 0.95 * guitar_waveform / peak\n",
    "    \n",
    "    # Remove batch dimension if we added it\n",
    "    if b == 1:\n",
    "        guitar_waveform = guitar_waveform.squeeze(0)\n",
    "        \n",
    "    return guitar_waveform\n",
    "\n",
    "# STAGE 1: Extract all stems with htdemucs_ft\n",
    "print(\"STAGE 1: Separating with htdemucs_ft...\")\n",
    "model_stage1 = get_model(\"htdemucs_ft\")\n",
    "model_stage1.eval()\n",
    "model_stage1.to(device)\n",
    "\n",
    "# Prepare audio for first model\n",
    "waveform_stage1 = prepare_audio(sample_waveform, sample_rate, model_stage1.samplerate)\n",
    "waveform_stage1 = waveform_stage1.to(device)\n",
    "\n",
    "# Separate first stage\n",
    "with torch.no_grad():\n",
    "    sources_stage1 = apply_model(model_stage1, waveform_stage1.unsqueeze(0))[0]\n",
    "    sources_stage1 = sources_stage1.cpu()\n",
    "\n",
    "# Get the \"other\" stem\n",
    "other_index = model_stage1.sources.index('other') if 'other' in model_stage1.sources else None\n",
    "if other_index is None:\n",
    "    print(\"Warning: 'other' source not found in model 1. Using all non-guitar sources combined.\")\n",
    "    # Combine all sources except guitar to create \"other\"\n",
    "    if 'guitar' in model_stage1.sources:\n",
    "        guitar_index = model_stage1.sources.index('guitar')\n",
    "        all_sources = torch.zeros_like(sources_stage1[0])\n",
    "        for i, src in enumerate(model_stage1.sources):\n",
    "            if i != guitar_index:\n",
    "                all_sources += sources_stage1[i]\n",
    "        other_waveform = all_sources\n",
    "    else:\n",
    "        # If no guitar source, just use the first stem as \"other\"\n",
    "        other_waveform = sources_stage1[0]\n",
    "else:\n",
    "    other_waveform = sources_stage1[other_index]\n",
    "\n",
    "# Save the other stem\n",
    "other_file = os.path.join(output_dir, song_name, \"stage1_other.wav\")\n",
    "torchaudio.save(other_file, other_waveform, model_stage1.samplerate)\n",
    "print(f\"Saved 'other' stem to {other_file}\")\n",
    "\n",
    "# STAGE 2: Extract guitar from \"other\" stem using htdemucs_6s\n",
    "print(\"STAGE 2: Extracting guitar from 'other' using htdemucs_6s...\")\n",
    "model_stage2 = get_model(\"htdemucs_6s\")\n",
    "model_stage2.eval()\n",
    "model_stage2.to(device)\n",
    "\n",
    "# Prepare the \"other\" stem for second model\n",
    "other_waveform = prepare_audio(other_waveform, model_stage1.samplerate, model_stage2.samplerate)\n",
    "other_waveform = other_waveform.to(device)\n",
    "\n",
    "# Separate second stage\n",
    "with torch.no_grad():\n",
    "    sources_stage2 = apply_model(model_stage2, other_waveform.unsqueeze(0))[0]\n",
    "    sources_stage2 = sources_stage2.cpu()\n",
    "\n",
    "# Get the guitar from second separation\n",
    "if 'guitar' in model_stage2.sources:\n",
    "    guitar_index = model_stage2.sources.index('guitar')\n",
    "    extracted_guitar = sources_stage2[guitar_index]\n",
    "    \n",
    "    # Save the extracted guitar\n",
    "    guitar_file = os.path.join(output_dir, song_name, \"stage2_guitar_from_other.wav\")\n",
    "    torchaudio.save(guitar_file, extracted_guitar, model_stage2.samplerate)\n",
    "    print(f\"Saved extracted guitar to {guitar_file}\")\n",
    "    \n",
    "    # Enhance and save\n",
    "    enhanced_guitar = enhance_guitar(extracted_guitar, model_stage2.samplerate)\n",
    "    enhanced_file = os.path.join(output_dir, song_name, \"stage2_guitar_enhanced.wav\")\n",
    "    torchaudio.save(enhanced_file, enhanced_guitar, model_stage2.samplerate)\n",
    "    print(f\"Saved enhanced guitar to {enhanced_file}\")\n",
    "else:\n",
    "    print(\"Error: 'guitar' source not found in the second model\")\n",
    "\n",
    "# BONUS: Also get the guitar from the first separation for comparison\n",
    "if 'guitar' in model_stage1.sources:\n",
    "    guitar_index = model_stage1.sources.index('guitar')\n",
    "    original_guitar = sources_stage1[guitar_index]\n",
    "    \n",
    "    # Save the original guitar stem\n",
    "    orig_guitar_file = os.path.join(output_dir, song_name, \"stage1_original_guitar.wav\")\n",
    "    torchaudio.save(orig_guitar_file, original_guitar, model_stage1.samplerate)\n",
    "    print(f\"Saved original guitar stem to {orig_guitar_file}\")\n",
    "    \n",
    "    # Create an enhanced version of the original guitar\n",
    "    enhanced_orig_guitar = enhance_guitar(original_guitar, model_stage1.samplerate)\n",
    "    enhanced_orig_file = os.path.join(output_dir, song_name, \"stage1_original_guitar_enhanced.wav\")\n",
    "    torchaudio.save(enhanced_orig_file, enhanced_orig_guitar, model_stage1.samplerate)\n",
    "    print(f\"Saved enhanced original guitar to {enhanced_orig_file}\")\n",
    "    \n",
    "    # FINAL STEP: Try combining both guitar extractions for maximum clarity\n",
    "    # Resample if needed to match sample rates\n",
    "    if model_stage1.samplerate != model_stage2.samplerate:\n",
    "        original_guitar = torchaudio.functional.resample(\n",
    "            original_guitar, model_stage1.samplerate, model_stage2.samplerate)\n",
    "    \n",
    "    # Make sure shapes match\n",
    "    min_length = min(original_guitar.shape[1], extracted_guitar.shape[1])\n",
    "    original_guitar = original_guitar[:, :min_length]\n",
    "    extracted_guitar = extracted_guitar[:, :min_length]\n",
    "    \n",
    "    # Blend with 70% from first model, 30% from second model\n",
    "    combined_guitar = 0.7 * original_guitar + 0.3 * extracted_guitar\n",
    "    \n",
    "    # Enhance the combined result\n",
    "    enhanced_combined = enhance_guitar(combined_guitar, model_stage2.samplerate)\n",
    "    combined_file = os.path.join(output_dir, song_name, \"combined_guitar_enhanced.wav\")\n",
    "    torchaudio.save(combined_file, enhanced_combined, model_stage2.samplerate)\n",
    "    print(f\"Saved combined enhanced guitar to {combined_file}\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Audio File → Audio Analysis Model → Extract tempo/rhythm → \n",
    "Text Description → LLM → Strumming Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.4423e-03, -1.4287e-03, -1.4154e-03,  ..., -1.6368e-01,\n",
       "          -1.2318e-01, -9.4925e-02],\n",
       "         [-1.9065e-04, -1.9043e-04, -1.8271e-04,  ..., -2.3660e-01,\n",
       "          -2.3136e-01, -2.3273e-01]]),\n",
       " 44100)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "def trim_audio(input_file, output_file=None, start_sec=0, end_sec=None):\n",
    "    \"\"\"\n",
    "    Trim audio file to specified start and end times.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file: Path to the input audio file\n",
    "    - output_file: Path to save the trimmed file (if None, returns without saving)\n",
    "    - start_sec: Start time in seconds\n",
    "    - end_sec: End time in seconds (if None, trims to the end of the file)\n",
    "    \n",
    "    Returns:\n",
    "    - trimmed_waveform: Tensor containing the trimmed audio\n",
    "    - sample_rate: Sample rate of the audio\n",
    "    \"\"\"\n",
    "    # Load the audio\n",
    "    waveform, sample_rate = torchaudio.load(input_file)\n",
    "    \n",
    "    # Convert time to samples\n",
    "    start_sample = int(start_sec * sample_rate)\n",
    "    end_sample = int(end_sec * sample_rate) if end_sec is not None else waveform.shape[1]\n",
    "    \n",
    "    # Trim the audio\n",
    "    trimmed_waveform = waveform[:, start_sample:end_sample]\n",
    "    \n",
    "    # Save the trimmed audio if output_file is provided\n",
    "    if output_file:\n",
    "        torchaudio.save(output_file, trimmed_waveform, sample_rate)\n",
    "    \n",
    "    return trimmed_waveform, sample_rate\n",
    "\n",
    "trim_audio(f'outputs/{song_name}/stage2_guitar_enhanced.wav', start_sec = 10, end_sec = 100, output_file=f'outputs/{song_name}/stage2_guitar_enhanced_cut.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"dynamic_guitar_strum_analysis.py  –  chords & notes processed **independently**\n",
    "==========================================================================\n",
    "\n",
    "* Deep‑Chroma (madmom) → **chord timeline** (segment‑level)\n",
    "* torchcrepe (or pyin fallback) → **note timeline** (event‑level)\n",
    "* Original beat‑aligned strum/chord/bar/section logic LEFT INTACT so your UI\n",
    "  keeps working, but we **do not overwrite chords with notes** anymore.\n",
    "* Extra DataFrames returned: `chords_timeline`, `notes_timeline`.\n",
    "* One label per DataFrame → no clobbering; overlapping times are fine.\n",
    "\n",
    "Install (CPU only):\n",
    "    pip install librosa madmom torch torchaudio torchcrepe pandas tqdm \"numpy<1.24\"\n",
    "\n",
    "If you later add a CUDA PyTorch wheel, torchcrepe will use it automatically.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import warnings, traceback\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np, librosa, pandas as pd, torch, torchaudio\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Dataclasses (unchanged for strums/bars/sections)\n",
    "# --------------------------------------------------------------------------\n",
    "@dataclass\n",
    "class Strum:\n",
    "    time: float; bar: int; sub_16: int; direction: str; velocity: float; kind: str; label: str   # kind NOTE|CHORD|NONE\n",
    "\n",
    "@dataclass\n",
    "class BarSummary:\n",
    "    bar: int; bit_pattern: str; down_up: str; mean_vel: float; chords: List[str]\n",
    "\n",
    "@dataclass\n",
    "class Section:\n",
    "    start_bar: int; end_bar: int; pattern_bits: str; chords: List[str]\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Polyphony helper (v2) -----------------------------------------------------\n",
    "# --------------------------------------------------------------------------\n",
    "def _is_polyphonic(mag_db: np.ndarray,\n",
    "                   peak_db: float = -35.0,\n",
    "                   min_peaks: int = 3,\n",
    "                   dom_margin: float = 8.0) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if the frame is almost certainly a chord.\n",
    "    Override: if the strongest peak is `dom_margin` dB louder than the\n",
    "    2nd‑strongest, treat as monophonic even when `min_peaks` is exceeded.\n",
    "    \"\"\"\n",
    "    strong = mag_db > peak_db\n",
    "    # indices of strong bins\n",
    "    idx = np.flatnonzero(strong)\n",
    "    if len(idx) == 0:\n",
    "        return False\n",
    "\n",
    "    # dominant‑peak override\n",
    "    sorted_db = np.sort(mag_db[idx])\n",
    "    if len(sorted_db) >= 2 and sorted_db[-1] - sorted_db[-2] >= dom_margin:\n",
    "        return False                            # clearly one string dominates\n",
    "\n",
    "    # otherwise count distinct strong groups\n",
    "    groups = np.split(strong, np.flatnonzero(~strong) + 1)\n",
    "    n_peaks = sum(g.any() for g in groups)\n",
    "    return n_peaks >= min_peaks\n",
    "\n",
    "\n",
    "def spectral_centroid_direction(y: np.ndarray, sr: int, onset_frames: np.ndarray) -> List[str]:\n",
    "    \"\"\"Classify Down/Up strokes by sign of spectral‑centroid slope around attack.\"\"\"\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=512)[0]\n",
    "    dirs = []\n",
    "    for f in onset_frames:\n",
    "        a = max(0, f-2)\n",
    "        b = min(len(cent)-1, f+2)\n",
    "        dirs.append('D' if cent[b] - cent[a] < 0 else 'U')\n",
    "    return dirs\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 1.  Deep‑Chroma chord timeline (segment‑level) ----------------------------\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def chord_timeline(audio_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Return DF with columns [start, end, chord].\"\"\"\n",
    "    from madmom.audio.chroma import DeepChromaProcessor\n",
    "    from madmom.features.chords import DeepChromaChordRecognitionProcessor\n",
    "    chroma = DeepChromaProcessor()(audio_path)\n",
    "    segs   = DeepChromaChordRecognitionProcessor()(chroma)\n",
    "    df = pd.DataFrame(segs, columns=[\"start\", \"end\", \"label\"])\n",
    "    df[\"label\"] = df[\"label\"].str.split(\"/\").str[0]\n",
    "    return df\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.  torchcrepe / pyin note timeline (event‑level) -------------------------\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.  Gated note timeline (event‑level, single‑string only) -----------------\n",
    "# --------------------------------------------------------------------------\n",
    "def note_timeline(audio_path: str,\n",
    "                  hop_s: float = 0.01,\n",
    "                  conf_thresh: float = .8,\n",
    "                  peak_db: float = -52,\n",
    "                  min_peaks: int = 4,\n",
    "                  device: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DF [time, note] containing ONLY intentionally plucked single‑string notes.\n",
    "    Strategy:\n",
    "      1. Find onsets (same settings as the rest of the pipeline).\n",
    "      2. For each onset grab a 40 ms slice and CQT → run _is_polyphonic().\n",
    "      3. Only if that slice is *not* polyphonic do we call torchcrepe/pyin.\n",
    "    \"\"\"\n",
    "    import torchcrepe, torchcrepe.decode as tcd, torchcrepe.filter as tcf\n",
    "\n",
    "    # --- load + onset detection ------------------------------------------\n",
    "    y, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "    onset_env   = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)\n",
    "    onset_times  = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "\n",
    "    # --- constants --------------------------------------------------------\n",
    "    slice_ms = 40                                   # analysis window\n",
    "    slice_samps = int(sr * slice_ms / 1000.0)\n",
    "    hop_len = int(round(16000 * hop_s))\n",
    "\n",
    "    # --- prepare harmonic layer & CQT for gate ---------------------------\n",
    "    y_harm, _ = librosa.effects.hpss(y)             # helps both gates\n",
    "    C = np.abs(librosa.cqt(y_harm,\n",
    "                           sr=sr,\n",
    "                           hop_length=512,\n",
    "                           n_bins=84,\n",
    "                           bins_per_octave=12))\n",
    "    C_db = librosa.amplitude_to_db(C, ref=np.max)\n",
    "\n",
    "    # --- choose device for torchcrepe ------------------------------------\n",
    "    if device is None:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "    # --- resample once for torchcrepe ------------------------------------\n",
    "    y16 = torchaudio.functional.resample(torch.tensor(y_harm),\n",
    "                                         sr, 16000) if sr != 16000 else torch.tensor(y_harm)\n",
    "    y16 = y16.unsqueeze(0).to(device)\n",
    "\n",
    "    rows = []\n",
    "    try:\n",
    "        for t, fr in zip(onset_times, onset_frames):\n",
    "            # ------------------------------------------------------------------\n",
    "            # 2·A  Polyphony gate  (CQT frame centred on onset)\n",
    "            # ------------------------------------------------------------------\n",
    "            # shift 30 ms (≈ 3 CQT hops at hop_length=512) past the onset\n",
    "            off = fr + 3\n",
    "            cqt_frame = C_db[:, off] if off < C_db.shape[1] else C_db[:, -1]\n",
    "            if _is_polyphonic(cqt_frame, peak_db, min_peaks):\n",
    "                continue                                  # reject → chord\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # 2·B  Periodicity gate (torchcrepe, harmonic layer only)\n",
    "            # ------------------------------------------------------------------\n",
    "            start16 = max(0, int(t * 16000) - hop_len//2)\n",
    "            end16   = start16 + hop_len\n",
    "            frame = y16[..., start16:end16]               # shape (1, N)\n",
    "            f0, pdist = torchcrepe.predict(frame,\n",
    "                                           16000,\n",
    "                                           hop_len,\n",
    "                                           model='full',\n",
    "                                           decoder=tcd.argmax,\n",
    "                                           fmin=80, fmax=1200,\n",
    "                                           batch_size=64,\n",
    "                                           device=device,\n",
    "                                           return_periodicity=True)\n",
    "            f0 = tcf.median(f0, 3)\n",
    "            hz   = float(f0.squeeze())\n",
    "            pval = float(pdist.squeeze())\n",
    "            if pval < conf_thresh or not (80 < hz < 1200):\n",
    "                continue                                  # weak/confused\n",
    "\n",
    "            rows.append((t, librosa.hz_to_note(hz, octave=False)))\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"torchcrepe failed ({e}); falling back to pyin\")\n",
    "        for t, fr in zip(onset_times, onset_frames):\n",
    "            if _is_polyphonic(C_db[:, fr], peak_db, min_peaks):\n",
    "                continue\n",
    "            start = max(0, fr*512)\n",
    "            end   = start + slice_samps\n",
    "            f0, _, _ = librosa.pyin(y[start:end], fmin=80, fmax=1200, sr=sr)\n",
    "            if f0 is not None and not np.isnan(f0).all():\n",
    "                hz = float(np.nanmedian(f0))\n",
    "                rows.append((t, librosa.hz_to_note(hz, octave=False)))\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\"time\", \"note\"])\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3.  Helper: beat‑level chord map (legacy, for strums/bars) ----------------\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def chord_sequence_by_beat(chords_df: pd.DataFrame, beat_times: np.ndarray):\n",
    "    idx, seg_i = {}, 0\n",
    "    for b, bt in enumerate(beat_times):\n",
    "        while seg_i+1 < len(chords_df) and chords_df.iloc[seg_i]['end'] <= bt:\n",
    "            seg_i += 1\n",
    "        idx[b] = chords_df.iloc[seg_i]['label']\n",
    "    return idx\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4.  Core analysis (strums/bars/sections) – chord map only -----------------\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def analyse_audio(audio_path: str, return_dataframes: bool = True):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr, units='frames', tightness=400)\n",
    "    tempo = float(np.atleast_1d(tempo)[0])\n",
    "    beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "    # ----- chord & note timelines (independent) -----\n",
    "    chords_df = chord_timeline(audio_path)\n",
    "    notes_df  = note_timeline(audio_path)\n",
    "    beat_chords = chord_sequence_by_beat(chords_df, beat_times)\n",
    "\n",
    "    # ----- onsets / strums (keep original behaviour) -----\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)\n",
    "    onset_times  = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "\n",
    "    grid_step = 60/tempo/4\n",
    "    grid_times = np.arange(beat_times[0], beat_times[-1]+grid_step, grid_step)\n",
    "\n",
    "    y_harm, _ = librosa.effects.hpss(y)\n",
    "    chroma = librosa.feature.chroma_cqt(y=y_harm, sr=sr)\n",
    "    directions = spectral_centroid_direction(y, sr, onset_frames)\n",
    "    rms = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)[0]\n",
    "\n",
    "    strums: List[Strum] = []\n",
    "    for i, (t, fr) in enumerate(zip(onset_times, onset_frames)):\n",
    "        gidx = int(np.argmin(np.abs(grid_times - t)))\n",
    "        bar_idx, sub16 = divmod(gidx, 16)\n",
    "        vel = float(rms[min(len(rms)-1, fr)])\n",
    "        kind, label = 'CHORD', beat_chords.get(int(np.argmin(np.abs(beat_times - t))), 'N')\n",
    "        strums.append(Strum(time=float(t), bar=bar_idx+1, sub_16=sub16,\n",
    "                            direction=directions[i], velocity=vel,\n",
    "                            kind=kind, label=label))\n",
    "\n",
    "    # ----- summarise bars/sections (same as before) -----\n",
    "    bars: Dict[int, BarSummary] = {}\n",
    "    for s in strums:\n",
    "        b = bars.setdefault(s.bar, BarSummary(bar=s.bar, bit_pattern=['0']*16,\n",
    "                     down_up=['-']*16, mean_vel=0.0, chords=[]))\n",
    "        b.bit_pattern[s.sub_16] = '1'\n",
    "        b.down_up[s.sub_16] = s.direction\n",
    "        b.mean_vel += s.velocity\n",
    "        if s.kind == 'CHORD':\n",
    "            b.chords.append(s.label)\n",
    "    for b in bars.values():\n",
    "        hits = b.bit_pattern.count('1')\n",
    "        b.mean_vel /= max(1, hits)\n",
    "        b.bit_pattern = ''.join(b.bit_pattern)\n",
    "        b.down_up = ' '.join(b.down_up)\n",
    "        b.chords = sorted(set(b.chords))\n",
    "\n",
    "    ordered = [bars[k] for k in sorted(bars)]\n",
    "    # simple section clustering unchanged for brevity ...\n",
    "\n",
    "    if return_dataframes:\n",
    "        return dict(\n",
    "            tempo_bpm=tempo,\n",
    "            strums=pd.DataFrame([asdict(s) for s in strums]),\n",
    "            bars=pd.DataFrame([asdict(b) for b in ordered]),\n",
    "            chords_timeline=chords_df,\n",
    "            notes_timeline=notes_df,\n",
    "        )\n",
    "    else:\n",
    "        return dict(\n",
    "            tempo_bpm=tempo,\n",
    "            strums=[asdict(s) for s in strums],\n",
    "            bars=[asdict(b) for b in ordered],\n",
    "            chords_timeline=chords_df.to_dict('records'),\n",
    "            notes_timeline=notes_df.to_dict('records'),\n",
    "        )\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Notebook helper -----------------------------------------------------------\n",
    "\n",
    "def run_in_notebook(audio_path: str):\n",
    "    data = analyse_audio(audio_path, return_dataframes=True)\n",
    "    from IPython.display import display\n",
    "    print(f\"Tempo ≈ {data['tempo_bpm']:.1f} BPM\\n\")\n",
    "    print(\"Chord segments:\"); display(data['chords_timeline'].head())\n",
    "    print(\"Note events:\");   display(data['notes_timeline'].head())\n",
    "    print(\"\\nStrums (first 10):\"); display(data['strums'].head(10))\n",
    "    print(\"\\nBars:\"); display(data['bars'].head())\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv/lib/python3.9/site-packages/madmom/io/audio.py:493: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  file_sample_rate, signal = wavfile.read(filename, mmap=True)\n",
      "/var/folders/sw/2zngpy_n771gjrnyvn_x3mlw0000gn/T/ipykernel_33464/3170762675.py:185: UserWarning: torchcrepe failed (only one element tensors can be converted to Python scalars); falling back to pyin\n",
      "  warnings.warn(f\"torchcrepe failed ({e}); falling back to pyin\")\n"
     ]
    }
   ],
   "source": [
    "audio_path = f'outputs/{song_name}/stage2_guitar_enhanced_cut.wav'\n",
    "# data = run_in_notebook(audio_path)\n",
    "data = analyse_audio(audio_path, return_dataframes=True)\n",
    "return_dataframes = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.775601</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.437370</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.366168</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.492336</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.712925</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>84.834104</td>\n",
       "      <td>F♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>85.298503</td>\n",
       "      <td>F♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>86.459501</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>87.852698</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>89.872834</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          time note\n",
       "0     9.775601    E\n",
       "1    10.437370    E\n",
       "2    11.366168    G\n",
       "3    12.492336    G\n",
       "4    12.712925    G\n",
       "..         ...  ...\n",
       "121  84.834104   F♯\n",
       "122  85.298503   F♯\n",
       "123  86.459501    G\n",
       "124  87.852698    E\n",
       "125  89.872834    G\n",
       "\n",
       "[126 rows x 2 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['notes_timeline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv/lib/python3.9/site-packages/madmom/io/audio.py:493: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  file_sample_rate, signal = wavfile.read(filename, mmap=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.6</td>\n",
       "      <td>13.7</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.7</td>\n",
       "      <td>15.3</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.3</td>\n",
       "      <td>16.3</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.3</td>\n",
       "      <td>17.3</td>\n",
       "      <td>D:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17.3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.0</td>\n",
       "      <td>21.1</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21.1</td>\n",
       "      <td>22.6</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22.6</td>\n",
       "      <td>23.6</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23.6</td>\n",
       "      <td>24.6</td>\n",
       "      <td>G:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>26.3</td>\n",
       "      <td>28.4</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>28.4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30.0</td>\n",
       "      <td>30.9</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30.9</td>\n",
       "      <td>31.9</td>\n",
       "      <td>D:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>31.9</td>\n",
       "      <td>33.6</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>33.6</td>\n",
       "      <td>35.7</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>35.7</td>\n",
       "      <td>37.3</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>37.3</td>\n",
       "      <td>38.3</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>38.3</td>\n",
       "      <td>39.3</td>\n",
       "      <td>G:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>39.3</td>\n",
       "      <td>41.0</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>41.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>43.1</td>\n",
       "      <td>44.4</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>44.4</td>\n",
       "      <td>45.7</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>45.7</td>\n",
       "      <td>46.6</td>\n",
       "      <td>D:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>46.6</td>\n",
       "      <td>48.2</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>48.2</td>\n",
       "      <td>50.3</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>50.3</td>\n",
       "      <td>52.0</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>52.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>53.0</td>\n",
       "      <td>53.8</td>\n",
       "      <td>G:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>53.8</td>\n",
       "      <td>55.6</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>55.6</td>\n",
       "      <td>57.6</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>57.6</td>\n",
       "      <td>59.4</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>59.4</td>\n",
       "      <td>60.1</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>60.1</td>\n",
       "      <td>61.2</td>\n",
       "      <td>D:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>61.2</td>\n",
       "      <td>62.9</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>62.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>65.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>66.7</td>\n",
       "      <td>67.7</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>67.7</td>\n",
       "      <td>68.5</td>\n",
       "      <td>D:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>68.5</td>\n",
       "      <td>70.1</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>70.1</td>\n",
       "      <td>72.4</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>72.4</td>\n",
       "      <td>74.0</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>74.0</td>\n",
       "      <td>74.9</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>74.9</td>\n",
       "      <td>75.8</td>\n",
       "      <td>D:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>75.8</td>\n",
       "      <td>77.6</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>77.6</td>\n",
       "      <td>79.7</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>79.7</td>\n",
       "      <td>81.3</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>81.3</td>\n",
       "      <td>82.2</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>82.2</td>\n",
       "      <td>83.2</td>\n",
       "      <td>G:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>83.2</td>\n",
       "      <td>85.0</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>85.0</td>\n",
       "      <td>86.9</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>86.9</td>\n",
       "      <td>88.7</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>88.7</td>\n",
       "      <td>89.4</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>89.4</td>\n",
       "      <td>90.0</td>\n",
       "      <td>G:maj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    start   end  label\n",
       "0     0.0   9.9      N\n",
       "1     9.9  11.6  E:min\n",
       "2    11.6  13.7  C:maj\n",
       "3    13.7  15.3  A:maj\n",
       "4    15.3  16.3  C:maj\n",
       "5    16.3  17.3  D:maj\n",
       "6    17.3  19.0  E:min\n",
       "7    19.0  21.1  C:maj\n",
       "8    21.1  22.6  A:maj\n",
       "9    22.6  23.6  C:maj\n",
       "10   23.6  24.6  G:maj\n",
       "11   24.6  26.3  E:min\n",
       "12   26.3  28.4  C:maj\n",
       "13   28.4  30.0  A:maj\n",
       "14   30.0  30.9  C:maj\n",
       "15   30.9  31.9  D:maj\n",
       "16   31.9  33.6  E:min\n",
       "17   33.6  35.7  C:maj\n",
       "18   35.7  37.3  A:maj\n",
       "19   37.3  38.3  C:maj\n",
       "20   38.3  39.3  G:maj\n",
       "21   39.3  41.0  E:min\n",
       "22   41.0  43.1  C:maj\n",
       "23   43.1  44.4  A:maj\n",
       "24   44.4  45.7  C:maj\n",
       "25   45.7  46.6  D:maj\n",
       "26   46.6  48.2  E:min\n",
       "27   48.2  50.3  C:maj\n",
       "28   50.3  52.0  A:maj\n",
       "29   52.0  53.0  C:maj\n",
       "30   53.0  53.8  G:maj\n",
       "31   53.8  55.6  E:min\n",
       "32   55.6  57.6  C:maj\n",
       "33   57.6  59.4  A:maj\n",
       "34   59.4  60.1  C:maj\n",
       "35   60.1  61.2  D:maj\n",
       "36   61.2  62.9  E:min\n",
       "37   62.9  65.0  C:maj\n",
       "38   65.0  66.7  A:maj\n",
       "39   66.7  67.7  C:maj\n",
       "40   67.7  68.5  D:maj\n",
       "41   68.5  70.1  E:min\n",
       "42   70.1  72.4  C:maj\n",
       "43   72.4  74.0  A:maj\n",
       "44   74.0  74.9  C:maj\n",
       "45   74.9  75.8  D:maj\n",
       "46   75.8  77.6  E:min\n",
       "47   77.6  79.7  C:maj\n",
       "48   79.7  81.3  A:maj\n",
       "49   81.3  82.2  C:maj\n",
       "50   82.2  83.2  G:maj\n",
       "51   83.2  85.0  E:min\n",
       "52   85.0  86.9  C:maj\n",
       "53   86.9  88.7  A:maj\n",
       "54   88.7  89.4  C:maj\n",
       "55   89.4  90.0  G:maj"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chord_timeline(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert pandas DataFrames and NumPy arrays to JSON-serializable types\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        return obj.to_dict(orient='records')  # Convert DataFrame to list of dictionaries\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.integer, np.floating)):\n",
    "        return float(obj) if isinstance(obj, np.floating) else int(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list) or isinstance(obj, tuple):\n",
    "        return [convert_to_serializable(i) for i in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert the data and save to JSON\n",
    "serializable_data = convert_to_serializable(data)\n",
    "\n",
    "# Save to file\n",
    "with open(f'outputs/{song_name}/guitar_data.json', 'w') as f:\n",
    "    json.dump(serializable_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tempo_bpm', 'strums', 'bars', 'chords_timeline', 'notes_timeline'])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seperator.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
