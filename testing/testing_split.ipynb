{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "song_path = 'Bon_Iver_St._Vincent_-_Roslyn_Lyrics.mp3'\n",
    "\n",
    "song_name = os.path.splitext(os.path.basename(song_path))[0].replace('_', '')\n",
    "os.makedirs(f'{output_dir}/{song_name}', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available: ['soundfile']\n"
     ]
    }
   ],
   "source": [
    "import torchaudio as ta\n",
    "print(\"Available:\", ta.list_audio_backends())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFmpeg location: /opt/homebrew/bin/ffmpeg\n",
      "Loading audio from Bon_Iver_St._Vincent_-_Roslyn_Lyrics.mp3...\n",
      "Loaded audio with shape torch.Size([2, 14800214]) and sample rate 48000\n",
      "Using device: mps\n",
      "STAGE 1: Separating with htdemucs_ft...\n",
      "Saved 'other' stem to outputs/BonIverSt.Vincent-RoslynLyrics/stage1_other.wav\n",
      "STAGE 2: Extracting guitar from 'other' using htdemucs_6s...\n",
      "Saved extracted guitar to outputs/BonIverSt.Vincent-RoslynLyrics/stage2_guitar_from_other.wav\n",
      "Saved enhanced guitar to outputs/BonIverSt.Vincent-RoslynLyrics/stage2_guitar_enhanced.wav\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from demucs.pretrained import get_model\n",
    "from demucs.apply import apply_model\n",
    "\n",
    "import subprocess\n",
    "result = subprocess.run(['which', 'ffmpeg'], capture_output=True, text=True)\n",
    "print(f\"FFmpeg location: {result.stdout.strip()}\")\n",
    "\n",
    "# Load the audio\n",
    "print(f\"Loading audio from {song_path}...\")\n",
    "sample_waveform, sample_rate = torchaudio.load(song_path)\n",
    "print(f\"Loaded audio with shape {sample_waveform.shape} and sample rate {sample_rate}\")\n",
    "\n",
    "# Determine device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def prepare_audio(waveform, source_sr, target_sr):\n",
    "    \"\"\"Prepare audio for model input\"\"\"\n",
    "    # Resample if needed\n",
    "    if source_sr != target_sr:\n",
    "        waveform = torchaudio.functional.resample(waveform, source_sr, target_sr)\n",
    "        \n",
    "    # Handle channels\n",
    "    if waveform.shape[0] > 2:\n",
    "        waveform = waveform[:2, :]\n",
    "    elif waveform.shape[0] == 1:\n",
    "        waveform = torch.cat([waveform, waveform], dim=0)\n",
    "        \n",
    "    return waveform\n",
    "\n",
    "def enhance_guitar(guitar_waveform, sample_rate):\n",
    "    \"\"\"Enhance guitar with filters and transient processing\"\"\"\n",
    "    # Add batch dimension if needed\n",
    "    if guitar_waveform.dim() == 2:\n",
    "        guitar_waveform = guitar_waveform.unsqueeze(0)\n",
    "        \n",
    "    # Now we can safely unpack dimensions\n",
    "    b, c, t = guitar_waveform.shape\n",
    "    \n",
    "    # FFT for frequency domain processing\n",
    "    guitar_waveform_freq = torch.fft.rfft(guitar_waveform, dim=2)\n",
    "    \n",
    "    # Create high-pass filter (reduce below 80Hz)\n",
    "    freqs = torch.fft.rfftfreq(t, d=1/sample_rate)\n",
    "    high_pass = (1 - torch.exp(-freqs/80))\n",
    "    \n",
    "    # Create mid boost around 2-4kHz (presence)\n",
    "    mid_boost = 1.0 + 0.5 * torch.exp(-((freqs - 3000)/500)**2)\n",
    "    \n",
    "    # Apply filters\n",
    "    filter_curve = high_pass.view(1, 1, -1) * mid_boost.view(1, 1, -1)\n",
    "    guitar_waveform_freq *= filter_curve\n",
    "    \n",
    "    # Back to time domain\n",
    "    guitar_waveform = torch.fft.irfft(guitar_waveform_freq, n=t, dim=2)\n",
    "    \n",
    "    # Apply subtle compression\n",
    "    peak = guitar_waveform.abs().max()\n",
    "    if peak > 0:\n",
    "        # Simple soft knee compression\n",
    "        threshold = 0.7\n",
    "        ratio = 3.0\n",
    "        gain = 1.2\n",
    "        \n",
    "        above_thresh = (guitar_waveform.abs() > threshold * peak).float()\n",
    "        comp_factor = 1.0 - above_thresh * (1.0 - 1.0/ratio) * (guitar_waveform.abs() - threshold * peak) / (peak * (1.0 - threshold))\n",
    "        guitar_waveform = guitar_waveform * comp_factor * gain\n",
    "        \n",
    "        # Final limiter\n",
    "        peak = guitar_waveform.abs().max()\n",
    "        if peak > 0.95:\n",
    "            guitar_waveform = 0.95 * guitar_waveform / peak\n",
    "    \n",
    "    # Remove batch dimension if we added it\n",
    "    if b == 1:\n",
    "        guitar_waveform = guitar_waveform.squeeze(0)\n",
    "        \n",
    "    return guitar_waveform\n",
    "\n",
    "# STAGE 1: Extract all stems with htdemucs_ft\n",
    "print(\"STAGE 1: Separating with htdemucs_ft...\")\n",
    "model_stage1 = get_model(\"htdemucs_ft\")\n",
    "model_stage1.eval()\n",
    "model_stage1.to(device)\n",
    "\n",
    "# Prepare audio for first model\n",
    "waveform_stage1 = prepare_audio(sample_waveform, sample_rate, model_stage1.samplerate)\n",
    "waveform_stage1 = waveform_stage1.to(device)\n",
    "\n",
    "# Separate first stage\n",
    "with torch.no_grad():\n",
    "    sources_stage1 = apply_model(model_stage1, waveform_stage1.unsqueeze(0))[0]\n",
    "    sources_stage1 = sources_stage1.cpu()\n",
    "\n",
    "# Get the \"other\" stem\n",
    "other_index = model_stage1.sources.index('other') if 'other' in model_stage1.sources else None\n",
    "if other_index is None:\n",
    "    print(\"Warning: 'other' source not found in model 1. Using all non-guitar sources combined.\")\n",
    "    # Combine all sources except guitar to create \"other\"\n",
    "    if 'guitar' in model_stage1.sources:\n",
    "        guitar_index = model_stage1.sources.index('guitar')\n",
    "        all_sources = torch.zeros_like(sources_stage1[0])\n",
    "        for i, src in enumerate(model_stage1.sources):\n",
    "            if i != guitar_index:\n",
    "                all_sources += sources_stage1[i]\n",
    "        other_waveform = all_sources\n",
    "    else:\n",
    "        # If no guitar source, just use the first stem as \"other\"\n",
    "        other_waveform = sources_stage1[0]\n",
    "else:\n",
    "    other_waveform = sources_stage1[other_index]\n",
    "\n",
    "# Save the other stem\n",
    "other_file = os.path.join(output_dir, song_name, \"stage1_other.wav\")\n",
    "torchaudio.save(other_file, other_waveform, model_stage1.samplerate)\n",
    "print(f\"Saved 'other' stem to {other_file}\")\n",
    "\n",
    "# STAGE 2: Extract guitar from \"other\" stem using htdemucs_6s\n",
    "print(\"STAGE 2: Extracting guitar from 'other' using htdemucs_6s...\")\n",
    "model_stage2 = get_model(\"htdemucs_6s\")\n",
    "model_stage2.eval()\n",
    "model_stage2.to(device)\n",
    "\n",
    "# Prepare the \"other\" stem for second model\n",
    "other_waveform = prepare_audio(other_waveform, model_stage1.samplerate, model_stage2.samplerate)\n",
    "other_waveform = other_waveform.to(device)\n",
    "\n",
    "# Separate second stage\n",
    "with torch.no_grad():\n",
    "    sources_stage2 = apply_model(model_stage2, other_waveform.unsqueeze(0))[0]\n",
    "    sources_stage2 = sources_stage2.cpu()\n",
    "\n",
    "# Get the guitar from second separation\n",
    "if 'guitar' in model_stage2.sources:\n",
    "    guitar_index = model_stage2.sources.index('guitar')\n",
    "    extracted_guitar = sources_stage2[guitar_index]\n",
    "    \n",
    "    # Save the extracted guitar\n",
    "    guitar_file = os.path.join(output_dir, song_name, \"stage2_guitar_from_other.wav\")\n",
    "    torchaudio.save(guitar_file, extracted_guitar, model_stage2.samplerate)\n",
    "    print(f\"Saved extracted guitar to {guitar_file}\")\n",
    "    \n",
    "    # Enhance and save\n",
    "    enhanced_guitar = enhance_guitar(extracted_guitar, model_stage2.samplerate)\n",
    "    enhanced_file = os.path.join(output_dir, song_name, \"stage2_guitar_enhanced.wav\")\n",
    "    torchaudio.save(enhanced_file, enhanced_guitar, model_stage2.samplerate)\n",
    "    print(f\"Saved enhanced guitar to {enhanced_file}\")\n",
    "else:\n",
    "    print(\"Error: 'guitar' source not found in the second model\")\n",
    "\n",
    "# BONUS: Also get the guitar from the first separation for comparison\n",
    "if 'guitar' in model_stage1.sources:\n",
    "    guitar_index = model_stage1.sources.index('guitar')\n",
    "    original_guitar = sources_stage1[guitar_index]\n",
    "    \n",
    "    # Save the original guitar stem\n",
    "    orig_guitar_file = os.path.join(output_dir, song_name, \"stage1_original_guitar.wav\")\n",
    "    torchaudio.save(orig_guitar_file, original_guitar, model_stage1.samplerate)\n",
    "    print(f\"Saved original guitar stem to {orig_guitar_file}\")\n",
    "    \n",
    "    # Create an enhanced version of the original guitar\n",
    "    enhanced_orig_guitar = enhance_guitar(original_guitar, model_stage1.samplerate)\n",
    "    enhanced_orig_file = os.path.join(output_dir, song_name, \"stage1_original_guitar_enhanced.wav\")\n",
    "    torchaudio.save(enhanced_orig_file, enhanced_orig_guitar, model_stage1.samplerate)\n",
    "    print(f\"Saved enhanced original guitar to {enhanced_orig_file}\")\n",
    "    \n",
    "    # FINAL STEP: Try combining both guitar extractions for maximum clarity\n",
    "    # Resample if needed to match sample rates\n",
    "    if model_stage1.samplerate != model_stage2.samplerate:\n",
    "        original_guitar = torchaudio.functional.resample(\n",
    "            original_guitar, model_stage1.samplerate, model_stage2.samplerate)\n",
    "    \n",
    "    # Make sure shapes match\n",
    "    min_length = min(original_guitar.shape[1], extracted_guitar.shape[1])\n",
    "    original_guitar = original_guitar[:, :min_length]\n",
    "    extracted_guitar = extracted_guitar[:, :min_length]\n",
    "    \n",
    "    # Blend with 70% from first model, 30% from second model\n",
    "    combined_guitar = 0.7 * original_guitar + 0.3 * extracted_guitar\n",
    "    \n",
    "    # Enhance the combined result\n",
    "    enhanced_combined = enhance_guitar(combined_guitar, model_stage2.samplerate)\n",
    "    combined_file = os.path.join(output_dir, song_name, \"combined_guitar_enhanced.wav\")\n",
    "    torchaudio.save(combined_file, enhanced_combined, model_stage2.samplerate)\n",
    "    print(f\"Saved combined enhanced guitar to {combined_file}\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Audio File → Audio Analysis Model → Extract tempo/rhythm → \n",
    "Text Description → LLM → Strumming Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1056, -0.1083, -0.1123,  ...,  0.0472,  0.0460,  0.0447],\n",
       "         [ 0.0179,  0.0209,  0.0228,  ...,  0.0922,  0.0914,  0.0905]]),\n",
       " 44100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "def trim_audio(input_file, output_file=None, start_sec=0, end_sec=None):\n",
    "    \"\"\"\n",
    "    Trim audio file to specified start and end times.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file: Path to the input audio file\n",
    "    - output_file: Path to save the trimmed file (if None, returns without saving)\n",
    "    - start_sec: Start time in seconds\n",
    "    - end_sec: End time in seconds (if None, trims to the end of the file)\n",
    "    \n",
    "    Returns:\n",
    "    - trimmed_waveform: Tensor containing the trimmed audio\n",
    "    - sample_rate: Sample rate of the audio\n",
    "    \"\"\"\n",
    "    # Load the audio\n",
    "    waveform, sample_rate = torchaudio.load(input_file)\n",
    "    \n",
    "    # Convert time to samples\n",
    "    start_sample = int(start_sec * sample_rate)\n",
    "    end_sample = int(end_sec * sample_rate) if end_sec is not None else waveform.shape[1]\n",
    "    \n",
    "    # Trim the audio\n",
    "    trimmed_waveform = waveform[:, start_sample:end_sample]\n",
    "    \n",
    "    # Save the trimmed audio if output_file is provided\n",
    "    if output_file:\n",
    "        torchaudio.save(output_file, trimmed_waveform, sample_rate)\n",
    "    \n",
    "    return trimmed_waveform, sample_rate\n",
    "\n",
    "trim_audio(f'outputs/{song_name}/stage2_guitar_enhanced.wav', start_sec = 10, end_sec = 100, output_file=f'outputs/{song_name}/stage2_guitar_enhanced_cut.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"dynamic_guitar_strum_analysis.py  –  chords & notes processed **independently**\n",
    "==========================================================================\n",
    "\n",
    "* Deep‑Chroma (madmom) → **chord timeline** (segment‑level)\n",
    "* torchcrepe (or pyin fallback) → **note timeline** (event‑level)\n",
    "* Original beat‑aligned strum/chord/bar/section logic LEFT INTACT so your UI\n",
    "  keeps working, but we **do not overwrite chords with notes** anymore.\n",
    "* Extra DataFrames returned: `chords_timeline`, `notes_timeline`.\n",
    "* One label per DataFrame → no clobbering; overlapping times are fine.\n",
    "\n",
    "Install (CPU only):\n",
    "    pip install librosa madmom torch torchaudio torchcrepe pandas tqdm \"numpy<1.24\"\n",
    "\n",
    "If you later add a CUDA PyTorch wheel, torchcrepe will use it automatically.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import warnings, traceback\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np, librosa, pandas as pd, torch, torchaudio\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Dataclasses (unchanged for strums/bars/sections)\n",
    "# --------------------------------------------------------------------------\n",
    "@dataclass\n",
    "class Strum:\n",
    "    time: float; bar: int; sub_16: int; direction: str; velocity: float; kind: str; label: str   # kind NOTE|CHORD|NONE\n",
    "\n",
    "@dataclass\n",
    "class BarSummary:\n",
    "    bar: int; bit_pattern: str; down_up: str; mean_vel: float; chords: List[str]\n",
    "\n",
    "@dataclass\n",
    "class Section:\n",
    "    start_bar: int; end_bar: int; pattern_bits: str; chords: List[str]\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Polyphony helper (v2) -----------------------------------------------------\n",
    "# --------------------------------------------------------------------------\n",
    "def _is_polyphonic(mag_db: np.ndarray,\n",
    "                   peak_db: float = -35.0,\n",
    "                   min_peaks: int = 3,\n",
    "                   dom_margin: float = 8.0) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if the frame is almost certainly a chord.\n",
    "    Override: if the strongest peak is `dom_margin` dB louder than the\n",
    "    2nd‑strongest, treat as monophonic even when `min_peaks` is exceeded.\n",
    "    \"\"\"\n",
    "    strong = mag_db > peak_db\n",
    "    # indices of strong bins\n",
    "    idx = np.flatnonzero(strong)\n",
    "    if len(idx) == 0:\n",
    "        return False\n",
    "\n",
    "    # dominant‑peak override\n",
    "    sorted_db = np.sort(mag_db[idx])\n",
    "    if len(sorted_db) >= 2 and sorted_db[-1] - sorted_db[-2] >= dom_margin:\n",
    "        return False                            # clearly one string dominates\n",
    "\n",
    "    # otherwise count distinct strong groups\n",
    "    groups = np.split(strong, np.flatnonzero(~strong) + 1)\n",
    "    n_peaks = sum(g.any() for g in groups)\n",
    "    return n_peaks >= min_peaks\n",
    "\n",
    "\n",
    "def spectral_centroid_direction(y: np.ndarray, sr: int, onset_frames: np.ndarray) -> List[str]:\n",
    "    \"\"\"Classify Down/Up strokes by sign of spectral‑centroid slope around attack.\"\"\"\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=512)[0]\n",
    "    dirs = []\n",
    "    for f in onset_frames:\n",
    "        a = max(0, f-2)\n",
    "        b = min(len(cent)-1, f+2)\n",
    "        dirs.append('D' if cent[b] - cent[a] < 0 else 'U')\n",
    "    return dirs\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 1.  Deep‑Chroma chord timeline (segment‑level) ----------------------------\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def chord_timeline(audio_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Return DF with columns [start, end, chord].\"\"\"\n",
    "    from madmom.audio.chroma import DeepChromaProcessor\n",
    "    from madmom.features.chords import DeepChromaChordRecognitionProcessor\n",
    "    chroma = DeepChromaProcessor()(audio_path)\n",
    "    segs   = DeepChromaChordRecognitionProcessor()(chroma)\n",
    "    df = pd.DataFrame(segs, columns=[\"start\", \"end\", \"label\"])\n",
    "    df[\"label\"] = df[\"label\"].str.split(\"/\").str[0]\n",
    "    return df\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.  torchcrepe / pyin note timeline (event‑level) -------------------------\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.  Gated note timeline (event‑level, single‑string only) -----------------\n",
    "# --------------------------------------------------------------------------\n",
    "def note_timeline(audio_path: str,\n",
    "                  hop_s: float = 0.01,\n",
    "                  conf_thresh: float = .8,\n",
    "                  peak_db: float = -52,\n",
    "                  min_peaks: int = 4,\n",
    "                  device: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DF [time, note] containing ONLY intentionally plucked single‑string notes.\n",
    "    Strategy:\n",
    "      1. Find onsets (same settings as the rest of the pipeline).\n",
    "      2. For each onset grab a 40 ms slice and CQT → run _is_polyphonic().\n",
    "      3. Only if that slice is *not* polyphonic do we call torchcrepe/pyin.\n",
    "    \"\"\"\n",
    "    import torchcrepe, torchcrepe.decode as tcd, torchcrepe.filter as tcf\n",
    "\n",
    "    # --- load + onset detection ------------------------------------------\n",
    "    y, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "    onset_env   = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)\n",
    "    onset_times  = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "\n",
    "    # --- constants --------------------------------------------------------\n",
    "    slice_ms = 40                                   # analysis window\n",
    "    slice_samps = int(sr * slice_ms / 1000.0)\n",
    "    hop_len = int(round(16000 * hop_s))\n",
    "\n",
    "    # --- prepare harmonic layer & CQT for gate ---------------------------\n",
    "    y_harm, _ = librosa.effects.hpss(y)             # helps both gates\n",
    "    C = np.abs(librosa.cqt(y_harm,\n",
    "                           sr=sr,\n",
    "                           hop_length=512,\n",
    "                           n_bins=84,\n",
    "                           bins_per_octave=12))\n",
    "    C_db = librosa.amplitude_to_db(C, ref=np.max)\n",
    "\n",
    "    # --- choose device for torchcrepe ------------------------------------\n",
    "    if device is None:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "    # --- resample once for torchcrepe ------------------------------------\n",
    "    y16 = torchaudio.functional.resample(torch.tensor(y_harm),\n",
    "                                         sr, 16000) if sr != 16000 else torch.tensor(y_harm)\n",
    "    y16 = y16.unsqueeze(0).to(device)\n",
    "\n",
    "    rows = []\n",
    "    try:\n",
    "        for t, fr in zip(onset_times, onset_frames):\n",
    "            # ------------------------------------------------------------------\n",
    "            # 2·A  Polyphony gate  (CQT frame centred on onset)\n",
    "            # ------------------------------------------------------------------\n",
    "            # shift 30 ms (≈ 3 CQT hops at hop_length=512) past the onset\n",
    "            off = fr + 3\n",
    "            cqt_frame = C_db[:, off] if off < C_db.shape[1] else C_db[:, -1]\n",
    "            if _is_polyphonic(cqt_frame, peak_db, min_peaks):\n",
    "                continue                                  # reject → chord\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # 2·B  Periodicity gate (torchcrepe, harmonic layer only)\n",
    "            # ------------------------------------------------------------------\n",
    "            start16 = max(0, int(t * 16000) - hop_len//2)\n",
    "            end16   = start16 + hop_len\n",
    "            frame = y16[..., start16:end16]               # shape (1, N)\n",
    "            f0, pdist = torchcrepe.predict(frame,\n",
    "                                           16000,\n",
    "                                           hop_len,\n",
    "                                           model='full',\n",
    "                                           decoder=tcd.argmax,\n",
    "                                           fmin=80, fmax=1200,\n",
    "                                           batch_size=64,\n",
    "                                           device=device,\n",
    "                                           return_periodicity=True)\n",
    "            f0 = tcf.median(f0, 3)\n",
    "            hz   = float(f0.squeeze())\n",
    "            pval = float(pdist.squeeze())\n",
    "            if pval < conf_thresh or not (80 < hz < 1200):\n",
    "                continue                                  # weak/confused\n",
    "\n",
    "            rows.append((t, librosa.hz_to_note(hz, octave=False)))\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"torchcrepe failed ({e}); falling back to pyin\")\n",
    "        for t, fr in zip(onset_times, onset_frames):\n",
    "            if _is_polyphonic(C_db[:, fr], peak_db, min_peaks):\n",
    "                continue\n",
    "            start = max(0, fr*512)\n",
    "            end   = start + slice_samps\n",
    "            f0, _, _ = librosa.pyin(y[start:end], fmin=80, fmax=1200, sr=sr)\n",
    "            if f0 is not None and not np.isnan(f0).all():\n",
    "                hz = float(np.nanmedian(f0))\n",
    "                rows.append((t, librosa.hz_to_note(hz, octave=False)))\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\"time\", \"note\"])\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3.  Helper: beat‑level chord map (legacy, for strums/bars) ----------------\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def chord_sequence_by_beat(chords_df: pd.DataFrame, beat_times: np.ndarray):\n",
    "    idx, seg_i = {}, 0\n",
    "    for b, bt in enumerate(beat_times):\n",
    "        while seg_i+1 < len(chords_df) and chords_df.iloc[seg_i]['end'] <= bt:\n",
    "            seg_i += 1\n",
    "        idx[b] = chords_df.iloc[seg_i]['label']\n",
    "    return idx\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4.  Core analysis (strums/bars/sections) – chord map only -----------------\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def analyse_audio(audio_path: str, return_dataframes: bool = True):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr, units='frames', tightness=400)\n",
    "    tempo = float(np.atleast_1d(tempo)[0])\n",
    "    beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "    # ----- chord & note timelines (independent) -----\n",
    "    chords_df = chord_timeline(audio_path)\n",
    "    notes_df  = note_timeline(audio_path)\n",
    "    beat_chords = chord_sequence_by_beat(chords_df, beat_times)\n",
    "\n",
    "    # ----- onsets / strums (keep original behaviour) -----\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)\n",
    "    onset_times  = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "\n",
    "    grid_step = 60/tempo/4\n",
    "    grid_times = np.arange(beat_times[0], beat_times[-1]+grid_step, grid_step)\n",
    "\n",
    "    y_harm, _ = librosa.effects.hpss(y)\n",
    "    chroma = librosa.feature.chroma_cqt(y=y_harm, sr=sr)\n",
    "    directions = spectral_centroid_direction(y, sr, onset_frames)\n",
    "    rms = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)[0]\n",
    "\n",
    "    strums: List[Strum] = []\n",
    "    for i, (t, fr) in enumerate(zip(onset_times, onset_frames)):\n",
    "        gidx = int(np.argmin(np.abs(grid_times - t)))\n",
    "        bar_idx, sub16 = divmod(gidx, 16)\n",
    "        vel = float(rms[min(len(rms)-1, fr)])\n",
    "        kind, label = 'CHORD', beat_chords.get(int(np.argmin(np.abs(beat_times - t))), 'N')\n",
    "        strums.append(Strum(time=float(t), bar=bar_idx+1, sub_16=sub16,\n",
    "                            direction=directions[i], velocity=vel,\n",
    "                            kind=kind, label=label))\n",
    "\n",
    "    # ----- summarise bars/sections (same as before) -----\n",
    "    bars: Dict[int, BarSummary] = {}\n",
    "    for s in strums:\n",
    "        b = bars.setdefault(s.bar, BarSummary(bar=s.bar, bit_pattern=['0']*16,\n",
    "                     down_up=['-']*16, mean_vel=0.0, chords=[]))\n",
    "        b.bit_pattern[s.sub_16] = '1'\n",
    "        b.down_up[s.sub_16] = s.direction\n",
    "        b.mean_vel += s.velocity\n",
    "        if s.kind == 'CHORD':\n",
    "            b.chords.append(s.label)\n",
    "    for b in bars.values():\n",
    "        hits = b.bit_pattern.count('1')\n",
    "        b.mean_vel /= max(1, hits)\n",
    "        b.bit_pattern = ''.join(b.bit_pattern)\n",
    "        b.down_up = ' '.join(b.down_up)\n",
    "        b.chords = sorted(set(b.chords))\n",
    "\n",
    "    ordered = [bars[k] for k in sorted(bars)]\n",
    "    # simple section clustering unchanged for brevity ...\n",
    "\n",
    "    if return_dataframes:\n",
    "        return dict(\n",
    "            tempo_bpm=tempo,\n",
    "            strums=pd.DataFrame([asdict(s) for s in strums]),\n",
    "            bars=pd.DataFrame([asdict(b) for b in ordered]),\n",
    "            chords_timeline=chords_df,\n",
    "            notes_timeline=notes_df,\n",
    "        )\n",
    "    else:\n",
    "        return dict(\n",
    "            tempo_bpm=tempo,\n",
    "            strums=[asdict(s) for s in strums],\n",
    "            bars=[asdict(b) for b in ordered],\n",
    "            chords_timeline=chords_df.to_dict('records'),\n",
    "            notes_timeline=notes_df.to_dict('records'),\n",
    "        )\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Notebook helper -----------------------------------------------------------\n",
    "\n",
    "def run_in_notebook(audio_path: str):\n",
    "    data = analyse_audio(audio_path, return_dataframes=True)\n",
    "    from IPython.display import display\n",
    "    print(f\"Tempo ≈ {data['tempo_bpm']:.1f} BPM\\n\")\n",
    "    print(\"Chord segments:\"); display(data['chords_timeline'].head())\n",
    "    print(\"Note events:\");   display(data['notes_timeline'].head())\n",
    "    print(\"\\nStrums (first 10):\"); display(data['strums'].head(10))\n",
    "    print(\"\\nBars:\"); display(data['bars'].head())\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielcrake/Desktop/Guitar-Separator/seperator_env/lib/python3.9/site-packages/madmom/__init__.py:21: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/Users/danielcrake/Desktop/Guitar-Separator/seperator_env/lib/python3.9/site-packages/madmom/io/audio.py:493: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  file_sample_rate, signal = wavfile.read(filename, mmap=True)\n",
      "/var/folders/sw/2zngpy_n771gjrnyvn_x3mlw0000gn/T/ipykernel_45790/3170762675.py:185: UserWarning: torchcrepe failed (only one element tensors can be converted to Python scalars); falling back to pyin\n",
      "  warnings.warn(f\"torchcrepe failed ({e}); falling back to pyin\")\n"
     ]
    }
   ],
   "source": [
    "audio_path = f'outputs/{song_name}/stage2_guitar_enhanced_cut.wav'\n",
    "# data = run_in_notebook(audio_path)\n",
    "data = analyse_audio(audio_path, return_dataframes=True)\n",
    "return_dataframes = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>F#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>C#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>7.1</td>\n",
       "      <td>A#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.1</td>\n",
       "      <td>9.9</td>\n",
       "      <td>D#:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.9</td>\n",
       "      <td>11.1</td>\n",
       "      <td>A#:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11.1</td>\n",
       "      <td>15.2</td>\n",
       "      <td>D#:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15.2</td>\n",
       "      <td>17.6</td>\n",
       "      <td>A#:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17.6</td>\n",
       "      <td>20.1</td>\n",
       "      <td>A#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20.1</td>\n",
       "      <td>22.3</td>\n",
       "      <td>F#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>22.3</td>\n",
       "      <td>24.8</td>\n",
       "      <td>A#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24.8</td>\n",
       "      <td>27.2</td>\n",
       "      <td>F#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>27.2</td>\n",
       "      <td>30.1</td>\n",
       "      <td>D#:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>30.1</td>\n",
       "      <td>31.7</td>\n",
       "      <td>A#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>31.7</td>\n",
       "      <td>35.3</td>\n",
       "      <td>D#:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>35.3</td>\n",
       "      <td>38.2</td>\n",
       "      <td>F#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>38.2</td>\n",
       "      <td>40.2</td>\n",
       "      <td>A#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>40.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>D#:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>42.0</td>\n",
       "      <td>44.9</td>\n",
       "      <td>A#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>44.9</td>\n",
       "      <td>46.2</td>\n",
       "      <td>F#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>46.2</td>\n",
       "      <td>50.5</td>\n",
       "      <td>D#:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50.5</td>\n",
       "      <td>51.7</td>\n",
       "      <td>A#:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>51.7</td>\n",
       "      <td>54.9</td>\n",
       "      <td>D#:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>54.9</td>\n",
       "      <td>56.8</td>\n",
       "      <td>A#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>56.8</td>\n",
       "      <td>60.2</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>60.2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>C#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>62.4</td>\n",
       "      <td>66.0</td>\n",
       "      <td>F#:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>66.0</td>\n",
       "      <td>67.3</td>\n",
       "      <td>A#:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>67.3</td>\n",
       "      <td>69.7</td>\n",
       "      <td>A#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>69.7</td>\n",
       "      <td>71.7</td>\n",
       "      <td>A#:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>71.7</td>\n",
       "      <td>73.9</td>\n",
       "      <td>F#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>73.9</td>\n",
       "      <td>77.4</td>\n",
       "      <td>C#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>77.4</td>\n",
       "      <td>78.2</td>\n",
       "      <td>A#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>78.2</td>\n",
       "      <td>81.1</td>\n",
       "      <td>G#:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>81.1</td>\n",
       "      <td>82.6</td>\n",
       "      <td>F:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>82.6</td>\n",
       "      <td>90.0</td>\n",
       "      <td>A#:maj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    start   end   label\n",
       "0     0.0   0.2       N\n",
       "1     0.2   4.2  F#:maj\n",
       "2     4.2   4.9  C#:maj\n",
       "3     4.9   7.1  A#:maj\n",
       "4     7.1   9.9  D#:min\n",
       "5     9.9  11.1  A#:min\n",
       "6    11.1  15.2  D#:min\n",
       "7    15.2  17.6  A#:min\n",
       "8    17.6  20.1  A#:maj\n",
       "9    20.1  22.3  F#:maj\n",
       "10   22.3  24.8  A#:maj\n",
       "11   24.8  27.2  F#:maj\n",
       "12   27.2  30.1  D#:min\n",
       "13   30.1  31.7  A#:maj\n",
       "14   31.7  35.3  D#:min\n",
       "15   35.3  38.2  F#:maj\n",
       "16   38.2  40.2  A#:maj\n",
       "17   40.2  42.0  D#:min\n",
       "18   42.0  44.9  A#:maj\n",
       "19   44.9  46.2  F#:maj\n",
       "20   46.2  50.5  D#:min\n",
       "21   50.5  51.7  A#:min\n",
       "22   51.7  54.9  D#:min\n",
       "23   54.9  56.8  A#:maj\n",
       "24   56.8  60.2       N\n",
       "25   60.2  62.4  C#:maj\n",
       "26   62.4  66.0  F#:min\n",
       "27   66.0  67.3  A#:min\n",
       "28   67.3  69.7  A#:maj\n",
       "29   69.7  71.7  A#:min\n",
       "30   71.7  73.9  F#:maj\n",
       "31   73.9  77.4  C#:maj\n",
       "32   77.4  78.2  A#:maj\n",
       "33   78.2  81.1  G#:maj\n",
       "34   81.1  82.6   F:maj\n",
       "35   82.6  90.0  A#:maj"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['chords_timeline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert pandas DataFrames and NumPy arrays to JSON-serializable types\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        return obj.to_dict(orient='records')  # Convert DataFrame to list of dictionaries\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.integer, np.floating)):\n",
    "        return float(obj) if isinstance(obj, np.floating) else int(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list) or isinstance(obj, tuple):\n",
    "        return [convert_to_serializable(i) for i in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert the data and save to JSON\n",
    "serializable_data = convert_to_serializable(data)\n",
    "\n",
    "# Save to file\n",
    "with open(f'outputs/{song_name}/guitar_data.json', 'w') as f:\n",
    "    json.dump(serializable_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tempo_bpm', 'strums', 'bars', 'chords_timeline', 'notes_timeline'])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HVQT raw: (6, 144, 8843)\n",
      "\n",
      "Results:\n",
      "Found 447 notes\n",
      "\n",
      "First few notes:\n",
      "   time  pitch note_name  string  fret  duration  confidence\n",
      "0   0.0     40        E2       0     0  0.743039    9.697382\n",
      "1   0.0     45        A2       1     0  0.441179    8.104714\n",
      "2   0.0     50        D3       2     0  0.835918    4.915620\n",
      "3   0.0     55        G3       3     0  0.650159    3.640748\n",
      "4   0.0     59        B3       4     0  0.789478    5.761517\n",
      "\n",
      "Found 121 chords\n",
      "\n",
      "First few chords:\n",
      "       time  duration   chord_name root  quality          fret_positions  \\\n",
      "0  0.000000  0.835918  D (unknown)    D  unknown      [0, 0, 0, 0, 0, 0]   \n",
      "1  0.626939  0.208980  E (unknown)    E  unknown   [-1, 0, -1, 0, -1, 0]   \n",
      "2  0.835918  0.162540  D (unknown)    D  unknown   [0, -1, 0, -1, -1, 0]   \n",
      "3  0.975238  0.185760      E Minor    E      min   [0, -1, -1, 0, 0, -1]   \n",
      "4  2.577415  0.116100  E (unknown)    E  unknown  [0, -1, -1, -1, -1, 0]   \n",
      "\n",
      "   num_notes  \n",
      "0          6  \n",
      "1          3  \n",
      "2          3  \n",
      "3          3  \n",
      "4          2  \n",
      "\n",
      "Tablature:\n",
      "e |-0--0--0--0--0--0--0--0--------0--0--0--0--0--0-----0--0--0-\n",
      "B |-0--------0-----------0--------0--0--0--0--0--0--0--0--0----\n",
      "G |-0-----0--0-----0--------------------0-----------------0----\n",
      "D |-0--------0--0-----0-----0--------0--0--0--0-----0--0-------\n",
      "A |-0--0--0--0--------------0--------0--0--0--0--0-------------\n",
      "E |-0--------0--0--------------0--0--0--0--0--0--0-----0--0----\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import librosa\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Helper utilities\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "\n",
    "def to_numpy(x):\n",
    "    \"\"\"Return a NumPy array no matter where the tensor currently lives.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy()\n",
    "    return np.asarray(x)\n",
    "\n",
    "\n",
    "def ensure_cpu(x):\n",
    "    \"\"\"Move a tensor to CPU if it is on an accelerator.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu()\n",
    "    return x\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Data classes\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "\n",
    "@dataclass\n",
    "class GuitarNote:\n",
    "    time: float\n",
    "    pitch: float\n",
    "    string: int\n",
    "    fret: int\n",
    "    duration: float\n",
    "    confidence: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GuitarChord:\n",
    "    time: float\n",
    "    duration: float\n",
    "    root: str\n",
    "    quality: str\n",
    "    notes: List[GuitarNote]\n",
    "    fret_positions: List[int]\n",
    "    chord_name: str\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Core processing\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "\n",
    "def process_fretnet_output_126dim(output: Dict,\n",
    "                                 confidence_threshold: float = 0.5,\n",
    "                                 hop_length: int = 512,\n",
    "                                 sr: int = 22050,\n",
    "                                 n_frets: int = 21) -> List[GuitarNote]:\n",
    "    \"\"\"Convert FretNet raw output (126‑dim tablature) into a list of GuitarNote objects.\"\"\"\n",
    "\n",
    "    tablature = output[\"tablature\"]\n",
    "    onsets = output.get(\"onsets\", None)\n",
    "\n",
    "    # convert tensors to NumPy – always!\n",
    "    tablature = to_numpy(tablature)\n",
    "    if onsets is not None:\n",
    "        onsets = to_numpy(onsets)\n",
    "\n",
    "    # Remove batch dim if present -> (frames, 126)\n",
    "    if tablature.ndim == 3:\n",
    "        tablature = tablature[0]\n",
    "\n",
    "    n_frames = tablature.shape[0]\n",
    "    n_strings = 6\n",
    "\n",
    "    # reshape to (frames, strings, frets)\n",
    "    if tablature.shape[1] == n_strings * n_frets:\n",
    "        tab_3d = tablature.reshape(n_frames, n_strings, n_frets)\n",
    "        tab_2d = None\n",
    "    elif tablature.shape[1] == n_strings:  # already condensed (frames × strings)\n",
    "        tab_2d = tablature\n",
    "        tab_3d = None\n",
    "    else:\n",
    "        print(\"[process_fretnet_output_126dim] Unexpected tablature shape:\", tablature.shape)\n",
    "        return []\n",
    "\n",
    "    open_strings = [40, 45, 50, 55, 59, 64]  # MIDI numbers of E2, A2, D3, G3, B3, E4\n",
    "    notes: List[GuitarNote] = []\n",
    "\n",
    "    for string_idx in range(n_strings):\n",
    "        if tab_3d is not None:\n",
    "            string_activations = tab_3d[:, string_idx, :]\n",
    "            fret_indices = np.argmax(string_activations, axis=1)\n",
    "            fret_confidences = np.max(string_activations, axis=1)\n",
    "\n",
    "            open_string_conf = string_activations[:, 0]\n",
    "            silence_frames = (\n",
    "                (fret_confidences < confidence_threshold)\n",
    "                | ((fret_indices == 0) & (open_string_conf < confidence_threshold))\n",
    "            )\n",
    "            active_frames = (~silence_frames)\n",
    "        else:\n",
    "            string_frets = tab_2d[:, string_idx]\n",
    "            active_frames = string_frets >= 0\n",
    "            fret_indices = string_frets.astype(int)\n",
    "            fret_confidences = np.full_like(string_frets, 0.8)\n",
    "\n",
    "        if not active_frames.any():\n",
    "            continue\n",
    "\n",
    "        # onset signal may or may not be present – handle flexible dims\n",
    "        onset_signal = None\n",
    "        if onsets is not None:\n",
    "            if onsets.ndim == 2 and string_idx < onsets.shape[1]:\n",
    "                onset_signal = onsets[:, string_idx]\n",
    "            elif onsets.ndim == 3 and string_idx < onsets.shape[2]:\n",
    "                onset_signal = onsets[0, :, string_idx]\n",
    "\n",
    "        # segment active regions\n",
    "        changes = np.diff(np.concatenate(([False], active_frames, [False])))\n",
    "        note_starts = np.where(changes)[0][::2]\n",
    "        note_ends = np.where(changes)[0][1::2]\n",
    "\n",
    "        for start, end in zip(note_starts, note_ends):\n",
    "            if end - start < 2:  # ignore too short\n",
    "                continue\n",
    "            segment_frets = fret_indices[start:end]\n",
    "            segment_conf = fret_confidences[start:end]\n",
    "\n",
    "            values, counts = np.unique(segment_frets, return_counts=True)\n",
    "            fret = int(values[np.argmax(counts)])\n",
    "            if fret < 0 or fret > n_frets:\n",
    "                continue\n",
    "\n",
    "            start_time = (start * hop_length) / sr\n",
    "            duration = ((end - start) * hop_length) / sr\n",
    "            if duration < 0.05:\n",
    "                continue\n",
    "\n",
    "            midi_pitch = open_strings[string_idx] + fret\n",
    "            confidence = float(np.mean(segment_conf))\n",
    "\n",
    "            notes.append(GuitarNote(\n",
    "                time=start_time,\n",
    "                pitch=midi_pitch,\n",
    "                string=string_idx,\n",
    "                fret=fret,\n",
    "                duration=duration,\n",
    "                confidence=confidence,\n",
    "            ))\n",
    "\n",
    "    return sorted(notes, key=lambda n: n.time)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Chord grouping helpers (original logic kept unchanged)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "\n",
    "def identify_chord_type(pitch_classes: List[int]):\n",
    "    note_names = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "    unique_pcs = sorted(set(pitch_classes))\n",
    "\n",
    "    chord_patterns = {\n",
    "        (0, 4, 7): (\"maj\", \"Major\"),\n",
    "        (0, 3, 7): (\"min\", \"Minor\"),\n",
    "        (0, 4, 7, 11): (\"maj7\", \"Major 7\"),\n",
    "        (0, 3, 7, 10): (\"min7\", \"Minor 7\"),\n",
    "        (0, 4, 7, 10): (\"7\", \"Dominant 7\"),\n",
    "        (0, 3, 6): (\"dim\", \"Diminished\"),\n",
    "        (0, 4, 8): (\"aug\", \"Augmented\"),\n",
    "        (0, 5, 7): (\"sus4\", \"Suspended 4\"),\n",
    "        (0, 2, 7): (\"sus2\", \"Suspended 2\"),\n",
    "    }\n",
    "\n",
    "    for root_pc in unique_pcs:\n",
    "        intervals = tuple(sorted(((pc - root_pc) % 12) for pc in unique_pcs))\n",
    "        if intervals in chord_patterns:\n",
    "            quality, full_name = chord_patterns[intervals]\n",
    "            root_name = note_names[root_pc]\n",
    "            return root_name, quality, f\"{root_name} {full_name}\"\n",
    "\n",
    "    if unique_pcs:\n",
    "        root_pc = unique_pcs[0]\n",
    "        return note_names[root_pc], \"unknown\", f\"{note_names[root_pc]} (unknown)\"\n",
    "\n",
    "    return None, None, None\n",
    "\n",
    "\n",
    "def analyze_chord(notes: List[GuitarNote]) -> Optional[GuitarChord]:\n",
    "    pitch_classes = [n.pitch % 12 for n in notes]\n",
    "    fret_positions = [-1] * 6\n",
    "    for n in notes:\n",
    "        fret_positions[n.string] = n.fret\n",
    "\n",
    "    root, quality, chord_name = identify_chord_type(pitch_classes)\n",
    "    if root is None:\n",
    "        return None\n",
    "\n",
    "    start_time = min(n.time for n in notes)\n",
    "    end_time = max(n.time + n.duration for n in notes)\n",
    "\n",
    "    return GuitarChord(\n",
    "        time=start_time,\n",
    "        duration=end_time - start_time,\n",
    "        root=root,\n",
    "        quality=quality,\n",
    "        notes=notes,\n",
    "        fret_positions=fret_positions,\n",
    "        chord_name=chord_name,\n",
    "    )\n",
    "\n",
    "\n",
    "def group_notes_to_chords(notes: List[GuitarNote], time_threshold: float = 0.05, min_notes: int = 2):\n",
    "    time_groups: Dict[float, List[GuitarNote]] = defaultdict(list)\n",
    "    for n in notes:\n",
    "        key = round(n.time / time_threshold) * time_threshold\n",
    "        time_groups[key].append(n)\n",
    "\n",
    "    chords: List[GuitarChord] = []\n",
    "    for t, grp in time_groups.items():\n",
    "        if len(grp) < min_notes:\n",
    "            continue\n",
    "        grp.sort(key=lambda n: n.string)\n",
    "        chord = analyze_chord(grp)\n",
    "        if chord:\n",
    "            chords.append(chord)\n",
    "    return sorted(chords, key=lambda c: c.time)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Tablature helper\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "\n",
    "def create_tablature_from_notes(notes: List[GuitarNote], max_time: float = 30.0):\n",
    "    if not notes:\n",
    "        return \"No notes detected\"\n",
    "\n",
    "    time_slots = defaultdict(lambda: [-1] * 6)\n",
    "    for n in notes:\n",
    "        if n.time > max_time:\n",
    "            break\n",
    "        slot = int(n.time * 4) / 4  # quarter‑second grid\n",
    "        time_slots[slot][n.string] = n.fret\n",
    "\n",
    "    lines = [\"e |\", \"B |\", \"G |\", \"D |\", \"A |\", \"E |\"]\n",
    "    for t in sorted(time_slots.keys())[:20]:\n",
    "        frets = time_slots[t]\n",
    "        for i in range(6):  # display high → low\n",
    "            fret = frets[5 - i]\n",
    "            if fret == -1:\n",
    "                lines[i] += \"---\"\n",
    "            elif fret == 0:\n",
    "                lines[i] += \"-0-\"\n",
    "            elif fret < 10:\n",
    "                lines[i] += f\"-{fret}-\"\n",
    "            else:\n",
    "                lines[i] += f\"{fret}-\"\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Main entry point\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "\n",
    "def fretnet_transcribe_updated(\n",
    "    audio_path: str,\n",
    "    model_path: str,\n",
    "    confidence_threshold: float = 0.5,\n",
    "    chord_time_threshold: float = 0.05,\n",
    "    device: str = \"mps\",\n",
    "):\n",
    "    \"\"\"High‑level wrapper: load model, run inference, return structured results.\"\"\"\n",
    "\n",
    "    from amt_tools.features import HVQT\n",
    "    from guitar_transcription_continuous.models import FretNet\n",
    "\n",
    "    # choose device gracefully\n",
    "    if device == \"mps\" and not torch.backends.mps.is_available():\n",
    "        device = \"cpu\"\n",
    "    dev = torch.device(device)\n",
    "\n",
    "    # ─── load model checkpoint ───\n",
    "    ckpt = torch.load(model_path, map_location=\"cpu\", weights_only=False)\n",
    "    if isinstance(ckpt, FretNet):\n",
    "        model = ckpt\n",
    "    else:\n",
    "        model = FretNet()\n",
    "        state_dict = ckpt.get(\"model_state_dict\", ckpt)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    model.eval().to(dev)\n",
    "\n",
    "    # ─── feature extraction ───\n",
    "    y, sr = librosa.load(audio_path, sr=22050)\n",
    "    hvqt = HVQT(\n",
    "        sample_rate=sr,\n",
    "        hop_length=512,\n",
    "        n_bins=144,\n",
    "        bins_per_octave=36,\n",
    "        fmin=librosa.note_to_hz(\"E2\"),\n",
    "    )\n",
    "    feats = hvqt.process_audio(y)  # (6, 144, T)\n",
    "    print(\"HVQT raw:\", feats.shape)\n",
    "    \n",
    "    # pad frame axis (axis 2) so T % 9 == 0\n",
    "    frame_axis = 2\n",
    "    pad = (-feats.shape[frame_axis]) % 9\n",
    "    if pad:\n",
    "        feats = np.pad(feats, ((0, 0), (0, 0), (0, pad)), mode=\"constant\")\n",
    "\n",
    "    feats_tensor = torch.tensor(feats, dtype=torch.float32).unsqueeze(0).to(dev)  # (1, 6, 144, T′)\n",
    "\n",
    "    assert feats_tensor.shape[1] == 6\n",
    "    assert feats_tensor.shape[2] == 144\n",
    "    assert feats_tensor.shape[3] % 9 == 0, \"frame axis must be multiple of 9\"\n",
    "\n",
    "    global output\n",
    "\n",
    "    # ─── inference ───\n",
    "    with torch.no_grad():\n",
    "        output = model(feats_tensor)\n",
    "\n",
    "    # move tensors in output back to CPU\n",
    "    \n",
    "    output = {k: ensure_cpu(v) for k, v in output.items()}\n",
    "\n",
    "    # ─── post‑processing ───\n",
    "    notes = process_fretnet_output_126dim(output, confidence_threshold)\n",
    "    chords = group_notes_to_chords(notes, chord_time_threshold)\n",
    "\n",
    "    notes_df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"time\": n.time,\n",
    "                \"pitch\": n.pitch,\n",
    "                \"note_name\": librosa.midi_to_note(n.pitch),\n",
    "                \"string\": n.string,\n",
    "                \"fret\": n.fret,\n",
    "                \"duration\": n.duration,\n",
    "                \"confidence\": n.confidence,\n",
    "            }\n",
    "            for n in notes\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chords_df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"time\": c.time,\n",
    "                \"duration\": c.duration,\n",
    "                \"chord_name\": c.chord_name,\n",
    "                \"root\": c.root,\n",
    "                \"quality\": c.quality,\n",
    "                \"fret_positions\": c.fret_positions,\n",
    "                \"num_notes\": len(c.notes),\n",
    "            }\n",
    "            for c in chords\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    tablature = create_tablature_from_notes(notes)\n",
    "\n",
    "    return {\n",
    "        \"notes\": notes,\n",
    "        \"chords\": chords,\n",
    "        \"notes_df\": notes_df,\n",
    "        \"chords_df\": chords_df,\n",
    "        \"tablature\": tablature,\n",
    "        \"raw_output\": output,\n",
    "    }\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Example usage (comment out if importing as a module)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "audio_path = '/Users/danielcrake/Desktop/Guitar-Separator/testing/outputs/FontainesD.C.-BugOfficialVideo/stage2_guitar_enhanced.wav'\n",
    "model_path = \"/Users/danielcrake/Desktop/Guitar-Separator/FretNet/models/fold-0/model-2000.pt\"\n",
    "\n",
    "results = fretnet_transcribe_updated(audio_path, model_path)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "if not results['notes_df'].empty:\n",
    "    print(f\"Found {len(results['notes_df'])} notes\")\n",
    "    print(\"\\nFirst few notes:\")\n",
    "    print(results['notes_df'].head())\n",
    "\n",
    "if not results['chords_df'].empty:\n",
    "    print(f\"\\nFound {len(results['chords_df'])} chords\")\n",
    "    print(\"\\nFirst few chords:\")\n",
    "    print(results['chords_df'].head())\n",
    "\n",
    "print(\"\\nTablature:\")\n",
    "print(results['tablature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tablature': tensor([[[  6.9916, -10.3610, -10.8343,  ..., -16.0457, -15.7000, -16.1509],\n",
       "          [ 12.9874, -13.7650, -17.0816,  ..., -12.9151, -13.8413, -13.1564],\n",
       "          [  3.3934,  -7.1001,  -8.7826,  ..., -19.3541, -19.5007, -20.1660],\n",
       "          ...,\n",
       "          [ 14.6498, -15.1403, -16.6030,  ..., -20.1774, -20.3374, -20.9411],\n",
       "          [ 12.4819, -14.7568, -16.2506,  ..., -15.3762, -15.7362, -15.1316],\n",
       "          [ 10.2246, -14.2621, -18.2183,  ..., -15.4078, -15.1092, -14.8960]]]),\n",
       " 'tablature_rel': tensor([[[ 1.3033e-03, -9.0692e-05, -9.7414e-04,  ...,  1.9838e-06,\n",
       "           -5.2828e-06,  2.7949e-05],\n",
       "          [ 1.3033e-03, -9.0692e-05, -9.7414e-04,  ...,  1.9838e-06,\n",
       "           -5.2828e-06,  2.7949e-05],\n",
       "          [ 1.3033e-03, -9.0692e-05, -9.7414e-04,  ...,  1.9838e-06,\n",
       "           -5.2828e-06,  2.7949e-05],\n",
       "          ...,\n",
       "          [ 1.3033e-03, -9.0692e-05, -9.7414e-04,  ...,  1.9838e-06,\n",
       "           -5.2828e-06,  2.7949e-05],\n",
       "          [ 1.3033e-03, -9.0692e-05, -9.7414e-04,  ...,  1.9838e-06,\n",
       "           -5.2828e-06,  2.7949e-05],\n",
       "          [ 1.3033e-03, -9.0692e-05, -9.7414e-04,  ...,  1.9838e-06,\n",
       "           -5.2828e-06,  2.7949e-05]]]),\n",
       " 'onsets': tensor([[[-11.4790, -12.5482, -11.6865,  ..., -16.1674, -17.8387, -17.1236],\n",
       "          [-18.4223, -26.1350, -25.4929,  ..., -20.0985, -20.6645, -20.1287],\n",
       "          [ -9.9798, -15.7143,  -8.9504,  ..., -19.6130, -20.4149, -19.3551],\n",
       "          ...,\n",
       "          [-17.8794, -21.0127, -16.0044,  ..., -23.0069, -23.7796, -23.0894],\n",
       "          [-19.3411, -18.8529, -16.0329,  ..., -18.5343, -19.2093, -18.1965],\n",
       "          [-18.4522, -24.8705, -20.6444,  ..., -24.0681, -23.1330, -23.6466]]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'notes'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import amt_tools.tools as tools\n",
    "\n",
    "tools.KEY_NOTES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielcrake/Desktop/Guitar-Separator/seperator_env/lib/python3.9/site-packages/jams/__init__.py:5: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/Users/danielcrake/Desktop/Guitar-Separator/seperator_env/lib/python3.9/site-packages/amt_tools/features/stream.py:16: RuntimeWarning: Could not import keyboard, likely because an X connection could not be acquired.\n",
      "  warnings.warn('Could not import keyboard, likely because an X ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model device after fix: mps\n"
     ]
    }
   ],
   "source": [
    "from guitar_transcription_continuous.estimators import StackedPitchListTablatureWrapper\n",
    "from amt_tools.features import HCQT\n",
    "\n",
    "from amt_tools.transcribe import ComboEstimator, \\\n",
    "                                 TablatureWrapper, \\\n",
    "                                 StackedOffsetsWrapper, \\\n",
    "                                 StackedNoteTranscriber\n",
    "from amt_tools.inference import run_offline\n",
    "\n",
    "import guitar_transcription_continuous.utils as utils\n",
    "import amt_tools.tools as tools\n",
    "\n",
    "# Regular imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import librosa\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "# Define path to model and audio to transcribe\n",
    "model_path = '/Users/danielcrake/Desktop/Guitar-Separator/FretNet/models/fold-0/model-2000.pt'\n",
    "audio_path = '/Users/danielcrake/Desktop/Guitar-Separator/testing/outputs/JeffBuckley-LoverYouShouldveComeOverAudio/stage2_guitar_enhanced_cut.wav'\n",
    "\n",
    "# Number of samples per second of audio\n",
    "sample_rate = 22050\n",
    "# Number of samples between frames\n",
    "hop_length = 512\n",
    "# Flag to re-acquire ground-truth data and re-calculate features\n",
    "reset_data = False\n",
    "# Choose the GPU on which to perform evaluation\n",
    "gpu_id = 0\n",
    "\n",
    "\n",
    "# device = torch.device(f'device=\"mps\" if torch.backends.mps.is_available() else \"cpu\"')\n",
    "\n",
    "# Load the model\n",
    "# Initialize a device pointer for loading the model\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the model\n",
    "model = torch.load(model_path, map_location=device, weights_only=False)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# CRITICAL FIX: Update internal device properties\n",
    "model.device = device\n",
    "if hasattr(model, 'tablature_layer'):\n",
    "    model.tablature_layer.device = device\n",
    "\n",
    "# Fix all submodules with device properties\n",
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, 'device'):\n",
    "        module.device = device\n",
    "\n",
    "print(f\"Model device after fix: {model.device}\")\n",
    "\n",
    "\n",
    "##############################\n",
    "# Predictions                #\n",
    "##############################\n",
    "\n",
    "# Load in the audio and normalize it\n",
    "audio, _ = tools.load_normalize_audio(audio_path, sample_rate)\n",
    "\n",
    "# Create an HCQT feature extraction module comprising\n",
    "# the first five harmonics and a sub-harmonic, where each\n",
    "# harmonic transform spans 4 octaves w/ 3 bins per semitone\n",
    "data_proc = HCQT(sample_rate=sample_rate,\n",
    "                 hop_length=hop_length,\n",
    "                 fmin=librosa.note_to_hz('E2'),\n",
    "                 harmonics=[0.5, 1, 2, 3, 4, 5],\n",
    "                 n_bins=144, bins_per_octave=36)\n",
    "\n",
    "# Compute the features\n",
    "features = {tools.KEY_FEATS : data_proc.process_audio(audio),\n",
    "            tools.KEY_TIMES : data_proc.get_times(audio)}\n",
    "\n",
    "# Initialize the estimation pipeline\n",
    "estimator = ComboEstimator([\n",
    "    # Discrete tablature -> stacked multi pitch array\n",
    "    TablatureWrapper(profile=model.profile),\n",
    "    # Stacked multi pitch array -> stacked offsets array\n",
    "    StackedOffsetsWrapper(profile=model.profile),\n",
    "    # Stacked multi pitch array -> stacked notes\n",
    "    StackedNoteTranscriber(profile=model.profile),\n",
    "    # Continuous tablature arrays -> stacked pitch list\n",
    "    StackedPitchListTablatureWrapper(profile=model.profile,\n",
    "                                     multi_pitch_key=tools.KEY_TABLATURE,\n",
    "                                     multi_pitch_rel_key=utils.KEY_TABLATURE_REL)])\n",
    "\n",
    "# Perform inference offline\n",
    "predictions = run_offline(features, model, estimator)\n",
    "\n",
    "# Extract the estimated notes\n",
    "stacked_notes_est = predictions[tools.KEY_NOTES]\n",
    "\n",
    "##############################\n",
    "# Plotting                   #\n",
    "##############################\n",
    "\n",
    "# Convert the estimated notes to frets\n",
    "stacked_frets_est = tools.stacked_notes_to_frets(stacked_notes_est)\n",
    "\n",
    "# Plot estimated tablature and add an appropriate title\n",
    "fig_est = tools.initialize_figure(interactive=False, figsize=(20, 5))\n",
    "fig_est = tools.plot_guitar_tablature(stacked_frets_est, fig=fig_est)\n",
    "fig_est.suptitle('Inference')\n",
    "\n",
    "# Display the plot\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced chord recognition system ready!\n",
      "\n",
      "Usage:\n",
      "  chord_results, recognizer = analyze_guitar_chords(predictions)\n",
      "  quick_chord_check(predictions)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import itertools\n",
    "\n",
    "@dataclass\n",
    "class ChordResult:\n",
    "    time_start: float\n",
    "    time_end: float\n",
    "    chord_name: str\n",
    "    root_note: str\n",
    "    chord_quality: str\n",
    "    notes: List[str]\n",
    "    confidence: float\n",
    "    bass_note: Optional[str] = None\n",
    "\n",
    "class AdvancedChordRecognizer:\n",
    "    def __init__(self):\n",
    "        self.setup_chord_database()\n",
    "        self.setup_guitar_tuning()\n",
    "        \n",
    "    def setup_guitar_tuning(self):\n",
    "        \"\"\"Standard guitar tuning (low to high): E-A-D-G-B-E\"\"\"\n",
    "        self.string_tuning = {\n",
    "            0: 40,  # Low E (E2)\n",
    "            1: 45,  # A (A2) \n",
    "            2: 50,  # D (D3)\n",
    "            3: 55,  # G (G3)\n",
    "            4: 59,  # B (B3)\n",
    "            5: 64   # High E (E4)\n",
    "        }\n",
    "        \n",
    "    def setup_chord_database(self):\n",
    "        \"\"\"Comprehensive chord pattern database\"\"\"\n",
    "        self.note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "        \n",
    "        # Define chord intervals (semitones from root)\n",
    "        self.chord_patterns = {\n",
    "            # Triads\n",
    "            'major': [0, 4, 7],\n",
    "            'minor': [0, 3, 7],\n",
    "            'diminished': [0, 3, 6],\n",
    "            'augmented': [0, 4, 8],\n",
    "            'sus2': [0, 2, 7],\n",
    "            'sus4': [0, 5, 7],\n",
    "            \n",
    "            # Seventh chords\n",
    "            'major7': [0, 4, 7, 11],\n",
    "            'minor7': [0, 3, 7, 10],\n",
    "            'dominant7': [0, 4, 7, 10],\n",
    "            'diminished7': [0, 3, 6, 9],\n",
    "            'half-diminished7': [0, 3, 6, 10],\n",
    "            'major7#11': [0, 4, 7, 11, 18],\n",
    "            \n",
    "            # Extended chords\n",
    "            'add9': [0, 4, 7, 14],\n",
    "            'major9': [0, 4, 7, 11, 14],\n",
    "            'minor9': [0, 3, 7, 10, 14],\n",
    "            '9': [0, 4, 7, 10, 14],\n",
    "            '11': [0, 4, 7, 10, 14, 17],\n",
    "            '13': [0, 4, 7, 10, 14, 21],\n",
    "            \n",
    "            # Power chord\n",
    "            'power': [0, 7],\n",
    "            '5': [0, 7],  # Same as power chord\n",
    "        }\n",
    "        \n",
    "        # Common guitar chord voicings\n",
    "        self.guitar_voicings = {\n",
    "            'C': {'frets': [3, 3, 2, 0, 1, 0], 'notes': ['G', 'C', 'E', 'G', 'C', 'E']},\n",
    "            'G': {'frets': [3, 2, 0, 0, 3, 3], 'notes': ['G', 'B', 'D', 'G', 'B', 'G']},\n",
    "            'Am': {'frets': [-1, 0, 2, 2, 1, 0], 'notes': [None, 'A', 'E', 'A', 'C', 'E']},\n",
    "            'F': {'frets': [1, 3, 3, 2, 1, 1], 'notes': ['F', 'A', 'C', 'F', 'A', 'F']},\n",
    "            'Dm': {'frets': [-1, -1, 0, 2, 3, 1], 'notes': [None, None, 'D', 'A', 'D', 'F']},\n",
    "            'Em': {'frets': [0, 2, 2, 0, 0, 0], 'notes': ['E', 'B', 'E', 'G', 'B', 'E']},\n",
    "        }\n",
    "\n",
    "    def midi_to_note(self, midi_num: float) -> str:\n",
    "        \"\"\"Convert MIDI number to note name\"\"\"\n",
    "        return self.note_names[int(midi_num) % 12]\n",
    "    \n",
    "    def note_to_number(self, note: str) -> int:\n",
    "        \"\"\"Convert note name to chromatic number (0-11)\"\"\"\n",
    "        return self.note_names.index(note)\n",
    "    \n",
    "    def normalize_notes(self, notes: List[str]) -> List[int]:\n",
    "        \"\"\"Convert notes to chromatic numbers and remove duplicates\"\"\"\n",
    "        return sorted(list(set([self.note_to_number(note) for note in notes])))\n",
    "    \n",
    "    def find_chord_intervals(self, notes: List[int]) -> List[int]:\n",
    "        \"\"\"Calculate intervals from the lowest note\"\"\"\n",
    "        if not notes:\n",
    "            return []\n",
    "        root = min(notes)\n",
    "        return [(note - root) % 12 for note in notes]\n",
    "    \n",
    "    def match_chord_pattern(self, intervals: List[int]) -> Tuple[str, float]:\n",
    "        \"\"\"Match intervals to known chord patterns\"\"\"\n",
    "        intervals_set = set(intervals)\n",
    "        best_match = ('unknown', 0.0)\n",
    "        \n",
    "        for chord_type, pattern in self.chord_patterns.items():\n",
    "            pattern_set = set(pattern)\n",
    "            \n",
    "            # Calculate match score\n",
    "            if len(pattern_set) == 0:\n",
    "                continue\n",
    "                \n",
    "            intersection = len(intervals_set & pattern_set)\n",
    "            union = len(intervals_set | pattern_set)\n",
    "            \n",
    "            # Jaccard similarity with bonus for exact matches\n",
    "            if pattern_set == intervals_set:\n",
    "                score = 1.0\n",
    "            else:\n",
    "                score = intersection / union if union > 0 else 0\n",
    "                \n",
    "            # Bonus for having the root and fifth\n",
    "            if 0 in intervals_set and 7 in intervals_set:\n",
    "                score += 0.1\n",
    "                \n",
    "            if score > best_match[1]:\n",
    "                best_match = (chord_type, score)\n",
    "        \n",
    "        return best_match\n",
    "    \n",
    "    def find_best_root(self, notes: List[int]) -> Tuple[int, str, float]:\n",
    "        \"\"\"Try each note as root and find best chord match\"\"\"\n",
    "        best_result = (0, 'unknown', 0.0)\n",
    "        \n",
    "        for root_candidate in notes:\n",
    "            # Calculate intervals with this root\n",
    "            intervals = [(note - root_candidate) % 12 for note in notes]\n",
    "            intervals = sorted(list(set(intervals)))\n",
    "            \n",
    "            chord_type, confidence = self.match_chord_pattern(intervals)\n",
    "            \n",
    "            if confidence > best_result[2]:\n",
    "                best_result = (root_candidate, chord_type, confidence)\n",
    "        \n",
    "        return best_result\n",
    "    \n",
    "    def extract_notes_in_timeframe(self, notes_data: Dict, start_time: float, end_time: float) -> List[Tuple[str, int]]:\n",
    "        \"\"\"Extract all notes active in a given timeframe\"\"\"\n",
    "        active_notes = []\n",
    "        \n",
    "        for string_id, (pitches, intervals) in notes_data.items():\n",
    "            if len(intervals) == 0:\n",
    "                continue\n",
    "                \n",
    "            for i, interval in enumerate(intervals):\n",
    "                note_start, note_end = interval[0], interval[1]\n",
    "                \n",
    "                # Check if note overlaps with our timeframe\n",
    "                if note_start <= end_time and note_end >= start_time:\n",
    "                    midi_pitch = pitches[i]\n",
    "                    note_name = self.midi_to_note(midi_pitch)\n",
    "                    note_number = int(midi_pitch) % 12\n",
    "                    active_notes.append((note_name, note_number))\n",
    "        \n",
    "        return active_notes\n",
    "    \n",
    "    def temporal_smoothing(self, chord_sequence: List[ChordResult], min_duration: float = 0.5) -> List[ChordResult]:\n",
    "        \"\"\"Remove very short chord changes and smooth sequence\"\"\"\n",
    "        if not chord_sequence:\n",
    "            return []\n",
    "            \n",
    "        smoothed = []\n",
    "        current_chord = chord_sequence[0]\n",
    "        \n",
    "        for next_chord in chord_sequence[1:]:\n",
    "            duration = current_chord.time_end - current_chord.time_start\n",
    "            \n",
    "            # If chord is too short, try to merge with next if they're similar\n",
    "            if duration < min_duration:\n",
    "                if (next_chord.root_note == current_chord.root_note and \n",
    "                    next_chord.chord_quality == current_chord.chord_quality):\n",
    "                    # Merge chords\n",
    "                    current_chord.time_end = next_chord.time_end\n",
    "                    current_chord.confidence = max(current_chord.confidence, next_chord.confidence)\n",
    "                    continue\n",
    "            \n",
    "            smoothed.append(current_chord)\n",
    "            current_chord = next_chord\n",
    "        \n",
    "        smoothed.append(current_chord)\n",
    "        return smoothed\n",
    "    \n",
    "    def analyze_chord_progression(self, predictions: Dict, \n",
    "                                time_resolution: float = 0.25,\n",
    "                                min_notes: int = 2,\n",
    "                                confidence_threshold: float = 0.3) -> List[ChordResult]:\n",
    "        \"\"\"Main method to analyze chord progression\"\"\"\n",
    "        \n",
    "        notes_data = predictions['notes']\n",
    "        times = predictions.get('times', [])\n",
    "        \n",
    "        if len(times) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Create time grid\n",
    "        start_time = float(times[0])\n",
    "        end_time = float(times[-1])\n",
    "        time_points = np.arange(start_time, end_time, time_resolution)\n",
    "        \n",
    "        chord_results = []\n",
    "        \n",
    "        for i in range(len(time_points) - 1):\n",
    "            window_start = time_points[i]\n",
    "            window_end = time_points[i + 1]\n",
    "            \n",
    "            # Extract notes in this time window\n",
    "            active_notes = self.extract_notes_in_timeframe(notes_data, window_start, window_end)\n",
    "            \n",
    "            if len(active_notes) < min_notes:\n",
    "                continue\n",
    "            \n",
    "            # Get unique note names and numbers\n",
    "            note_names = [note[0] for note in active_notes]\n",
    "            note_numbers = [note[1] for note in active_notes]\n",
    "            unique_notes = list(set(note_numbers))\n",
    "            \n",
    "            if len(unique_notes) < min_notes:\n",
    "                continue\n",
    "            \n",
    "            # Find best chord match\n",
    "            root_number, chord_quality, confidence = self.find_best_root(unique_notes)\n",
    "            \n",
    "            if confidence < confidence_threshold:\n",
    "                continue\n",
    "            \n",
    "            root_note = self.note_names[root_number]\n",
    "            bass_note = self.note_names[min(unique_notes)]  # Lowest note as bass\n",
    "            \n",
    "            # Create chord name\n",
    "            if chord_quality == 'major':\n",
    "                chord_name = root_note\n",
    "            elif chord_quality == 'power' or chord_quality == '5':\n",
    "                chord_name = f\"{root_note}5\"\n",
    "            else:\n",
    "                chord_name = f\"{root_note}{chord_quality}\"\n",
    "            \n",
    "            # Add slash chord notation if bass != root\n",
    "            if bass_note != root_note and chord_quality not in ['power', '5']:\n",
    "                chord_name += f\"/{bass_note}\"\n",
    "            \n",
    "            chord_result = ChordResult(\n",
    "                time_start=window_start,\n",
    "                time_end=window_end,\n",
    "                chord_name=chord_name,\n",
    "                root_note=root_note,\n",
    "                chord_quality=chord_quality,\n",
    "                notes=list(set(note_names)),\n",
    "                confidence=confidence,\n",
    "                bass_note=bass_note if bass_note != root_note else None\n",
    "            )\n",
    "            \n",
    "            chord_results.append(chord_result)\n",
    "        \n",
    "        # Merge consecutive identical chords\n",
    "        merged_chords = self.merge_consecutive_chords(chord_results)\n",
    "        \n",
    "        # Apply temporal smoothing\n",
    "        smoothed_chords = self.temporal_smoothing(merged_chords)\n",
    "        \n",
    "        return smoothed_chords\n",
    "    \n",
    "    def merge_consecutive_chords(self, chord_results: List[ChordResult]) -> List[ChordResult]:\n",
    "        \"\"\"Merge consecutive identical chords\"\"\"\n",
    "        if not chord_results:\n",
    "            return []\n",
    "        \n",
    "        merged = []\n",
    "        current = chord_results[0]\n",
    "        \n",
    "        for next_chord in chord_results[1:]:\n",
    "            if (current.chord_name == next_chord.chord_name and \n",
    "                abs(current.time_end - next_chord.time_start) < 0.1):  # Small gap tolerance\n",
    "                # Merge chords\n",
    "                current.time_end = next_chord.time_end\n",
    "                current.confidence = max(current.confidence, next_chord.confidence)\n",
    "                # Combine notes\n",
    "                current.notes = list(set(current.notes + next_chord.notes))\n",
    "            else:\n",
    "                merged.append(current)\n",
    "                current = next_chord\n",
    "        \n",
    "        merged.append(current)\n",
    "        return merged\n",
    "    \n",
    "    def print_chord_analysis(self, chord_results: List[ChordResult], max_chords: int = 20):\n",
    "        \"\"\"Print formatted chord analysis\"\"\"\n",
    "        print(f\"\\n🎸 CHORD ANALYSIS RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if not chord_results:\n",
    "            print(\"No chords detected.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(chord_results)} chord segments\")\n",
    "        print(f\"Showing first {min(max_chords, len(chord_results))} chords:\\n\")\n",
    "        \n",
    "        for i, chord in enumerate(chord_results[:max_chords]):\n",
    "            duration = chord.time_end - chord.time_start\n",
    "            confidence_bar = \"█\" * int(chord.confidence * 10) + \"░\" * (10 - int(chord.confidence * 10))\n",
    "            \n",
    "            print(f\"{i+1:2d}. {chord.time_start:6.1f}s - {chord.time_end:6.1f}s \"\n",
    "                  f\"({duration:4.1f}s) | {chord.chord_name:8s} | \"\n",
    "                  f\"Confidence: {confidence_bar} {chord.confidence:.2f}\")\n",
    "            print(f\"     Notes: {', '.join(sorted(chord.notes))}\")\n",
    "            \n",
    "            if i > 0 and i % 5 == 0:\n",
    "                print()\n",
    "    \n",
    "    def export_chord_progression(self, chord_results: List[ChordResult]) -> str:\n",
    "        \"\"\"Export chord progression as a simple string\"\"\"\n",
    "        if not chord_results:\n",
    "            return \"No chords detected\"\n",
    "        \n",
    "        # Group by chord name for summary\n",
    "        chord_sequence = []\n",
    "        for chord in chord_results:\n",
    "            duration = chord.time_end - chord.time_start\n",
    "            if duration >= 0.5:  # Only include chords lasting at least 0.5 seconds\n",
    "                chord_sequence.append(chord.chord_name)\n",
    "        \n",
    "        # Remove consecutive duplicates\n",
    "        unique_sequence = []\n",
    "        for chord in chord_sequence:\n",
    "            if not unique_sequence or chord != unique_sequence[-1]:\n",
    "                unique_sequence.append(chord)\n",
    "        \n",
    "        return \" | \".join(unique_sequence)\n",
    "\n",
    "# Usage example and analysis function\n",
    "def analyze_guitar_chords(predictions):\n",
    "    \"\"\"Analyze guitar chords from prediction data\"\"\"\n",
    "    recognizer = AdvancedChordRecognizer()\n",
    "    \n",
    "    print(\"🔍 Analyzing guitar chords...\")\n",
    "    chord_results = recognizer.analyze_chord_progression(\n",
    "        predictions, \n",
    "        time_resolution=0.25,  # Analyze every 0.25 seconds\n",
    "        min_notes=2,           # Need at least 2 notes for a chord\n",
    "        confidence_threshold=0.4  # Minimum confidence for chord detection\n",
    "    )\n",
    "    \n",
    "    # Print analysis\n",
    "    recognizer.print_chord_analysis(chord_results)\n",
    "    \n",
    "    # Export progression\n",
    "    progression = recognizer.export_chord_progression(chord_results)\n",
    "    print(f\"\\n🎵 CHORD PROGRESSION:\")\n",
    "    print(f\"   {progression}\")\n",
    "    \n",
    "    return chord_results, recognizer\n",
    "\n",
    "# Quick analysis of specific time segments\n",
    "def quick_chord_check(predictions, time_segments=None):\n",
    "    \"\"\"Quick chord analysis for specific time segments\"\"\"\n",
    "    if time_segments is None:\n",
    "        time_segments = [(10, 15), (18, 23), (45, 50), (80, 85)]\n",
    "    \n",
    "    recognizer = AdvancedChordRecognizer()\n",
    "    \n",
    "    print(\"\\n🎯 QUICK CHORD CHECK:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for start, end in time_segments:\n",
    "        notes = recognizer.extract_notes_in_timeframe(predictions['notes'], start, end)\n",
    "        if len(notes) >= 2:\n",
    "            note_names = [note[0] for note in notes]\n",
    "            note_numbers = [note[1] for note in notes]\n",
    "            unique_notes = list(set(note_numbers))\n",
    "            \n",
    "            root_number, chord_quality, confidence = recognizer.find_best_root(unique_notes)\n",
    "            root_note = recognizer.note_names[root_number]\n",
    "            \n",
    "            chord_name = f\"{root_note}{chord_quality}\" if chord_quality != 'major' else root_note\n",
    "            \n",
    "            print(f\"{start:2d}-{end:2d}s: {chord_name:8s} (conf: {confidence:.2f}) \"\n",
    "                  f\"Notes: {', '.join(sorted(set(note_names)))}\")\n",
    "        else:\n",
    "            print(f\"{start:2d}-{end:2d}s: Not enough notes\")\n",
    "\n",
    "print(\"Advanced chord recognition system ready!\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  chord_results, recognizer = analyze_guitar_chords(predictions)\")\n",
    "print(\"  quick_chord_check(predictions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyzing guitar chords...\n",
      "\n",
      "🎸 CHORD ANALYSIS RESULTS\n",
      "============================================================\n",
      "Found 17 chord segments\n",
      "Showing first 17 chords:\n",
      "\n",
      " 1.   37.2s -   38.0s ( 0.8s) | D5       | Confidence: ███████████ 1.10\n",
      "     Notes: A, D\n",
      " 2.   38.8s -   39.8s ( 1.0s) | C5       | Confidence: ███████████ 1.10\n",
      "     Notes: C, G\n",
      " 3.   41.0s -   44.0s ( 3.0s) | E5       | Confidence: ███████████ 1.10\n",
      "     Notes: B, E\n",
      " 4.   50.8s -   51.8s ( 1.0s) | C5       | Confidence: ███████████ 1.10\n",
      "     Notes: C, G\n",
      " 5.   52.2s -   53.0s ( 0.8s) | Baugmented/G | Confidence: ██████░░░░ 0.67\n",
      "     Notes: B, G\n",
      " 6.   57.0s -   57.5s ( 0.5s) | E5       | Confidence: ███████████ 1.10\n",
      "     Notes: B, E\n",
      "\n",
      " 7.   57.5s -   57.8s ( 0.2s) | D5       | Confidence: ███████████ 1.10\n",
      "     Notes: A, D\n",
      " 8.   62.8s -   63.5s ( 0.8s) | Csus2    | Confidence: ███████████ 1.10\n",
      "     Notes: C, D, G\n",
      " 9.   63.5s -   63.8s ( 0.2s) | G5       | Confidence: ███████████ 1.10\n",
      "     Notes: D, G\n",
      "10.   66.5s -   68.2s ( 1.8s) | Baugmented/G | Confidence: ██████░░░░ 0.67\n",
      "     Notes: B, G\n",
      "11.   73.5s -   75.0s ( 1.5s) | A#diminished/E | Confidence: ██████░░░░ 0.67\n",
      "     Notes: A#, E\n",
      "\n",
      "12.   80.5s -   81.5s ( 1.0s) | G5       | Confidence: ███████████ 1.10\n",
      "     Notes: D, G\n",
      "13.   81.5s -   81.8s ( 0.2s) | Dsus4    | Confidence: ███████████ 1.10\n",
      "     Notes: A, D, G\n",
      "14.   81.8s -   82.0s ( 0.2s) | G5       | Confidence: ███████████ 1.10\n",
      "     Notes: D, G\n",
      "15.   84.5s -   84.8s ( 0.2s) | D5       | Confidence: ███████████ 1.10\n",
      "     Notes: A, D\n",
      "16.   86.8s -   87.8s ( 1.0s) | C5       | Confidence: ███████████ 1.10\n",
      "     Notes: C, G\n",
      "\n",
      "17.   89.0s -   89.2s ( 0.2s) | Gmajor7/F# | Confidence: █████░░░░░ 0.50\n",
      "     Notes: F#, G\n",
      "\n",
      "🎵 CHORD PROGRESSION:\n",
      "   D5 | C5 | E5 | C5 | Baugmented/G | E5 | Csus2 | Baugmented/G | A#diminished/E | G5 | C5\n",
      "\n",
      "🎯 QUICK CHORD CHECK:\n",
      "----------------------------------------\n",
      "10-15s: Not enough notes\n",
      "18-23s: Not enough notes\n",
      "45-50s: Bminor   (conf: 0.67) Notes: B, D\n",
      "80-85s: Dsus4    (conf: 1.10) Notes: A, D, G\n"
     ]
    }
   ],
   "source": [
    "chord_results, recognizer = analyze_guitar_chords(predictions)\n",
    "quick_chord_check(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flexible guitar analysis system ready!\n",
      "This system analyzes ANY fret pattern without rigid shape constraints\n",
      "\n",
      "Usage:\n",
      "  patterns, analyzer = analyze_flexible_guitar_patterns(predictions)\n",
      "  analyze_specific_fret_pattern([3, 2, 0, 0, 3, 3])  # Test any pattern\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "@dataclass\n",
    "class FretChord:\n",
    "    time_start: float\n",
    "    time_end: float\n",
    "    fret_pattern: List[int]  # Frets for each string (6 strings, -1 = not played)\n",
    "    primary_chord: str\n",
    "    alternative_chords: List[Tuple[str, float]]  # Alternative interpretations with confidence\n",
    "    notes_played: List[str]\n",
    "    bass_note: str\n",
    "    chord_intervals: List[int]\n",
    "    chord_complexity: str  # 'simple', 'extended', 'complex'\n",
    "\n",
    "class FlexibleGuitarAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.setup_guitar_system()\n",
    "        self.setup_flexible_chord_theory()\n",
    "        \n",
    "    def setup_guitar_system(self):\n",
    "        \"\"\"Setup guitar tuning and fretboard knowledge\"\"\"\n",
    "        self.string_names = ['E', 'A', 'D', 'G', 'B', 'E']  # Low to high\n",
    "        self.string_tuning = [40, 45, 50, 55, 59, 64]  # MIDI numbers for open strings\n",
    "        self.note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "        \n",
    "    def setup_flexible_chord_theory(self):\n",
    "        \"\"\"Setup comprehensive chord theory for any combination\"\"\"\n",
    "        # More comprehensive chord patterns\n",
    "        self.chord_intervals = {\n",
    "            # Basic triads\n",
    "            'major': [0, 4, 7],\n",
    "            'minor': [0, 3, 7],\n",
    "            'diminished': [0, 3, 6],\n",
    "            'augmented': [0, 4, 8],\n",
    "            'sus2': [0, 2, 7],\n",
    "            'sus4': [0, 5, 7],\n",
    "            \n",
    "            # Seventh chords\n",
    "            'major7': [0, 4, 7, 11],\n",
    "            'minor7': [0, 3, 7, 10],\n",
    "            'dominant7': [0, 4, 7, 10],\n",
    "            'minor7b5': [0, 3, 6, 10],\n",
    "            'diminished7': [0, 3, 6, 9],\n",
    "            'major7#11': [0, 4, 7, 11, 18],\n",
    "            'minor/major7': [0, 3, 7, 11],\n",
    "            \n",
    "            # Extensions\n",
    "            'add9': [0, 4, 7, 14],\n",
    "            'add11': [0, 4, 7, 17],\n",
    "            'major9': [0, 4, 7, 11, 14],\n",
    "            'minor9': [0, 3, 7, 10, 14],\n",
    "            'dominant9': [0, 4, 7, 10, 14],\n",
    "            'major11': [0, 4, 7, 11, 14, 17],\n",
    "            'minor11': [0, 3, 7, 10, 14, 17],\n",
    "            'dominant11': [0, 4, 7, 10, 14, 17],\n",
    "            'major13': [0, 4, 7, 11, 14, 21],\n",
    "            'minor13': [0, 3, 7, 10, 14, 21],\n",
    "            'dominant13': [0, 4, 7, 10, 14, 21],\n",
    "            \n",
    "            # Altered dominants\n",
    "            '7b5': [0, 4, 6, 10],\n",
    "            '7#5': [0, 4, 8, 10],\n",
    "            '7b9': [0, 4, 7, 10, 13],\n",
    "            '7#9': [0, 4, 7, 10, 15],\n",
    "            '7#11': [0, 4, 7, 10, 18],\n",
    "            '7b13': [0, 4, 7, 10, 20],\n",
    "            \n",
    "            # Power chords and dyads\n",
    "            'power': [0, 7],\n",
    "            '5': [0, 7],\n",
    "            'octave': [0, 12],\n",
    "            'fourth': [0, 5],\n",
    "            'fifth': [0, 7],\n",
    "            \n",
    "            # Complex/ambiguous\n",
    "            '6': [0, 4, 7, 9],\n",
    "            'minor6': [0, 3, 7, 9],\n",
    "            '6/9': [0, 4, 7, 9, 14],\n",
    "            'minor6/9': [0, 3, 7, 9, 14],\n",
    "        }\n",
    "        \n",
    "        # Quality descriptors\n",
    "        self.chord_qualities = {\n",
    "            'major': 'Major',\n",
    "            'minor': 'Minor', \n",
    "            'diminished': 'Diminished',\n",
    "            'augmented': 'Augmented',\n",
    "            'power': 'Power Chord',\n",
    "            '5': 'Power Chord'\n",
    "        }\n",
    "    \n",
    "    def fret_to_midi(self, string_idx: int, fret: int) -> int:\n",
    "        \"\"\"Convert string and fret to MIDI note number\"\"\"\n",
    "        return self.string_tuning[string_idx] + fret\n",
    "    \n",
    "    def midi_to_note(self, midi_num: int) -> str:\n",
    "        \"\"\"Convert MIDI number to note name\"\"\"\n",
    "        return self.note_names[midi_num % 12]\n",
    "    \n",
    "    def frets_to_notes_and_midi(self, fret_pattern: List[int]) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"Convert fret pattern to note names and MIDI numbers\"\"\"\n",
    "        notes = []\n",
    "        midi_notes = []\n",
    "        \n",
    "        for string_idx, fret in enumerate(fret_pattern):\n",
    "            if fret >= 0:  # -1 means not played\n",
    "                midi_note = self.fret_to_midi(string_idx, fret)\n",
    "                note_name = self.midi_to_note(midi_note)\n",
    "                notes.append(note_name)\n",
    "                midi_notes.append(midi_note)\n",
    "        \n",
    "        return notes, midi_notes\n",
    "    \n",
    "    def analyze_chord_from_any_pattern(self, fret_pattern: List[int]) -> Tuple[str, List[Tuple[str, float]], List[int]]:\n",
    "        \"\"\"Analyze any fret pattern to find possible chord interpretations\"\"\"\n",
    "        notes, midi_notes = self.frets_to_notes_and_midi(fret_pattern)\n",
    "        \n",
    "        if len(notes) < 2:\n",
    "            return \"Single Note\", [], []\n",
    "        \n",
    "        # Get unique note classes (ignore octaves)\n",
    "        note_classes = sorted(list(set([midi % 12 for midi in midi_notes])))\n",
    "        \n",
    "        if len(note_classes) < 2:\n",
    "            return \"Unison\", [], note_classes\n",
    "        \n",
    "        # Try each note as potential root\n",
    "        possible_chords = []\n",
    "        \n",
    "        for root_candidate in note_classes:\n",
    "            intervals = [(note - root_candidate) % 12 for note in note_classes]\n",
    "            intervals = sorted(list(set(intervals)))\n",
    "            \n",
    "            # Match against all chord patterns\n",
    "            for chord_type, pattern in self.chord_intervals.items():\n",
    "                score = self.calculate_chord_match_score(intervals, pattern)\n",
    "                \n",
    "                if score > 0.5:  # Only consider reasonable matches\n",
    "                    root_note = self.note_names[root_candidate]\n",
    "                    chord_name = self.format_chord_name(root_note, chord_type)\n",
    "                    possible_chords.append((chord_name, score))\n",
    "        \n",
    "        # Sort by confidence\n",
    "        possible_chords.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Determine primary chord\n",
    "        if possible_chords:\n",
    "            primary_chord = possible_chords[0][0]\n",
    "            alternatives = possible_chords[1:5]  # Top 5 alternatives\n",
    "        else:\n",
    "            # Generate descriptive name for unrecognized patterns\n",
    "            primary_chord = self.generate_descriptive_name(note_classes, notes)\n",
    "            alternatives = []\n",
    "        \n",
    "        return primary_chord, alternatives, note_classes\n",
    "    \n",
    "    def calculate_chord_match_score(self, intervals: List[int], pattern: List[int]) -> float:\n",
    "        \"\"\"Calculate how well intervals match a chord pattern\"\"\"\n",
    "        intervals_set = set(intervals)\n",
    "        pattern_set = set(pattern)\n",
    "        \n",
    "        # Essential intervals that must be present\n",
    "        essential = {0}  # Root is essential\n",
    "        if 7 in pattern_set:  # Fifth is important for most chords\n",
    "            essential.add(7)\n",
    "        if 3 in pattern_set or 4 in pattern_set:  # Third defines major/minor\n",
    "            essential.update({3, 4})\n",
    "        \n",
    "        # Check if essential intervals are present\n",
    "        essential_score = len(essential & intervals_set) / len(essential) if essential else 1.0\n",
    "        \n",
    "        if essential_score < 0.5:  # Must have most essential intervals\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate overall match\n",
    "        if pattern_set <= intervals_set:  # All pattern notes present\n",
    "            coverage = len(pattern_set) / len(intervals_set)\n",
    "            return coverage * essential_score\n",
    "        \n",
    "        # Partial match\n",
    "        intersection = len(intervals_set & pattern_set)\n",
    "        union = len(intervals_set | pattern_set)\n",
    "        jaccard = intersection / union if union > 0 else 0\n",
    "        \n",
    "        return jaccard * essential_score * 0.8  # Penalize partial matches\n",
    "    \n",
    "    def format_chord_name(self, root: str, chord_type: str) -> str:\n",
    "        \"\"\"Format chord name properly\"\"\"\n",
    "        if chord_type == 'major':\n",
    "            return root\n",
    "        elif chord_type in ['power', '5']:\n",
    "            return f\"{root}5\"\n",
    "        elif chord_type == 'dominant7':\n",
    "            return f\"{root}7\"\n",
    "        elif chord_type == 'major7':\n",
    "            return f\"{root}maj7\"\n",
    "        elif chord_type == 'minor7':\n",
    "            return f\"{root}m7\"\n",
    "        elif chord_type == 'minor':\n",
    "            return f\"{root}m\"\n",
    "        elif chord_type == 'diminished':\n",
    "            return f\"{root}°\"\n",
    "        elif chord_type == 'augmented':\n",
    "            return f\"{root}+\"\n",
    "        elif chord_type == 'diminished7':\n",
    "            return f\"{root}°7\"\n",
    "        elif chord_type == 'minor7b5':\n",
    "            return f\"{root}m7♭5\"\n",
    "        else:\n",
    "            return f\"{root}{chord_type}\"\n",
    "    \n",
    "    def generate_descriptive_name(self, note_classes: List[int], note_names: List[str]) -> str:\n",
    "        \"\"\"Generate descriptive name for unrecognized patterns\"\"\"\n",
    "        if len(note_classes) == 2:\n",
    "            interval = (note_classes[1] - note_classes[0]) % 12\n",
    "            interval_names = {\n",
    "                1: \"minor 2nd\", 2: \"major 2nd\", 3: \"minor 3rd\", 4: \"major 3rd\",\n",
    "                5: \"perfect 4th\", 6: \"tritone\", 7: \"perfect 5th\", 8: \"minor 6th\",\n",
    "                9: \"major 6th\", 10: \"minor 7th\", 11: \"major 7th\"\n",
    "            }\n",
    "            root = self.note_names[note_classes[0]]\n",
    "            return f\"{root} {interval_names.get(interval, 'interval')}\"\n",
    "        \n",
    "        # For complex patterns, just list the notes\n",
    "        unique_notes = sorted(list(set(note_names)))\n",
    "        if len(unique_notes) <= 4:\n",
    "            return f\"{'/'.join(unique_notes)} cluster\"\n",
    "        else:\n",
    "            return f\"Complex chord ({len(unique_notes)} notes)\"\n",
    "    \n",
    "    def extract_chord_shapes_flexible(self, predictions: Dict, time_resolution: float = 0.8) -> List[FretChord]:\n",
    "        \"\"\"Extract chord shapes with flexible analysis\"\"\"\n",
    "        tablature = predictions['tablature']\n",
    "        times = predictions.get('times', [])\n",
    "        \n",
    "        if len(times) == 0:\n",
    "            return []\n",
    "        \n",
    "        chord_shapes = []\n",
    "        time_step = int(len(times) * time_resolution / (times[-1] - times[0]))\n",
    "        \n",
    "        for i in range(0, len(times) - time_step, time_step // 2):  # 50% overlap\n",
    "            start_idx = i\n",
    "            end_idx = min(i + time_step, len(times) - 1)\n",
    "            \n",
    "            # Collect all fret patterns in this window\n",
    "            patterns_in_window = []\n",
    "            \n",
    "            for time_idx in range(start_idx, end_idx):\n",
    "                if time_idx < tablature.shape[1]:\n",
    "                    pattern = [int(tablature[string][time_idx]) for string in range(6)]\n",
    "                    # Only consider if at least 2 strings are active\n",
    "                    if sum(1 for f in pattern if f >= 0) >= 2:\n",
    "                        patterns_in_window.append(tuple(pattern))\n",
    "            \n",
    "            if not patterns_in_window:\n",
    "                continue\n",
    "            \n",
    "            # Find most common pattern (or representative pattern)\n",
    "            pattern_counts = Counter(patterns_in_window)\n",
    "            most_common = pattern_counts.most_common(1)[0]\n",
    "            representative_pattern = list(most_common[0])\n",
    "            pattern_stability = most_common[1] / len(patterns_in_window)\n",
    "            \n",
    "            # Analyze the pattern\n",
    "            primary_chord, alternatives, intervals = self.analyze_chord_from_any_pattern(representative_pattern)\n",
    "            notes, midi_notes = self.frets_to_notes_and_midi(representative_pattern)\n",
    "            \n",
    "            # Determine bass note (lowest pitch)\n",
    "            bass_note = self.midi_to_note(min(midi_notes)) if midi_notes else \"\"\n",
    "            \n",
    "            # Classify complexity\n",
    "            complexity = self.classify_chord_complexity(len(set(notes)), intervals)\n",
    "            \n",
    "            chord_shape = FretChord(\n",
    "                time_start=times[start_idx],\n",
    "                time_end=times[end_idx],\n",
    "                fret_pattern=representative_pattern,\n",
    "                primary_chord=primary_chord,\n",
    "                alternative_chords=alternatives,\n",
    "                notes_played=notes,\n",
    "                bass_note=bass_note,\n",
    "                chord_intervals=intervals,\n",
    "                chord_complexity=complexity\n",
    "            )\n",
    "            \n",
    "            chord_shapes.append(chord_shape)\n",
    "        \n",
    "        return self.merge_similar_flexible_chords(chord_shapes)\n",
    "    \n",
    "    def classify_chord_complexity(self, num_unique_notes: int, intervals: List[int]) -> str:\n",
    "        \"\"\"Classify chord complexity\"\"\"\n",
    "        if num_unique_notes <= 2:\n",
    "            return \"simple\"\n",
    "        elif num_unique_notes <= 4 and len(intervals) <= 4:\n",
    "            return \"standard\"\n",
    "        elif num_unique_notes <= 5:\n",
    "            return \"extended\"\n",
    "        else:\n",
    "            return \"complex\"\n",
    "    \n",
    "    def merge_similar_flexible_chords(self, chord_shapes: List[FretChord]) -> List[FretChord]:\n",
    "        \"\"\"Merge similar chords with flexible criteria\"\"\"\n",
    "        if not chord_shapes:\n",
    "            return []\n",
    "        \n",
    "        merged = []\n",
    "        current = chord_shapes[0]\n",
    "        \n",
    "        for next_chord in chord_shapes[1:]:\n",
    "            # More flexible merging criteria\n",
    "            same_primary = current.primary_chord == next_chord.primary_chord\n",
    "            similar_pattern = self.patterns_similar(current.fret_pattern, next_chord.fret_pattern)\n",
    "            time_gap = abs(current.time_end - next_chord.time_start)\n",
    "            \n",
    "            if (same_primary or similar_pattern) and time_gap < 0.5:\n",
    "                # Merge chords\n",
    "                current.time_end = next_chord.time_end\n",
    "                # Combine alternative interpretations\n",
    "                combined_alts = current.alternative_chords + next_chord.alternative_chords\n",
    "                current.alternative_chords = sorted(list(set(combined_alts)), \n",
    "                                                  key=lambda x: x[1], reverse=True)[:3]\n",
    "            else:\n",
    "                merged.append(current)\n",
    "                current = next_chord\n",
    "        \n",
    "        merged.append(current)\n",
    "        return merged\n",
    "    \n",
    "    def patterns_similar(self, pattern1: List[int], pattern2: List[int], threshold: float = 0.7) -> bool:\n",
    "        \"\"\"Check if two fret patterns are similar\"\"\"\n",
    "        if len(pattern1) != len(pattern2):\n",
    "            return False\n",
    "        \n",
    "        matches = sum(1 for p1, p2 in zip(pattern1, pattern2) \n",
    "                     if (p1 == p2) or (p1 < 0 and p2 < 0))\n",
    "        total_strings = len(pattern1)\n",
    "        \n",
    "        return (matches / total_strings) >= threshold\n",
    "    \n",
    "    def create_flexible_chord_diagram(self, fret_pattern: List[int], chord_info: FretChord, \n",
    "                                    figsize: Tuple[float, float] = (4, 5)) -> plt.Figure:\n",
    "        \"\"\"Create chord diagram with additional analysis info\"\"\"\n",
    "        fig, (ax_chord, ax_info) = plt.subplots(2, 1, figsize=figsize, \n",
    "                                               gridspec_kw={'height_ratios': [4, 1]})\n",
    "        \n",
    "        # Draw main chord diagram\n",
    "        self._draw_chord_diagram_in_ax(ax_chord, fret_pattern, chord_info.primary_chord)\n",
    "        \n",
    "        # Add analysis info\n",
    "        ax_info.axis('off')\n",
    "        info_text = f\"Bass: {chord_info.bass_note}\\n\"\n",
    "        info_text += f\"Notes: {', '.join(chord_info.notes_played)}\\n\"\n",
    "        info_text += f\"Complexity: {chord_info.chord_complexity}\\n\"\n",
    "        \n",
    "        if chord_info.alternative_chords:\n",
    "            alt_names = [alt[0] for alt in chord_info.alternative_chords[:2]]\n",
    "            info_text += f\"Alternatives: {', '.join(alt_names)}\"\n",
    "        \n",
    "        ax_info.text(0.5, 0.5, info_text, ha='center', va='center', \n",
    "                    fontsize=10, transform=ax_info.transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def _draw_chord_diagram_in_ax(self, ax, fret_pattern: List[int], chord_name: str):\n",
    "        \"\"\"Draw chord diagram in existing axes\"\"\"\n",
    "        # Determine fret range to show\n",
    "        active_frets = [f for f in fret_pattern if f >= 0]\n",
    "        if not active_frets:\n",
    "            num_frets = 4\n",
    "            start_fret = 0\n",
    "        else:\n",
    "            min_fret = min(active_frets)\n",
    "            max_fret = max(active_frets)\n",
    "            \n",
    "            if min_fret == 0 or max_fret <= 4:\n",
    "                start_fret = 0\n",
    "                num_frets = max(4, max_fret) + 1\n",
    "            else:\n",
    "                start_fret = max(0, min_fret - 1)\n",
    "                num_frets = max_fret - start_fret + 2\n",
    "        \n",
    "        # Draw frets\n",
    "        for fret in range(num_frets):\n",
    "            y = num_frets - fret - 1\n",
    "            line_width = 3 if fret + start_fret == 0 else 1\n",
    "            ax.axhline(y=y, color='black', linewidth=line_width)\n",
    "        \n",
    "        # Draw strings\n",
    "        for string in range(6):\n",
    "            ax.axvline(x=string, color='black', linewidth=2)\n",
    "        \n",
    "        # Add fret numbers\n",
    "        for fret in range(num_frets):\n",
    "            actual_fret = fret + start_fret\n",
    "            if actual_fret > 0:\n",
    "                ax.text(-0.7, num_frets - fret - 1.5, str(actual_fret), \n",
    "                       ha='center', va='center', fontsize=8)\n",
    "        \n",
    "        # Add string names\n",
    "        string_names = ['E', 'A', 'D', 'G', 'B', 'E']\n",
    "        for string in range(6):\n",
    "            ax.text(string, num_frets + 0.3, string_names[string], \n",
    "                   ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        # Draw finger positions\n",
    "        for string, fret in enumerate(fret_pattern):\n",
    "            if fret >= 0:\n",
    "                if start_fret == 0 and fret == 0:\n",
    "                    # Open string\n",
    "                    circle = plt.Circle((string, num_frets + 0.15), 0.12, \n",
    "                                      color='white', ec='black', linewidth=2)\n",
    "                    ax.add_patch(circle)\n",
    "                else:\n",
    "                    # Fretted note\n",
    "                    fret_pos = fret - start_fret\n",
    "                    if 0 <= fret_pos < num_frets:\n",
    "                        circle = plt.Circle((string, num_frets - fret_pos - 0.5), 0.15, \n",
    "                                          color='red', ec='black', linewidth=2)\n",
    "                        ax.add_patch(circle)\n",
    "            else:\n",
    "                # Muted string\n",
    "                ax.text(string, num_frets + 0.15, 'X', ha='center', va='center', \n",
    "                       fontweight='bold', fontsize=12, color='red')\n",
    "        \n",
    "        ax.set_xlim(-1, 6)\n",
    "        ax.set_ylim(-0.8, num_frets + 0.8)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Add chord name\n",
    "        ax.text(2.5, -0.5, chord_name, ha='center', va='center', \n",
    "               fontweight='bold', fontsize=14)\n",
    "    \n",
    "    def print_flexible_analysis(self, chord_shapes: List[FretChord], max_chords: int = 15):\n",
    "        \"\"\"Print comprehensive flexible analysis\"\"\"\n",
    "        print(f\"\\n🎸 FLEXIBLE GUITAR ANALYSIS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if not chord_shapes:\n",
    "            print(\"No chord patterns detected.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Detected {len(chord_shapes)} distinct chord patterns\")\n",
    "        print(f\"Showing first {min(max_chords, len(chord_shapes))} patterns:\\n\")\n",
    "        \n",
    "        string_names = ['E', 'A', 'D', 'G', 'B', 'E']\n",
    "        \n",
    "        for i, chord in enumerate(chord_shapes[:max_chords]):\n",
    "            duration = chord.time_end - chord.time_start\n",
    "            \n",
    "            print(f\"{i+1:2d}. {chord.time_start:6.1f}s - {chord.time_end:6.1f}s \"\n",
    "                  f\"({duration:4.1f}s) | {chord.primary_chord:12s} | \"\n",
    "                  f\"{chord.chord_complexity}\")\n",
    "            \n",
    "            # Show fret pattern\n",
    "            fret_display = []\n",
    "            for j, fret in enumerate(chord.fret_pattern):\n",
    "                if fret >= 0:\n",
    "                    fret_display.append(f\"{string_names[j]}:{fret}\")\n",
    "                else:\n",
    "                    fret_display.append(f\"{string_names[j]}:X\")\n",
    "            \n",
    "            print(f\"     Frets: {' | '.join(fret_display)}\")\n",
    "            print(f\"     Notes: {', '.join(chord.notes_played)} (Bass: {chord.bass_note})\")\n",
    "            \n",
    "            # Show alternatives if any\n",
    "            if chord.alternative_chords:\n",
    "                alts = [f\"{alt[0]} ({alt[1]:.2f})\" for alt in chord.alternative_chords[:2]]\n",
    "                print(f\"     Also could be: {', '.join(alts)}\")\n",
    "            \n",
    "            print()\n",
    "\n",
    "# Main analysis function\n",
    "def analyze_flexible_guitar_patterns(predictions):\n",
    "    \"\"\"Flexible analysis that works with any fret patterns\"\"\"\n",
    "    analyzer = FlexibleGuitarAnalyzer()\n",
    "    \n",
    "    print(\"🔍 Analyzing guitar patterns with flexible recognition...\")\n",
    "    \n",
    "    # Extract patterns without rigid shape matching\n",
    "    chord_shapes = analyzer.extract_chord_shapes_flexible(predictions, time_resolution=1.2)\n",
    "    \n",
    "    # Print comprehensive analysis\n",
    "    analyzer.print_flexible_analysis(chord_shapes)\n",
    "    \n",
    "    # Create progression summary\n",
    "    progression_with_alts = []\n",
    "    for chord in chord_shapes:\n",
    "        main = chord.primary_chord\n",
    "        if chord.alternative_chords and chord.alternative_chords[0][1] > 0.8:\n",
    "            main += f\"/{chord.alternative_chords[0][0]}\"\n",
    "        progression_with_alts.append(main)\n",
    "    \n",
    "    print(f\"\\n🎵 DETECTED PROGRESSION:\")\n",
    "    print(f\"   {' | '.join(progression_with_alts)}\")\n",
    "    \n",
    "    return chord_shapes, analyzer\n",
    "\n",
    "# Quick pattern check\n",
    "def analyze_specific_fret_pattern(fret_pattern: List[int]):\n",
    "    \"\"\"Analyze any specific fret pattern\"\"\"\n",
    "    analyzer = FlexibleGuitarAnalyzer()\n",
    "    \n",
    "    print(f\"\\n🎯 ANALYZING PATTERN: {fret_pattern}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    primary, alternatives, intervals = analyzer.analyze_chord_from_any_pattern(fret_pattern)\n",
    "    notes, midi_notes = analyzer.frets_to_notes_and_midi(fret_pattern)\n",
    "    \n",
    "    print(f\"Primary interpretation: {primary}\")\n",
    "    print(f\"Notes: {', '.join(notes)}\")\n",
    "    print(f\"Intervals: {intervals}\")\n",
    "    \n",
    "    if alternatives:\n",
    "        print(f\"\\nAlternative interpretations:\")\n",
    "        for alt_name, confidence in alternatives[:3]:\n",
    "            print(f\"  {alt_name} (confidence: {confidence:.2f})\")\n",
    "    \n",
    "    return primary, alternatives\n",
    "\n",
    "print(\"Flexible guitar analysis system ready!\")\n",
    "print(\"This system analyzes ANY fret pattern without rigid shape constraints\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  patterns, analyzer = analyze_flexible_guitar_patterns(predictions)\")\n",
    "print(\"  analyze_specific_fret_pattern([3, 2, 0, 0, 3, 3])  # Test any pattern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyzing guitar patterns with flexible recognition...\n",
      "\n",
      "🎸 FLEXIBLE GUITAR ANALYSIS\n",
      "================================================================================\n",
      "Detected 114 distinct chord patterns\n",
      "Showing first 15 patterns:\n",
      "\n",
      " 1.    0.0s -    1.2s ( 1.2s) | C5           | simple\n",
      "     Frets: E:3 | A:3 | D:X | G:X | B:X | E:X\n",
      "     Notes: G, C (Bass: G)\n",
      "     Also could be: C5 (1.00), Cfifth (1.00)\n",
      "\n",
      " 2.    0.6s -    1.8s ( 1.2s) | C5           | standard\n",
      "     Frets: E:3 | A:3 | D:X | G:2 | B:X | E:X\n",
      "     Notes: G, C, A (Bass: G)\n",
      "     Also could be: C5 (0.67), Cfifth (0.67)\n",
      "\n",
      " 3.    1.2s -    2.3s ( 1.2s) | C major 3rd  | simple\n",
      "     Frets: E:0 | A:3 | D:X | G:X | B:X | E:X\n",
      "     Notes: E, C (Bass: E)\n",
      "\n",
      " 4.    1.7s -    2.9s ( 1.2s) | C5           | simple\n",
      "     Frets: E:3 | A:3 | D:X | G:X | B:X | E:X\n",
      "     Notes: G, C (Bass: G)\n",
      "     Also could be: C5 (1.00), Cfifth (1.00)\n",
      "\n",
      " 5.    2.3s -    3.5s ( 1.2s) | C5           | simple\n",
      "     Frets: E:3 | A:3 | D:X | G:X | B:X | E:X\n",
      "     Notes: G, C (Bass: G)\n",
      "     Also could be: C5 (1.00), Cfifth (1.00)\n",
      "\n",
      " 6.    7.0s -    8.2s ( 1.2s) | G            | standard\n",
      "     Frets: E:3 | A:2 | D:X | G:X | B:3 | E:X\n",
      "     Notes: G, B, D (Bass: G)\n",
      "     Also could be: Dfourth (0.67), G5 (0.67)\n",
      "\n",
      " 7.    7.5s -    8.7s ( 1.2s) | G major 3rd  | simple\n",
      "     Frets: E:3 | A:2 | D:X | G:X | B:X | E:X\n",
      "     Notes: G, B (Bass: G)\n",
      "\n",
      " 8.    8.1s -    9.3s ( 1.2s) | G major 3rd  | simple\n",
      "     Frets: E:3 | A:2 | D:X | G:X | B:X | E:X\n",
      "     Notes: G, B (Bass: G)\n",
      "\n",
      " 9.    9.3s -   10.5s ( 1.2s) | D5           | simple\n",
      "     Frets: E:X | A:0 | D:0 | G:X | B:X | E:X\n",
      "     Notes: A, D (Bass: A)\n",
      "     Also could be: D5 (1.00), Dfifth (1.00)\n",
      "\n",
      "10.    9.9s -   11.1s ( 1.2s) | D5           | simple\n",
      "     Frets: E:X | A:0 | D:0 | G:X | B:X | E:X\n",
      "     Notes: A, D (Bass: A)\n",
      "     Also could be: D5 (1.00), Dfifth (1.00)\n",
      "\n",
      "11.   10.4s -   11.6s ( 1.2s) | D5           | simple\n",
      "     Frets: E:X | A:0 | D:0 | G:X | B:X | E:X\n",
      "     Notes: A, D (Bass: A)\n",
      "     Also could be: D5 (1.00), Dfifth (1.00)\n",
      "\n",
      "12.   11.0s -   12.2s ( 1.2s) | D5           | simple\n",
      "     Frets: E:X | A:0 | D:0 | G:X | B:X | E:X\n",
      "     Notes: A, D (Bass: A)\n",
      "     Also could be: D5 (1.00), Dfifth (1.00)\n",
      "\n",
      "13.   11.6s -   12.8s ( 1.2s) | Unison       | simple\n",
      "     Frets: E:X | A:5 | D:0 | G:X | B:X | E:X\n",
      "     Notes: D, D (Bass: D)\n",
      "\n",
      "14.   30.2s -   31.4s ( 1.2s) | D            | standard\n",
      "     Frets: E:X | A:X | D:4 | G:2 | B:3 | E:2\n",
      "     Notes: F#, A, D, F# (Bass: F#)\n",
      "     Also could be: D5 (0.67), D5 (0.67)\n",
      "\n",
      "15.   30.8s -   32.0s ( 1.2s) | D            | standard\n",
      "     Frets: E:X | A:X | D:4 | G:2 | B:3 | E:2\n",
      "     Notes: F#, A, D, F# (Bass: F#)\n",
      "     Also could be: D5 (0.67), D5 (0.67)\n",
      "\n",
      "\n",
      "🎵 DETECTED PROGRESSION:\n",
      "   C5/C5 | C5 | C major 3rd | C5/C5 | C5/C5 | G | G major 3rd | G major 3rd | D5/D5 | D5/D5 | D5/D5 | D5/D5 | Unison | D | D | D | D | D | D | D | D | D | D | D | D | D | Dfourth | C | Em | Em | Em | Em | Em | Em | Em | Em | Em | Em | Em | D | D | D | D | D | D major 3rd | D major 3rd | D | Dfourth | Dfourth | Em | Em | Em | Em | Em | Em | Em | Em | Em | Em | Em | Dsus2/Asus4 | Dsus2/Asus4 | Dsus2/Asus4 | D5/D5 | D5/D5 | D major 2nd | D5/D5 | Dsus2/Asus4 | Dsus2/Asus4 | Csus2 | Em | Em | Em | Em | Em | Em | Em | Em | E5/E5 | E5/E5 | E5/E5 | E5 | C#° | C#° | Eminor6 | F#7 | F#7 | F#7#5 | F#7#5 | F#7#5 | F#7#5 | G | G | D | D | D | G | G | G | G | G | D | D | D | D | D major 3rd | D major 3rd | D | Dsus2/Asus4 | Dsus2/Asus4 | C | Em | Em | Em\n"
     ]
    }
   ],
   "source": [
    "patterns, analyzer = analyze_flexible_guitar_patterns(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seperator_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
