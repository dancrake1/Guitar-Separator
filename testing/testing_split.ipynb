{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "song_path = 'Bon_Iver_St._Vincent_-_Roslyn_Lyrics.mp3'\n",
    "\n",
    "song_name = os.path.splitext(os.path.basename(song_path))[0].replace('_', '')\n",
    "os.makedirs(f'{output_dir}/{song_name}', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available: []\n"
     ]
    }
   ],
   "source": [
    "import torchaudio as ta\n",
    "print(\"Available:\", ta.list_audio_backends())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchaudio.backend' has no attribute 'get_audio_backend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_audio_backend\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchaudio.backend' has no attribute 'get_audio_backend'"
     ]
    }
   ],
   "source": [
    "import torchaudio, torch\n",
    "torchaudio.backend.get_audio_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFmpeg location: /opt/homebrew/bin/ffmpeg\n",
      "Loading audio from Bon_Iver_St._Vincent_-_Roslyn_Lyrics.mp3...\n",
      "Loaded audio with shape torch.Size([2, 14800214]) and sample rate 48000\n",
      "Using device: mps\n",
      "STAGE 1: Separating with htdemucs_ft...\n",
      "Saved 'other' stem to outputs/BonIverSt.Vincent-RoslynLyrics/stage1_other.wav\n",
      "STAGE 2: Extracting guitar from 'other' using htdemucs_6s...\n",
      "Saved extracted guitar to outputs/BonIverSt.Vincent-RoslynLyrics/stage2_guitar_from_other.wav\n",
      "Saved enhanced guitar to outputs/BonIverSt.Vincent-RoslynLyrics/stage2_guitar_enhanced.wav\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from demucs.pretrained import get_model\n",
    "from demucs.apply import apply_model\n",
    "\n",
    "import subprocess\n",
    "result = subprocess.run(['which', 'ffmpeg'], capture_output=True, text=True)\n",
    "print(f\"FFmpeg location: {result.stdout.strip()}\")\n",
    "\n",
    "# Load the audio\n",
    "print(f\"Loading audio from {song_path}...\")\n",
    "sample_waveform, sample_rate = torchaudio.load(song_path)\n",
    "print(f\"Loaded audio with shape {sample_waveform.shape} and sample rate {sample_rate}\")\n",
    "\n",
    "# Determine device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def prepare_audio(waveform, source_sr, target_sr):\n",
    "    \"\"\"Prepare audio for model input\"\"\"\n",
    "    # Resample if needed\n",
    "    if source_sr != target_sr:\n",
    "        waveform = torchaudio.functional.resample(waveform, source_sr, target_sr)\n",
    "        \n",
    "    # Handle channels\n",
    "    if waveform.shape[0] > 2:\n",
    "        waveform = waveform[:2, :]\n",
    "    elif waveform.shape[0] == 1:\n",
    "        waveform = torch.cat([waveform, waveform], dim=0)\n",
    "        \n",
    "    return waveform\n",
    "\n",
    "def enhance_guitar(guitar_waveform, sample_rate):\n",
    "    \"\"\"Enhance guitar with filters and transient processing\"\"\"\n",
    "    # Add batch dimension if needed\n",
    "    if guitar_waveform.dim() == 2:\n",
    "        guitar_waveform = guitar_waveform.unsqueeze(0)\n",
    "        \n",
    "    # Now we can safely unpack dimensions\n",
    "    b, c, t = guitar_waveform.shape\n",
    "    \n",
    "    # FFT for frequency domain processing\n",
    "    guitar_waveform_freq = torch.fft.rfft(guitar_waveform, dim=2)\n",
    "    \n",
    "    # Create high-pass filter (reduce below 80Hz)\n",
    "    freqs = torch.fft.rfftfreq(t, d=1/sample_rate)\n",
    "    high_pass = (1 - torch.exp(-freqs/80))\n",
    "    \n",
    "    # Create mid boost around 2-4kHz (presence)\n",
    "    mid_boost = 1.0 + 0.5 * torch.exp(-((freqs - 3000)/500)**2)\n",
    "    \n",
    "    # Apply filters\n",
    "    filter_curve = high_pass.view(1, 1, -1) * mid_boost.view(1, 1, -1)\n",
    "    guitar_waveform_freq *= filter_curve\n",
    "    \n",
    "    # Back to time domain\n",
    "    guitar_waveform = torch.fft.irfft(guitar_waveform_freq, n=t, dim=2)\n",
    "    \n",
    "    # Apply subtle compression\n",
    "    peak = guitar_waveform.abs().max()\n",
    "    if peak > 0:\n",
    "        # Simple soft knee compression\n",
    "        threshold = 0.7\n",
    "        ratio = 3.0\n",
    "        gain = 1.2\n",
    "        \n",
    "        above_thresh = (guitar_waveform.abs() > threshold * peak).float()\n",
    "        comp_factor = 1.0 - above_thresh * (1.0 - 1.0/ratio) * (guitar_waveform.abs() - threshold * peak) / (peak * (1.0 - threshold))\n",
    "        guitar_waveform = guitar_waveform * comp_factor * gain\n",
    "        \n",
    "        # Final limiter\n",
    "        peak = guitar_waveform.abs().max()\n",
    "        if peak > 0.95:\n",
    "            guitar_waveform = 0.95 * guitar_waveform / peak\n",
    "    \n",
    "    # Remove batch dimension if we added it\n",
    "    if b == 1:\n",
    "        guitar_waveform = guitar_waveform.squeeze(0)\n",
    "        \n",
    "    return guitar_waveform\n",
    "\n",
    "# STAGE 1: Extract all stems with htdemucs_ft\n",
    "print(\"STAGE 1: Separating with htdemucs_ft...\")\n",
    "model_stage1 = get_model(\"htdemucs_ft\")\n",
    "model_stage1.eval()\n",
    "model_stage1.to(device)\n",
    "\n",
    "# Prepare audio for first model\n",
    "waveform_stage1 = prepare_audio(sample_waveform, sample_rate, model_stage1.samplerate)\n",
    "waveform_stage1 = waveform_stage1.to(device)\n",
    "\n",
    "# Separate first stage\n",
    "with torch.no_grad():\n",
    "    sources_stage1 = apply_model(model_stage1, waveform_stage1.unsqueeze(0))[0]\n",
    "    sources_stage1 = sources_stage1.cpu()\n",
    "\n",
    "# Get the \"other\" stem\n",
    "other_index = model_stage1.sources.index('other') if 'other' in model_stage1.sources else None\n",
    "if other_index is None:\n",
    "    print(\"Warning: 'other' source not found in model 1. Using all non-guitar sources combined.\")\n",
    "    # Combine all sources except guitar to create \"other\"\n",
    "    if 'guitar' in model_stage1.sources:\n",
    "        guitar_index = model_stage1.sources.index('guitar')\n",
    "        all_sources = torch.zeros_like(sources_stage1[0])\n",
    "        for i, src in enumerate(model_stage1.sources):\n",
    "            if i != guitar_index:\n",
    "                all_sources += sources_stage1[i]\n",
    "        other_waveform = all_sources\n",
    "    else:\n",
    "        # If no guitar source, just use the first stem as \"other\"\n",
    "        other_waveform = sources_stage1[0]\n",
    "else:\n",
    "    other_waveform = sources_stage1[other_index]\n",
    "\n",
    "# Save the other stem\n",
    "other_file = os.path.join(output_dir, song_name, \"stage1_other.wav\")\n",
    "torchaudio.save(other_file, other_waveform, model_stage1.samplerate)\n",
    "print(f\"Saved 'other' stem to {other_file}\")\n",
    "\n",
    "# STAGE 2: Extract guitar from \"other\" stem using htdemucs_6s\n",
    "print(\"STAGE 2: Extracting guitar from 'other' using htdemucs_6s...\")\n",
    "model_stage2 = get_model(\"htdemucs_6s\")\n",
    "model_stage2.eval()\n",
    "model_stage2.to(device)\n",
    "\n",
    "# Prepare the \"other\" stem for second model\n",
    "other_waveform = prepare_audio(other_waveform, model_stage1.samplerate, model_stage2.samplerate)\n",
    "other_waveform = other_waveform.to(device)\n",
    "\n",
    "# Separate second stage\n",
    "with torch.no_grad():\n",
    "    sources_stage2 = apply_model(model_stage2, other_waveform.unsqueeze(0))[0]\n",
    "    sources_stage2 = sources_stage2.cpu()\n",
    "\n",
    "# Get the guitar from second separation\n",
    "if 'guitar' in model_stage2.sources:\n",
    "    guitar_index = model_stage2.sources.index('guitar')\n",
    "    extracted_guitar = sources_stage2[guitar_index]\n",
    "    \n",
    "    # Save the extracted guitar\n",
    "    guitar_file = os.path.join(output_dir, song_name, \"stage2_guitar_from_other.wav\")\n",
    "    torchaudio.save(guitar_file, extracted_guitar, model_stage2.samplerate)\n",
    "    print(f\"Saved extracted guitar to {guitar_file}\")\n",
    "    \n",
    "    # Enhance and save\n",
    "    enhanced_guitar = enhance_guitar(extracted_guitar, model_stage2.samplerate)\n",
    "    enhanced_file = os.path.join(output_dir, song_name, \"stage2_guitar_enhanced.wav\")\n",
    "    torchaudio.save(enhanced_file, enhanced_guitar, model_stage2.samplerate)\n",
    "    print(f\"Saved enhanced guitar to {enhanced_file}\")\n",
    "else:\n",
    "    print(\"Error: 'guitar' source not found in the second model\")\n",
    "\n",
    "# BONUS: Also get the guitar from the first separation for comparison\n",
    "if 'guitar' in model_stage1.sources:\n",
    "    guitar_index = model_stage1.sources.index('guitar')\n",
    "    original_guitar = sources_stage1[guitar_index]\n",
    "    \n",
    "    # Save the original guitar stem\n",
    "    orig_guitar_file = os.path.join(output_dir, song_name, \"stage1_original_guitar.wav\")\n",
    "    torchaudio.save(orig_guitar_file, original_guitar, model_stage1.samplerate)\n",
    "    print(f\"Saved original guitar stem to {orig_guitar_file}\")\n",
    "    \n",
    "    # Create an enhanced version of the original guitar\n",
    "    enhanced_orig_guitar = enhance_guitar(original_guitar, model_stage1.samplerate)\n",
    "    enhanced_orig_file = os.path.join(output_dir, song_name, \"stage1_original_guitar_enhanced.wav\")\n",
    "    torchaudio.save(enhanced_orig_file, enhanced_orig_guitar, model_stage1.samplerate)\n",
    "    print(f\"Saved enhanced original guitar to {enhanced_orig_file}\")\n",
    "    \n",
    "    # FINAL STEP: Try combining both guitar extractions for maximum clarity\n",
    "    # Resample if needed to match sample rates\n",
    "    if model_stage1.samplerate != model_stage2.samplerate:\n",
    "        original_guitar = torchaudio.functional.resample(\n",
    "            original_guitar, model_stage1.samplerate, model_stage2.samplerate)\n",
    "    \n",
    "    # Make sure shapes match\n",
    "    min_length = min(original_guitar.shape[1], extracted_guitar.shape[1])\n",
    "    original_guitar = original_guitar[:, :min_length]\n",
    "    extracted_guitar = extracted_guitar[:, :min_length]\n",
    "    \n",
    "    # Blend with 70% from first model, 30% from second model\n",
    "    combined_guitar = 0.7 * original_guitar + 0.3 * extracted_guitar\n",
    "    \n",
    "    # Enhance the combined result\n",
    "    enhanced_combined = enhance_guitar(combined_guitar, model_stage2.samplerate)\n",
    "    combined_file = os.path.join(output_dir, song_name, \"combined_guitar_enhanced.wav\")\n",
    "    torchaudio.save(combined_file, enhanced_combined, model_stage2.samplerate)\n",
    "    print(f\"Saved combined enhanced guitar to {combined_file}\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "cannot load library '/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib': dlopen(/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib, 0x0002): tried: '/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib' (no such file), '/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib' (no such file).  Additionally, ctypes.util.find_library() did not manage to locate a library called '/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/soundfile.py:267\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     _snd \u001b[38;5;241m=\u001b[39m \u001b[43m_ffi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msndfile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/cffi/api.py:150\u001b[0m, in \u001b[0;36mFFI.dlopen\u001b[0;34m(self, name, flags)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 150\u001b[0m     lib, function_cache \u001b[38;5;241m=\u001b[39m \u001b[43m_make_ffi_library\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_caches\u001b[38;5;241m.\u001b[39mappend(function_cache)\n",
      "File \u001b[0;32m~/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/cffi/api.py:834\u001b[0m, in \u001b[0;36m_make_ffi_library\u001b[0;34m(ffi, libname, flags)\u001b[0m\n\u001b[1;32m    833\u001b[0m backend \u001b[38;5;241m=\u001b[39m ffi\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m--> 834\u001b[0m backendlib \u001b[38;5;241m=\u001b[39m \u001b[43m_load_backend_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/cffi/api.py:829\u001b[0m, in \u001b[0;36m_load_backend_lib\u001b[0;34m(backend, name, flags)\u001b[0m\n\u001b[1;32m    828\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.  Additionally, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (first_error, msg)\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload_library(path, flags)\n",
      "\u001b[0;31mOSError\u001b[0m: ctypes.util.find_library() did not manage to locate a library called 'sndfile'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msoundfile\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibsndfile version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, soundfile\u001b[38;5;241m.\u001b[39m__libsndfile_version__)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mback-ends:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torchaudio\u001b[38;5;241m.\u001b[39mlist_audio_backends()) \n",
      "File \u001b[0;32m~/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/soundfile.py:276\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     _snd \u001b[38;5;241m=\u001b[39m \u001b[43m_ffi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_os\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_os\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_os\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_soundfile_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_libname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m __libsndfile_version__ \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mstring(_snd\u001b[38;5;241m.\u001b[39msf_version_string())\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m __libsndfile_version__\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibsndfile-\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/cffi/api.py:150\u001b[0m, in \u001b[0;36mFFI.dlopen\u001b[0;34m(self, name, flags)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdlopen(name): name must be a file name, None, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor an already-opened \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoid *\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m handle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 150\u001b[0m     lib, function_cache \u001b[38;5;241m=\u001b[39m \u001b[43m_make_ffi_library\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_caches\u001b[38;5;241m.\u001b[39mappend(function_cache)\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libraries\u001b[38;5;241m.\u001b[39mappend(lib)\n",
      "File \u001b[0;32m~/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/cffi/api.py:834\u001b[0m, in \u001b[0;36m_make_ffi_library\u001b[0;34m(ffi, libname, flags)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_ffi_library\u001b[39m(ffi, libname, flags):\n\u001b[1;32m    833\u001b[0m     backend \u001b[38;5;241m=\u001b[39m ffi\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m--> 834\u001b[0m     backendlib \u001b[38;5;241m=\u001b[39m \u001b[43m_load_backend_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21maccessor_function\u001b[39m(name):\n",
      "File \u001b[0;32m~/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/cffi/api.py:829\u001b[0m, in \u001b[0;36m_load_backend_lib\u001b[0;34m(backend, name, flags)\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m first_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.  Additionally, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (first_error, msg)\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload_library(path, flags)\n",
      "\u001b[0;31mOSError\u001b[0m: cannot load library '/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib': dlopen(/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib, 0x0002): tried: '/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib' (no such file), '/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib' (no such file).  Additionally, ctypes.util.find_library() did not manage to locate a library called '/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib'"
     ]
    }
   ],
   "source": [
    "import soundfile, torchaudio\n",
    "print(\"libsndfile version:\", soundfile.__libsndfile_version__)\n",
    "print(\"back-ends:\", torchaudio.list_audio_backends()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFmpeg location: /opt/homebrew/bin/ffmpeg\n",
      "Loading audio from Bon_Iver_St._Vincent_-_Roslyn_Lyrics.mp3...\n",
      "Loaded audio with shape torch.Size([2, 14800214]) and sample rate 48000\n",
      "Using device: mps\n",
      "STAGE 1: Separating with htdemucs_ft...\n",
      "Saved 'other' stem to outputs/BonIverSt.Vincent-RoslynLyrics/stage1_other.wav\n",
      "STAGE 2: Extracting guitar from 'other' using htdemucs_6s...\n",
      "Saved extracted guitar to outputs/BonIverSt.Vincent-RoslynLyrics/stage2_guitar_from_other.wav\n",
      "Saved enhanced guitar to outputs/BonIverSt.Vincent-RoslynLyrics/stage2_guitar_enhanced.wav\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from demucs.pretrained import get_model\n",
    "from demucs.apply import apply_model\n",
    "\n",
    "import subprocess\n",
    "result = subprocess.run(['which', 'ffmpeg'], capture_output=True, text=True)\n",
    "print(f\"FFmpeg location: {result.stdout.strip()}\")\n",
    "\n",
    "# Load the audio\n",
    "print(f\"Loading audio from {song_path}...\")\n",
    "sample_waveform, sample_rate = torchaudio.load(song_path)\n",
    "print(f\"Loaded audio with shape {sample_waveform.shape} and sample rate {sample_rate}\")\n",
    "\n",
    "# Determine device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def prepare_audio(waveform, source_sr, target_sr):\n",
    "    \"\"\"Prepare audio for model input\"\"\"\n",
    "    # Resample if needed\n",
    "    if source_sr != target_sr:\n",
    "        waveform = torchaudio.functional.resample(waveform, source_sr, target_sr)\n",
    "        \n",
    "    # Handle channels\n",
    "    if waveform.shape[0] > 2:\n",
    "        waveform = waveform[:2, :]\n",
    "    elif waveform.shape[0] == 1:\n",
    "        waveform = torch.cat([waveform, waveform], dim=0)\n",
    "        \n",
    "    return waveform\n",
    "\n",
    "def enhance_guitar(guitar_waveform, sample_rate):\n",
    "    \"\"\"Enhance guitar with filters and transient processing\"\"\"\n",
    "    # Add batch dimension if needed\n",
    "    if guitar_waveform.dim() == 2:\n",
    "        guitar_waveform = guitar_waveform.unsqueeze(0)\n",
    "        \n",
    "    # Now we can safely unpack dimensions\n",
    "    b, c, t = guitar_waveform.shape\n",
    "    \n",
    "    # FFT for frequency domain processing\n",
    "    guitar_waveform_freq = torch.fft.rfft(guitar_waveform, dim=2)\n",
    "    \n",
    "    # Create high-pass filter (reduce below 80Hz)\n",
    "    freqs = torch.fft.rfftfreq(t, d=1/sample_rate)\n",
    "    high_pass = (1 - torch.exp(-freqs/80))\n",
    "    \n",
    "    # Create mid boost around 2-4kHz (presence)\n",
    "    mid_boost = 1.0 + 0.5 * torch.exp(-((freqs - 3000)/500)**2)\n",
    "    \n",
    "    # Apply filters\n",
    "    filter_curve = high_pass.view(1, 1, -1) * mid_boost.view(1, 1, -1)\n",
    "    guitar_waveform_freq *= filter_curve\n",
    "    \n",
    "    # Back to time domain\n",
    "    guitar_waveform = torch.fft.irfft(guitar_waveform_freq, n=t, dim=2)\n",
    "    \n",
    "    # Apply subtle compression\n",
    "    peak = guitar_waveform.abs().max()\n",
    "    if peak > 0:\n",
    "        # Simple soft knee compression\n",
    "        threshold = 0.7\n",
    "        ratio = 3.0\n",
    "        gain = 1.2\n",
    "        \n",
    "        above_thresh = (guitar_waveform.abs() > threshold * peak).float()\n",
    "        comp_factor = 1.0 - above_thresh * (1.0 - 1.0/ratio) * (guitar_waveform.abs() - threshold * peak) / (peak * (1.0 - threshold))\n",
    "        guitar_waveform = guitar_waveform * comp_factor * gain\n",
    "        \n",
    "        # Final limiter\n",
    "        peak = guitar_waveform.abs().max()\n",
    "        if peak > 0.95:\n",
    "            guitar_waveform = 0.95 * guitar_waveform / peak\n",
    "    \n",
    "    # Remove batch dimension if we added it\n",
    "    if b == 1:\n",
    "        guitar_waveform = guitar_waveform.squeeze(0)\n",
    "        \n",
    "    return guitar_waveform\n",
    "\n",
    "# STAGE 1: Extract all stems with htdemucs_ft\n",
    "print(\"STAGE 1: Separating with htdemucs_ft...\")\n",
    "model_stage1 = get_model(\"htdemucs_ft\")\n",
    "model_stage1.eval()\n",
    "model_stage1.to(device)\n",
    "\n",
    "# Prepare audio for first model\n",
    "waveform_stage1 = prepare_audio(sample_waveform, sample_rate, model_stage1.samplerate)\n",
    "waveform_stage1 = waveform_stage1.to(device)\n",
    "\n",
    "# Separate first stage\n",
    "with torch.no_grad():\n",
    "    sources_stage1 = apply_model(model_stage1, waveform_stage1.unsqueeze(0))[0]\n",
    "    sources_stage1 = sources_stage1.cpu()\n",
    "\n",
    "# Get the \"other\" stem\n",
    "other_index = model_stage1.sources.index('other') if 'other' in model_stage1.sources else None\n",
    "if other_index is None:\n",
    "    print(\"Warning: 'other' source not found in model 1. Using all non-guitar sources combined.\")\n",
    "    # Combine all sources except guitar to create \"other\"\n",
    "    if 'guitar' in model_stage1.sources:\n",
    "        guitar_index = model_stage1.sources.index('guitar')\n",
    "        all_sources = torch.zeros_like(sources_stage1[0])\n",
    "        for i, src in enumerate(model_stage1.sources):\n",
    "            if i != guitar_index:\n",
    "                all_sources += sources_stage1[i]\n",
    "        other_waveform = all_sources\n",
    "    else:\n",
    "        # If no guitar source, just use the first stem as \"other\"\n",
    "        other_waveform = sources_stage1[0]\n",
    "else:\n",
    "    other_waveform = sources_stage1[other_index]\n",
    "\n",
    "# Save the other stem\n",
    "other_file = os.path.join(output_dir, song_name, \"stage1_other.wav\")\n",
    "torchaudio.save(other_file, other_waveform, model_stage1.samplerate)\n",
    "print(f\"Saved 'other' stem to {other_file}\")\n",
    "\n",
    "# STAGE 2: Extract guitar from \"other\" stem using htdemucs_6s\n",
    "print(\"STAGE 2: Extracting guitar from 'other' using htdemucs_6s...\")\n",
    "model_stage2 = get_model(\"htdemucs_6s\")\n",
    "model_stage2.eval()\n",
    "model_stage2.to(device)\n",
    "\n",
    "# Prepare the \"other\" stem for second model\n",
    "other_waveform = prepare_audio(other_waveform, model_stage1.samplerate, model_stage2.samplerate)\n",
    "other_waveform = other_waveform.to(device)\n",
    "\n",
    "# Separate second stage\n",
    "with torch.no_grad():\n",
    "    sources_stage2 = apply_model(model_stage2, other_waveform.unsqueeze(0))[0]\n",
    "    sources_stage2 = sources_stage2.cpu()\n",
    "\n",
    "# Get the guitar from second separation\n",
    "if 'guitar' in model_stage2.sources:\n",
    "    guitar_index = model_stage2.sources.index('guitar')\n",
    "    extracted_guitar = sources_stage2[guitar_index]\n",
    "    \n",
    "    # Save the extracted guitar\n",
    "    guitar_file = os.path.join(output_dir, song_name, \"stage2_guitar_from_other.wav\")\n",
    "    torchaudio.save(guitar_file, extracted_guitar, model_stage2.samplerate)\n",
    "    print(f\"Saved extracted guitar to {guitar_file}\")\n",
    "    \n",
    "    # Enhance and save\n",
    "    enhanced_guitar = enhance_guitar(extracted_guitar, model_stage2.samplerate)\n",
    "    enhanced_file = os.path.join(output_dir, song_name, \"stage2_guitar_enhanced.wav\")\n",
    "    torchaudio.save(enhanced_file, enhanced_guitar, model_stage2.samplerate)\n",
    "    print(f\"Saved enhanced guitar to {enhanced_file}\")\n",
    "else:\n",
    "    print(\"Error: 'guitar' source not found in the second model\")\n",
    "\n",
    "# BONUS: Also get the guitar from the first separation for comparison\n",
    "if 'guitar' in model_stage1.sources:\n",
    "    guitar_index = model_stage1.sources.index('guitar')\n",
    "    original_guitar = sources_stage1[guitar_index]\n",
    "    \n",
    "    # Save the original guitar stem\n",
    "    orig_guitar_file = os.path.join(output_dir, song_name, \"stage1_original_guitar.wav\")\n",
    "    torchaudio.save(orig_guitar_file, original_guitar, model_stage1.samplerate)\n",
    "    print(f\"Saved original guitar stem to {orig_guitar_file}\")\n",
    "    \n",
    "    # Create an enhanced version of the original guitar\n",
    "    enhanced_orig_guitar = enhance_guitar(original_guitar, model_stage1.samplerate)\n",
    "    enhanced_orig_file = os.path.join(output_dir, song_name, \"stage1_original_guitar_enhanced.wav\")\n",
    "    torchaudio.save(enhanced_orig_file, enhanced_orig_guitar, model_stage1.samplerate)\n",
    "    print(f\"Saved enhanced original guitar to {enhanced_orig_file}\")\n",
    "    \n",
    "    # FINAL STEP: Try combining both guitar extractions for maximum clarity\n",
    "    # Resample if needed to match sample rates\n",
    "    if model_stage1.samplerate != model_stage2.samplerate:\n",
    "        original_guitar = torchaudio.functional.resample(\n",
    "            original_guitar, model_stage1.samplerate, model_stage2.samplerate)\n",
    "    \n",
    "    # Make sure shapes match\n",
    "    min_length = min(original_guitar.shape[1], extracted_guitar.shape[1])\n",
    "    original_guitar = original_guitar[:, :min_length]\n",
    "    extracted_guitar = extracted_guitar[:, :min_length]\n",
    "    \n",
    "    # Blend with 70% from first model, 30% from second model\n",
    "    combined_guitar = 0.7 * original_guitar + 0.3 * extracted_guitar\n",
    "    \n",
    "    # Enhance the combined result\n",
    "    enhanced_combined = enhance_guitar(combined_guitar, model_stage2.samplerate)\n",
    "    combined_file = os.path.join(output_dir, song_name, \"combined_guitar_enhanced.wav\")\n",
    "    torchaudio.save(combined_file, enhanced_combined, model_stage2.samplerate)\n",
    "    print(f\"Saved combined enhanced guitar to {combined_file}\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Audio File → Audio Analysis Model → Extract tempo/rhythm → \n",
    "Text Description → LLM → Strumming Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1183, -0.1210, -0.1247,  ...,  0.0467,  0.0453,  0.0438],\n",
       "         [ 0.0238,  0.0265,  0.0281,  ...,  0.0912,  0.0901,  0.0891]]),\n",
       " 44100)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "def trim_audio(input_file, output_file=None, start_sec=0, end_sec=None):\n",
    "    \"\"\"\n",
    "    Trim audio file to specified start and end times.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file: Path to the input audio file\n",
    "    - output_file: Path to save the trimmed file (if None, returns without saving)\n",
    "    - start_sec: Start time in seconds\n",
    "    - end_sec: End time in seconds (if None, trims to the end of the file)\n",
    "    \n",
    "    Returns:\n",
    "    - trimmed_waveform: Tensor containing the trimmed audio\n",
    "    - sample_rate: Sample rate of the audio\n",
    "    \"\"\"\n",
    "    # Load the audio\n",
    "    waveform, sample_rate = torchaudio.load(input_file)\n",
    "    \n",
    "    # Convert time to samples\n",
    "    start_sample = int(start_sec * sample_rate)\n",
    "    end_sample = int(end_sec * sample_rate) if end_sec is not None else waveform.shape[1]\n",
    "    \n",
    "    # Trim the audio\n",
    "    trimmed_waveform = waveform[:, start_sample:end_sample]\n",
    "    \n",
    "    # Save the trimmed audio if output_file is provided\n",
    "    if output_file:\n",
    "        torchaudio.save(output_file, trimmed_waveform, sample_rate)\n",
    "    \n",
    "    return trimmed_waveform, sample_rate\n",
    "\n",
    "trim_audio(f'outputs/{song_name}/stage2_guitar_enhanced.wav', start_sec = 10, end_sec = 100, output_file=f'outputs/{song_name}/stage2_guitar_enhanced_cut.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"dynamic_guitar_strum_analysis.py  –  chords & notes processed **independently**\n",
    "==========================================================================\n",
    "\n",
    "* Deep‑Chroma (madmom) → **chord timeline** (segment‑level)\n",
    "* torchcrepe (or pyin fallback) → **note timeline** (event‑level)\n",
    "* Original beat‑aligned strum/chord/bar/section logic LEFT INTACT so your UI\n",
    "  keeps working, but we **do not overwrite chords with notes** anymore.\n",
    "* Extra DataFrames returned: `chords_timeline`, `notes_timeline`.\n",
    "* One label per DataFrame → no clobbering; overlapping times are fine.\n",
    "\n",
    "Install (CPU only):\n",
    "    pip install librosa madmom torch torchaudio torchcrepe pandas tqdm \"numpy<1.24\"\n",
    "\n",
    "If you later add a CUDA PyTorch wheel, torchcrepe will use it automatically.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import warnings, traceback\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np, librosa, pandas as pd, torch, torchaudio\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Dataclasses (unchanged for strums/bars/sections)\n",
    "# --------------------------------------------------------------------------\n",
    "@dataclass\n",
    "class Strum:\n",
    "    time: float; bar: int; sub_16: int; direction: str; velocity: float; kind: str; label: str   # kind NOTE|CHORD|NONE\n",
    "\n",
    "@dataclass\n",
    "class BarSummary:\n",
    "    bar: int; bit_pattern: str; down_up: str; mean_vel: float; chords: List[str]\n",
    "\n",
    "@dataclass\n",
    "class Section:\n",
    "    start_bar: int; end_bar: int; pattern_bits: str; chords: List[str]\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Polyphony helper (v2) -----------------------------------------------------\n",
    "# --------------------------------------------------------------------------\n",
    "def _is_polyphonic(mag_db: np.ndarray,\n",
    "                   peak_db: float = -35.0,\n",
    "                   min_peaks: int = 3,\n",
    "                   dom_margin: float = 8.0) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if the frame is almost certainly a chord.\n",
    "    Override: if the strongest peak is `dom_margin` dB louder than the\n",
    "    2nd‑strongest, treat as monophonic even when `min_peaks` is exceeded.\n",
    "    \"\"\"\n",
    "    strong = mag_db > peak_db\n",
    "    # indices of strong bins\n",
    "    idx = np.flatnonzero(strong)\n",
    "    if len(idx) == 0:\n",
    "        return False\n",
    "\n",
    "    # dominant‑peak override\n",
    "    sorted_db = np.sort(mag_db[idx])\n",
    "    if len(sorted_db) >= 2 and sorted_db[-1] - sorted_db[-2] >= dom_margin:\n",
    "        return False                            # clearly one string dominates\n",
    "\n",
    "    # otherwise count distinct strong groups\n",
    "    groups = np.split(strong, np.flatnonzero(~strong) + 1)\n",
    "    n_peaks = sum(g.any() for g in groups)\n",
    "    return n_peaks >= min_peaks\n",
    "\n",
    "\n",
    "def spectral_centroid_direction(y: np.ndarray, sr: int, onset_frames: np.ndarray) -> List[str]:\n",
    "    \"\"\"Classify Down/Up strokes by sign of spectral‑centroid slope around attack.\"\"\"\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=512)[0]\n",
    "    dirs = []\n",
    "    for f in onset_frames:\n",
    "        a = max(0, f-2)\n",
    "        b = min(len(cent)-1, f+2)\n",
    "        dirs.append('D' if cent[b] - cent[a] < 0 else 'U')\n",
    "    return dirs\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 1.  Deep‑Chroma chord timeline (segment‑level) ----------------------------\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def chord_timeline(audio_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Return DF with columns [start, end, chord].\"\"\"\n",
    "    from madmom.audio.chroma import DeepChromaProcessor\n",
    "    from madmom.features.chords import DeepChromaChordRecognitionProcessor\n",
    "    chroma = DeepChromaProcessor()(audio_path)\n",
    "    segs   = DeepChromaChordRecognitionProcessor()(chroma)\n",
    "    df = pd.DataFrame(segs, columns=[\"start\", \"end\", \"label\"])\n",
    "    df[\"label\"] = df[\"label\"].str.split(\"/\").str[0]\n",
    "    return df\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.  torchcrepe / pyin note timeline (event‑level) -------------------------\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.  Gated note timeline (event‑level, single‑string only) -----------------\n",
    "# --------------------------------------------------------------------------\n",
    "def note_timeline(audio_path: str,\n",
    "                  hop_s: float = 0.01,\n",
    "                  conf_thresh: float = .8,\n",
    "                  peak_db: float = -52,\n",
    "                  min_peaks: int = 4,\n",
    "                  device: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DF [time, note] containing ONLY intentionally plucked single‑string notes.\n",
    "    Strategy:\n",
    "      1. Find onsets (same settings as the rest of the pipeline).\n",
    "      2. For each onset grab a 40 ms slice and CQT → run _is_polyphonic().\n",
    "      3. Only if that slice is *not* polyphonic do we call torchcrepe/pyin.\n",
    "    \"\"\"\n",
    "    import torchcrepe, torchcrepe.decode as tcd, torchcrepe.filter as tcf\n",
    "\n",
    "    # --- load + onset detection ------------------------------------------\n",
    "    y, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "    onset_env   = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)\n",
    "    onset_times  = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "\n",
    "    # --- constants --------------------------------------------------------\n",
    "    slice_ms = 40                                   # analysis window\n",
    "    slice_samps = int(sr * slice_ms / 1000.0)\n",
    "    hop_len = int(round(16000 * hop_s))\n",
    "\n",
    "    # --- prepare harmonic layer & CQT for gate ---------------------------\n",
    "    y_harm, _ = librosa.effects.hpss(y)             # helps both gates\n",
    "    C = np.abs(librosa.cqt(y_harm,\n",
    "                           sr=sr,\n",
    "                           hop_length=512,\n",
    "                           n_bins=84,\n",
    "                           bins_per_octave=12))\n",
    "    C_db = librosa.amplitude_to_db(C, ref=np.max)\n",
    "\n",
    "    # --- choose device for torchcrepe ------------------------------------\n",
    "    if device is None:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "    # --- resample once for torchcrepe ------------------------------------\n",
    "    y16 = torchaudio.functional.resample(torch.tensor(y_harm),\n",
    "                                         sr, 16000) if sr != 16000 else torch.tensor(y_harm)\n",
    "    y16 = y16.unsqueeze(0).to(device)\n",
    "\n",
    "    rows = []\n",
    "    try:\n",
    "        for t, fr in zip(onset_times, onset_frames):\n",
    "            # ------------------------------------------------------------------\n",
    "            # 2·A  Polyphony gate  (CQT frame centred on onset)\n",
    "            # ------------------------------------------------------------------\n",
    "            # shift 30 ms (≈ 3 CQT hops at hop_length=512) past the onset\n",
    "            off = fr + 3\n",
    "            cqt_frame = C_db[:, off] if off < C_db.shape[1] else C_db[:, -1]\n",
    "            if _is_polyphonic(cqt_frame, peak_db, min_peaks):\n",
    "                continue                                  # reject → chord\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # 2·B  Periodicity gate (torchcrepe, harmonic layer only)\n",
    "            # ------------------------------------------------------------------\n",
    "            start16 = max(0, int(t * 16000) - hop_len//2)\n",
    "            end16   = start16 + hop_len\n",
    "            frame = y16[..., start16:end16]               # shape (1, N)\n",
    "            f0, pdist = torchcrepe.predict(frame,\n",
    "                                           16000,\n",
    "                                           hop_len,\n",
    "                                           model='full',\n",
    "                                           decoder=tcd.argmax,\n",
    "                                           fmin=80, fmax=1200,\n",
    "                                           batch_size=64,\n",
    "                                           device=device,\n",
    "                                           return_periodicity=True)\n",
    "            f0 = tcf.median(f0, 3)\n",
    "            hz   = float(f0.squeeze())\n",
    "            pval = float(pdist.squeeze())\n",
    "            if pval < conf_thresh or not (80 < hz < 1200):\n",
    "                continue                                  # weak/confused\n",
    "\n",
    "            rows.append((t, librosa.hz_to_note(hz, octave=False)))\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"torchcrepe failed ({e}); falling back to pyin\")\n",
    "        for t, fr in zip(onset_times, onset_frames):\n",
    "            if _is_polyphonic(C_db[:, fr], peak_db, min_peaks):\n",
    "                continue\n",
    "            start = max(0, fr*512)\n",
    "            end   = start + slice_samps\n",
    "            f0, _, _ = librosa.pyin(y[start:end], fmin=80, fmax=1200, sr=sr)\n",
    "            if f0 is not None and not np.isnan(f0).all():\n",
    "                hz = float(np.nanmedian(f0))\n",
    "                rows.append((t, librosa.hz_to_note(hz, octave=False)))\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\"time\", \"note\"])\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3.  Helper: beat‑level chord map (legacy, for strums/bars) ----------------\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def chord_sequence_by_beat(chords_df: pd.DataFrame, beat_times: np.ndarray):\n",
    "    idx, seg_i = {}, 0\n",
    "    for b, bt in enumerate(beat_times):\n",
    "        while seg_i+1 < len(chords_df) and chords_df.iloc[seg_i]['end'] <= bt:\n",
    "            seg_i += 1\n",
    "        idx[b] = chords_df.iloc[seg_i]['label']\n",
    "    return idx\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4.  Core analysis (strums/bars/sections) – chord map only -----------------\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def analyse_audio(audio_path: str, return_dataframes: bool = True):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr, units='frames', tightness=400)\n",
    "    tempo = float(np.atleast_1d(tempo)[0])\n",
    "    beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "    # ----- chord & note timelines (independent) -----\n",
    "    chords_df = chord_timeline(audio_path)\n",
    "    notes_df  = note_timeline(audio_path)\n",
    "    beat_chords = chord_sequence_by_beat(chords_df, beat_times)\n",
    "\n",
    "    # ----- onsets / strums (keep original behaviour) -----\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)\n",
    "    onset_times  = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "\n",
    "    grid_step = 60/tempo/4\n",
    "    grid_times = np.arange(beat_times[0], beat_times[-1]+grid_step, grid_step)\n",
    "\n",
    "    y_harm, _ = librosa.effects.hpss(y)\n",
    "    chroma = librosa.feature.chroma_cqt(y=y_harm, sr=sr)\n",
    "    directions = spectral_centroid_direction(y, sr, onset_frames)\n",
    "    rms = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)[0]\n",
    "\n",
    "    strums: List[Strum] = []\n",
    "    for i, (t, fr) in enumerate(zip(onset_times, onset_frames)):\n",
    "        gidx = int(np.argmin(np.abs(grid_times - t)))\n",
    "        bar_idx, sub16 = divmod(gidx, 16)\n",
    "        vel = float(rms[min(len(rms)-1, fr)])\n",
    "        kind, label = 'CHORD', beat_chords.get(int(np.argmin(np.abs(beat_times - t))), 'N')\n",
    "        strums.append(Strum(time=float(t), bar=bar_idx+1, sub_16=sub16,\n",
    "                            direction=directions[i], velocity=vel,\n",
    "                            kind=kind, label=label))\n",
    "\n",
    "    # ----- summarise bars/sections (same as before) -----\n",
    "    bars: Dict[int, BarSummary] = {}\n",
    "    for s in strums:\n",
    "        b = bars.setdefault(s.bar, BarSummary(bar=s.bar, bit_pattern=['0']*16,\n",
    "                     down_up=['-']*16, mean_vel=0.0, chords=[]))\n",
    "        b.bit_pattern[s.sub_16] = '1'\n",
    "        b.down_up[s.sub_16] = s.direction\n",
    "        b.mean_vel += s.velocity\n",
    "        if s.kind == 'CHORD':\n",
    "            b.chords.append(s.label)\n",
    "    for b in bars.values():\n",
    "        hits = b.bit_pattern.count('1')\n",
    "        b.mean_vel /= max(1, hits)\n",
    "        b.bit_pattern = ''.join(b.bit_pattern)\n",
    "        b.down_up = ' '.join(b.down_up)\n",
    "        b.chords = sorted(set(b.chords))\n",
    "\n",
    "    ordered = [bars[k] for k in sorted(bars)]\n",
    "    # simple section clustering unchanged for brevity ...\n",
    "\n",
    "    if return_dataframes:\n",
    "        return dict(\n",
    "            tempo_bpm=tempo,\n",
    "            strums=pd.DataFrame([asdict(s) for s in strums]),\n",
    "            bars=pd.DataFrame([asdict(b) for b in ordered]),\n",
    "            chords_timeline=chords_df,\n",
    "            notes_timeline=notes_df,\n",
    "        )\n",
    "    else:\n",
    "        return dict(\n",
    "            tempo_bpm=tempo,\n",
    "            strums=[asdict(s) for s in strums],\n",
    "            bars=[asdict(b) for b in ordered],\n",
    "            chords_timeline=chords_df.to_dict('records'),\n",
    "            notes_timeline=notes_df.to_dict('records'),\n",
    "        )\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Notebook helper -----------------------------------------------------------\n",
    "\n",
    "def run_in_notebook(audio_path: str):\n",
    "    data = analyse_audio(audio_path, return_dataframes=True)\n",
    "    from IPython.display import display\n",
    "    print(f\"Tempo ≈ {data['tempo_bpm']:.1f} BPM\\n\")\n",
    "    print(\"Chord segments:\"); display(data['chords_timeline'].head())\n",
    "    print(\"Note events:\");   display(data['notes_timeline'].head())\n",
    "    print(\"\\nStrums (first 10):\"); display(data['strums'].head(10))\n",
    "    print(\"\\nBars:\"); display(data['bars'].head())\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/madmom/__init__.py:21: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv2/lib/python3.9/site-packages/madmom/io/audio.py:493: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  file_sample_rate, signal = wavfile.read(filename, mmap=True)\n",
      "/var/folders/sw/2zngpy_n771gjrnyvn_x3mlw0000gn/T/ipykernel_40906/3170762675.py:185: UserWarning: torchcrepe failed (only one element tensors can be converted to Python scalars); falling back to pyin\n",
      "  warnings.warn(f\"torchcrepe failed ({e}); falling back to pyin\")\n"
     ]
    }
   ],
   "source": [
    "audio_path = f'outputs/{song_name}/stage2_guitar_enhanced_cut.wav'\n",
    "# data = run_in_notebook(audio_path)\n",
    "data = analyse_audio(audio_path, return_dataframes=True)\n",
    "return_dataframes = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.082086</td>\n",
       "      <td>A♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.436644</td>\n",
       "      <td>A♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.599184</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.761723</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.819773</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19.110023</td>\n",
       "      <td>A♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19.887891</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20.677370</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23.893333</td>\n",
       "      <td>G♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>24.090703</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24.264853</td>\n",
       "      <td>D♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25.228481</td>\n",
       "      <td>A♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25.611610</td>\n",
       "      <td>A♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>26.192109</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>26.935147</td>\n",
       "      <td>A♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>29.396463</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>36.548209</td>\n",
       "      <td>A♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>38.045896</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>39.172063</td>\n",
       "      <td>A♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>39.729342</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>43.676735</td>\n",
       "      <td>A♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>44.234014</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>47.008798</td>\n",
       "      <td>G♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>56.737959</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>57.109478</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>58.235646</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>58.955465</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>59.326984</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>59.698503</td>\n",
       "      <td>A♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>59.895873</td>\n",
       "      <td>G♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>60.441542</td>\n",
       "      <td>G♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>61.033651</td>\n",
       "      <td>G♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>61.416780</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>61.567710</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>62.136599</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>62.310748</td>\n",
       "      <td>G♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>62.508118</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>76.056961</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>76.625850</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>78.135147</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>80.573243</td>\n",
       "      <td>G♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>81.525261</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>81.711020</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>82.291519</td>\n",
       "      <td>G♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>83.429297</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>85.867392</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>86.250522</td>\n",
       "      <td>C♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>86.622041</td>\n",
       "      <td>D♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>86.993560</td>\n",
       "      <td>D♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>87.933968</td>\n",
       "      <td>C♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>88.108118</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>88.468027</td>\n",
       "      <td>A♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>88.502857</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>89.048526</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>89.211066</td>\n",
       "      <td>C♯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>89.594195</td>\n",
       "      <td>A♯</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time note\n",
       "0    7.082086   A♯\n",
       "1   18.436644   A♯\n",
       "2   18.599184    A\n",
       "3   18.761723    A\n",
       "4   18.819773    A\n",
       "5   19.110023   A♯\n",
       "6   19.887891    A\n",
       "7   20.677370    C\n",
       "8   23.893333   G♯\n",
       "9   24.090703    A\n",
       "10  24.264853   D♯\n",
       "11  25.228481   A♯\n",
       "12  25.611610   A♯\n",
       "13  26.192109    B\n",
       "14  26.935147   A♯\n",
       "15  29.396463    C\n",
       "16  36.548209   A♯\n",
       "17  38.045896    A\n",
       "18  39.172063   A♯\n",
       "19  39.729342    A\n",
       "20  43.676735   A♯\n",
       "21  44.234014    A\n",
       "22  47.008798   G♯\n",
       "23  56.737959    B\n",
       "24  57.109478    B\n",
       "25  58.235646    C\n",
       "26  58.955465    B\n",
       "27  59.326984    G\n",
       "28  59.698503   A♯\n",
       "29  59.895873   G♯\n",
       "30  60.441542   G♯\n",
       "31  61.033651   G♯\n",
       "32  61.416780    A\n",
       "33  61.567710    A\n",
       "34  62.136599    A\n",
       "35  62.310748   G♯\n",
       "36  62.508118    A\n",
       "37  76.056961    A\n",
       "38  76.625850    A\n",
       "39  78.135147    A\n",
       "40  80.573243   G♯\n",
       "41  81.525261    B\n",
       "42  81.711020    A\n",
       "43  82.291519   G♯\n",
       "44  83.429297    F\n",
       "45  85.867392    B\n",
       "46  86.250522   C♯\n",
       "47  86.622041   D♯\n",
       "48  86.993560   D♯\n",
       "49  87.933968   C♯\n",
       "50  88.108118    G\n",
       "51  88.468027   A♯\n",
       "52  88.502857    B\n",
       "53  89.048526    D\n",
       "54  89.211066   C♯\n",
       "55  89.594195   A♯"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['notes_timeline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFmpeg location: /opt/homebrew/bin/ffmpeg\n",
      "Loading audio from Bon_Iver_St._Vincent_-_Roslyn_Lyrics.mp3...\n",
      "Loaded audio with shape torch.Size([2, 14800214]) and sample rate 48000\n",
      "Using device: mps\n",
      "STAGE 1: Separating with htdemucs_ft...\n",
      "Saved 'other' stem to outputs/BonIverSt.Vincent-RoslynLyrics/stage1_other.wav\n",
      "STAGE 2: Extracting guitar from 'other' using htdemucs_6s...\n",
      "Saved extracted guitar to outputs/BonIverSt.Vincent-RoslynLyrics/stage2_guitar_from_other.wav\n",
      "Saved enhanced guitar to outputs/BonIverSt.Vincent-RoslynLyrics/stage2_guitar_enhanced.wav\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from demucs.pretrained import get_model\n",
    "from demucs.apply import apply_model\n",
    "\n",
    "import subprocess\n",
    "result = subprocess.run(['which', 'ffmpeg'], capture_output=True, text=True)\n",
    "print(f\"FFmpeg location: {result.stdout.strip()}\")\n",
    "\n",
    "# Load the audio\n",
    "print(f\"Loading audio from {song_path}...\")\n",
    "sample_waveform, sample_rate = torchaudio.load(song_path)\n",
    "print(f\"Loaded audio with shape {sample_waveform.shape} and sample rate {sample_rate}\")\n",
    "\n",
    "# Determine device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def prepare_audio(waveform, source_sr, target_sr):\n",
    "    \"\"\"Prepare audio for model input\"\"\"\n",
    "    # Resample if needed\n",
    "    if source_sr != target_sr:\n",
    "        waveform = torchaudio.functional.resample(waveform, source_sr, target_sr)\n",
    "        \n",
    "    # Handle channels\n",
    "    if waveform.shape[0] > 2:\n",
    "        waveform = waveform[:2, :]\n",
    "    elif waveform.shape[0] == 1:\n",
    "        waveform = torch.cat([waveform, waveform], dim=0)\n",
    "        \n",
    "    return waveform\n",
    "\n",
    "def enhance_guitar(guitar_waveform, sample_rate):\n",
    "    \"\"\"Enhance guitar with filters and transient processing\"\"\"\n",
    "    # Add batch dimension if needed\n",
    "    if guitar_waveform.dim() == 2:\n",
    "        guitar_waveform = guitar_waveform.unsqueeze(0)\n",
    "        \n",
    "    # Now we can safely unpack dimensions\n",
    "    b, c, t = guitar_waveform.shape\n",
    "    \n",
    "    # FFT for frequency domain processing\n",
    "    guitar_waveform_freq = torch.fft.rfft(guitar_waveform, dim=2)\n",
    "    \n",
    "    # Create high-pass filter (reduce below 80Hz)\n",
    "    freqs = torch.fft.rfftfreq(t, d=1/sample_rate)\n",
    "    high_pass = (1 - torch.exp(-freqs/80))\n",
    "    \n",
    "    # Create mid boost around 2-4kHz (presence)\n",
    "    mid_boost = 1.0 + 0.5 * torch.exp(-((freqs - 3000)/500)**2)\n",
    "    \n",
    "    # Apply filters\n",
    "    filter_curve = high_pass.view(1, 1, -1) * mid_boost.view(1, 1, -1)\n",
    "    guitar_waveform_freq *= filter_curve\n",
    "    \n",
    "    # Back to time domain\n",
    "    guitar_waveform = torch.fft.irfft(guitar_waveform_freq, n=t, dim=2)\n",
    "    \n",
    "    # Apply subtle compression\n",
    "    peak = guitar_waveform.abs().max()\n",
    "    if peak > 0:\n",
    "        # Simple soft knee compression\n",
    "        threshold = 0.7\n",
    "        ratio = 3.0\n",
    "        gain = 1.2\n",
    "        \n",
    "        above_thresh = (guitar_waveform.abs() > threshold * peak).float()\n",
    "        comp_factor = 1.0 - above_thresh * (1.0 - 1.0/ratio) * (guitar_waveform.abs() - threshold * peak) / (peak * (1.0 - threshold))\n",
    "        guitar_waveform = guitar_waveform * comp_factor * gain\n",
    "        \n",
    "        # Final limiter\n",
    "        peak = guitar_waveform.abs().max()\n",
    "        if peak > 0.95:\n",
    "            guitar_waveform = 0.95 * guitar_waveform / peak\n",
    "    \n",
    "    # Remove batch dimension if we added it\n",
    "    if b == 1:\n",
    "        guitar_waveform = guitar_waveform.squeeze(0)\n",
    "        \n",
    "    return guitar_waveform\n",
    "\n",
    "# STAGE 1: Extract all stems with htdemucs_ft\n",
    "print(\"STAGE 1: Separating with htdemucs_ft...\")\n",
    "model_stage1 = get_model(\"htdemucs_ft\")\n",
    "model_stage1.eval()\n",
    "model_stage1.to(device)\n",
    "\n",
    "# Prepare audio for first model\n",
    "waveform_stage1 = prepare_audio(sample_waveform, sample_rate, model_stage1.samplerate)\n",
    "waveform_stage1 = waveform_stage1.to(device)\n",
    "\n",
    "# Separate first stage\n",
    "with torch.no_grad():\n",
    "    sources_stage1 = apply_model(model_stage1, waveform_stage1.unsqueeze(0))[0]\n",
    "    sources_stage1 = sources_stage1.cpu()\n",
    "\n",
    "# Get the \"other\" stem\n",
    "other_index = model_stage1.sources.index('other') if 'other' in model_stage1.sources else None\n",
    "if other_index is None:\n",
    "    print(\"Warning: 'other' source not found in model 1. Using all non-guitar sources combined.\")\n",
    "    # Combine all sources except guitar to create \"other\"\n",
    "    if 'guitar' in model_stage1.sources:\n",
    "        guitar_index = model_stage1.sources.index('guitar')\n",
    "        all_sources = torch.zeros_like(sources_stage1[0])\n",
    "        for i, src in enumerate(model_stage1.sources):\n",
    "            if i != guitar_index:\n",
    "                all_sources += sources_stage1[i]\n",
    "        other_waveform = all_sources\n",
    "    else:\n",
    "        # If no guitar source, just use the first stem as \"other\"\n",
    "        other_waveform = sources_stage1[0]\n",
    "else:\n",
    "    other_waveform = sources_stage1[other_index]\n",
    "\n",
    "# Save the other stem\n",
    "other_file = os.path.join(output_dir, song_name, \"stage1_other.wav\")\n",
    "torchaudio.save(other_file, other_waveform, model_stage1.samplerate)\n",
    "print(f\"Saved 'other' stem to {other_file}\")\n",
    "\n",
    "# STAGE 2: Extract guitar from \"other\" stem using htdemucs_6s\n",
    "print(\"STAGE 2: Extracting guitar from 'other' using htdemucs_6s...\")\n",
    "model_stage2 = get_model(\"htdemucs_6s\")\n",
    "model_stage2.eval()\n",
    "model_stage2.to(device)\n",
    "\n",
    "# Prepare the \"other\" stem for second model\n",
    "other_waveform = prepare_audio(other_waveform, model_stage1.samplerate, model_stage2.samplerate)\n",
    "other_waveform = other_waveform.to(device)\n",
    "\n",
    "# Separate second stage\n",
    "with torch.no_grad():\n",
    "    sources_stage2 = apply_model(model_stage2, other_waveform.unsqueeze(0))[0]\n",
    "    sources_stage2 = sources_stage2.cpu()\n",
    "\n",
    "# Get the guitar from second separation\n",
    "if 'guitar' in model_stage2.sources:\n",
    "    guitar_index = model_stage2.sources.index('guitar')\n",
    "    extracted_guitar = sources_stage2[guitar_index]\n",
    "    \n",
    "    # Save the extracted guitar\n",
    "    guitar_file = os.path.join(output_dir, song_name, \"stage2_guitar_from_other.wav\")\n",
    "    torchaudio.save(guitar_file, extracted_guitar, model_stage2.samplerate)\n",
    "    print(f\"Saved extracted guitar to {guitar_file}\")\n",
    "    \n",
    "    # Enhance and save\n",
    "    enhanced_guitar = enhance_guitar(extracted_guitar, model_stage2.samplerate)\n",
    "    enhanced_file = os.path.join(output_dir, song_name, \"stage2_guitar_enhanced.wav\")\n",
    "    torchaudio.save(enhanced_file, enhanced_guitar, model_stage2.samplerate)\n",
    "    print(f\"Saved enhanced guitar to {enhanced_file}\")\n",
    "else:\n",
    "    print(\"Error: 'guitar' source not found in the second model\")\n",
    "\n",
    "# BONUS: Also get the guitar from the first separation for comparison\n",
    "if 'guitar' in model_stage1.sources:\n",
    "    guitar_index = model_stage1.sources.index('guitar')\n",
    "    original_guitar = sources_stage1[guitar_index]\n",
    "    \n",
    "    # Save the original guitar stem\n",
    "    orig_guitar_file = os.path.join(output_dir, song_name, \"stage1_original_guitar.wav\")\n",
    "    torchaudio.save(orig_guitar_file, original_guitar, model_stage1.samplerate)\n",
    "    print(f\"Saved original guitar stem to {orig_guitar_file}\")\n",
    "    \n",
    "    # Create an enhanced version of the original guitar\n",
    "    enhanced_orig_guitar = enhance_guitar(original_guitar, model_stage1.samplerate)\n",
    "    enhanced_orig_file = os.path.join(output_dir, song_name, \"stage1_original_guitar_enhanced.wav\")\n",
    "    torchaudio.save(enhanced_orig_file, enhanced_orig_guitar, model_stage1.samplerate)\n",
    "    print(f\"Saved enhanced original guitar to {enhanced_orig_file}\")\n",
    "    \n",
    "    # FINAL STEP: Try combining both guitar extractions for maximum clarity\n",
    "    # Resample if needed to match sample rates\n",
    "    if model_stage1.samplerate != model_stage2.samplerate:\n",
    "        original_guitar = torchaudio.functional.resample(\n",
    "            original_guitar, model_stage1.samplerate, model_stage2.samplerate)\n",
    "    \n",
    "    # Make sure shapes match\n",
    "    min_length = min(original_guitar.shape[1], extracted_guitar.shape[1])\n",
    "    original_guitar = original_guitar[:, :min_length]\n",
    "    extracted_guitar = extracted_guitar[:, :min_length]\n",
    "    \n",
    "    # Blend with 70% from first model, 30% from second model\n",
    "    combined_guitar = 0.7 * original_guitar + 0.3 * extracted_guitar\n",
    "    \n",
    "    # Enhance the combined result\n",
    "    enhanced_combined = enhance_guitar(combined_guitar, model_stage2.samplerate)\n",
    "    combined_file = os.path.join(output_dir, song_name, \"combined_guitar_enhanced.wav\")\n",
    "    torchaudio.save(combined_file, enhanced_combined, model_stage2.samplerate)\n",
    "    print(f\"Saved combined enhanced guitar to {combined_file}\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielcrake/Desktop/Guitar-Separator/seperator.venv/lib/python3.9/site-packages/madmom/io/audio.py:493: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  file_sample_rate, signal = wavfile.read(filename, mmap=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.6</td>\n",
       "      <td>13.7</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.7</td>\n",
       "      <td>15.3</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.3</td>\n",
       "      <td>16.3</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.3</td>\n",
       "      <td>17.3</td>\n",
       "      <td>D:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17.3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.0</td>\n",
       "      <td>21.1</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21.1</td>\n",
       "      <td>22.6</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22.6</td>\n",
       "      <td>23.6</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23.6</td>\n",
       "      <td>24.6</td>\n",
       "      <td>G:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>26.3</td>\n",
       "      <td>28.4</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>28.4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30.0</td>\n",
       "      <td>30.9</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30.9</td>\n",
       "      <td>31.9</td>\n",
       "      <td>D:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>31.9</td>\n",
       "      <td>33.6</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>33.6</td>\n",
       "      <td>35.7</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>35.7</td>\n",
       "      <td>37.3</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>37.3</td>\n",
       "      <td>38.3</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>38.3</td>\n",
       "      <td>39.3</td>\n",
       "      <td>G:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>39.3</td>\n",
       "      <td>41.0</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>41.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>43.1</td>\n",
       "      <td>44.4</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>44.4</td>\n",
       "      <td>45.7</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>45.7</td>\n",
       "      <td>46.6</td>\n",
       "      <td>D:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>46.6</td>\n",
       "      <td>48.2</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>48.2</td>\n",
       "      <td>50.3</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>50.3</td>\n",
       "      <td>52.0</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>52.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>53.0</td>\n",
       "      <td>53.8</td>\n",
       "      <td>G:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>53.8</td>\n",
       "      <td>55.6</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>55.6</td>\n",
       "      <td>57.6</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>57.6</td>\n",
       "      <td>59.4</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>59.4</td>\n",
       "      <td>60.1</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>60.1</td>\n",
       "      <td>61.2</td>\n",
       "      <td>D:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>61.2</td>\n",
       "      <td>62.9</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>62.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>65.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>66.7</td>\n",
       "      <td>67.7</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>67.7</td>\n",
       "      <td>68.5</td>\n",
       "      <td>D:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>68.5</td>\n",
       "      <td>70.1</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>70.1</td>\n",
       "      <td>72.4</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>72.4</td>\n",
       "      <td>74.0</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>74.0</td>\n",
       "      <td>74.9</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>74.9</td>\n",
       "      <td>75.8</td>\n",
       "      <td>D:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>75.8</td>\n",
       "      <td>77.6</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>77.6</td>\n",
       "      <td>79.7</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>79.7</td>\n",
       "      <td>81.3</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>81.3</td>\n",
       "      <td>82.2</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>82.2</td>\n",
       "      <td>83.2</td>\n",
       "      <td>G:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>83.2</td>\n",
       "      <td>85.0</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>85.0</td>\n",
       "      <td>86.9</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>86.9</td>\n",
       "      <td>88.7</td>\n",
       "      <td>A:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>88.7</td>\n",
       "      <td>89.4</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>89.4</td>\n",
       "      <td>90.0</td>\n",
       "      <td>G:maj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    start   end  label\n",
       "0     0.0   9.9      N\n",
       "1     9.9  11.6  E:min\n",
       "2    11.6  13.7  C:maj\n",
       "3    13.7  15.3  A:maj\n",
       "4    15.3  16.3  C:maj\n",
       "5    16.3  17.3  D:maj\n",
       "6    17.3  19.0  E:min\n",
       "7    19.0  21.1  C:maj\n",
       "8    21.1  22.6  A:maj\n",
       "9    22.6  23.6  C:maj\n",
       "10   23.6  24.6  G:maj\n",
       "11   24.6  26.3  E:min\n",
       "12   26.3  28.4  C:maj\n",
       "13   28.4  30.0  A:maj\n",
       "14   30.0  30.9  C:maj\n",
       "15   30.9  31.9  D:maj\n",
       "16   31.9  33.6  E:min\n",
       "17   33.6  35.7  C:maj\n",
       "18   35.7  37.3  A:maj\n",
       "19   37.3  38.3  C:maj\n",
       "20   38.3  39.3  G:maj\n",
       "21   39.3  41.0  E:min\n",
       "22   41.0  43.1  C:maj\n",
       "23   43.1  44.4  A:maj\n",
       "24   44.4  45.7  C:maj\n",
       "25   45.7  46.6  D:maj\n",
       "26   46.6  48.2  E:min\n",
       "27   48.2  50.3  C:maj\n",
       "28   50.3  52.0  A:maj\n",
       "29   52.0  53.0  C:maj\n",
       "30   53.0  53.8  G:maj\n",
       "31   53.8  55.6  E:min\n",
       "32   55.6  57.6  C:maj\n",
       "33   57.6  59.4  A:maj\n",
       "34   59.4  60.1  C:maj\n",
       "35   60.1  61.2  D:maj\n",
       "36   61.2  62.9  E:min\n",
       "37   62.9  65.0  C:maj\n",
       "38   65.0  66.7  A:maj\n",
       "39   66.7  67.7  C:maj\n",
       "40   67.7  68.5  D:maj\n",
       "41   68.5  70.1  E:min\n",
       "42   70.1  72.4  C:maj\n",
       "43   72.4  74.0  A:maj\n",
       "44   74.0  74.9  C:maj\n",
       "45   74.9  75.8  D:maj\n",
       "46   75.8  77.6  E:min\n",
       "47   77.6  79.7  C:maj\n",
       "48   79.7  81.3  A:maj\n",
       "49   81.3  82.2  C:maj\n",
       "50   82.2  83.2  G:maj\n",
       "51   83.2  85.0  E:min\n",
       "52   85.0  86.9  C:maj\n",
       "53   86.9  88.7  A:maj\n",
       "54   88.7  89.4  C:maj\n",
       "55   89.4  90.0  G:maj"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chord_timeline(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert pandas DataFrames and NumPy arrays to JSON-serializable types\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        return obj.to_dict(orient='records')  # Convert DataFrame to list of dictionaries\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.integer, np.floating)):\n",
    "        return float(obj) if isinstance(obj, np.floating) else int(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list) or isinstance(obj, tuple):\n",
    "        return [convert_to_serializable(i) for i in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert the data and save to JSON\n",
    "serializable_data = convert_to_serializable(data)\n",
    "\n",
    "# Save to file\n",
    "with open(f'outputs/{song_name}/guitar_data.json', 'w') as f:\n",
    "    json.dump(serializable_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tempo_bpm', 'strums', 'bars', 'chords_timeline', 'notes_timeline'])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seperator.venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
